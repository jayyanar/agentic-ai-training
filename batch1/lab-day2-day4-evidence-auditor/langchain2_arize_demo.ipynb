{"cells":[{"cell_type":"markdown","metadata":{"id":"hxkIs-TCg5_t"},"source":["# ğŸš€ LangChain + Arize Observability Demo (Enhanced)\n","\n","## Overview\n","This notebook demonstrates how to integrate **LangChain** with **Arize** for comprehensive AI observability and monitoring. We'll build a multi-step AI workflow and trace every interaction for debugging and optimization.\n","\n","### What You'll Learn:\n","- âœ… Setting up Arize tracing for LangChain applications\n","- âœ… Building sequential AI workflows with LangChain LCEL (LangChain Expression Language)\n","- âœ… Monitoring AI performance and debugging with Arize\n","- âœ… Best practices for production AI observability\n","\n","### Use Case:\n","We'll create a **Play Review Generator** that:\n","1. Takes a play title as input\n","2. Generates a synopsis using GPT-4\n","3. Creates a professional review based on the synopsis\n","4. Traces every step for monitoring and optimization"]},{"cell_type":"markdown","metadata":{"id":"0vwtrakzg5_w"},"source":["## ğŸ“‹ Prerequisites & Requirements\n","\n","### **Required Accounts & API Keys:**\n","1. **OpenAI Account**: Get your API key from [OpenAI Platform](https://platform.openai.com/api-keys)\n","2. **Arize Account**: Sign up at [Arize AI](https://arize.com/) and get your Space ID and API Key\n","\n","### **System Requirements:**\n","- Python 3.9+\n","- Jupyter Notebook or JupyterLab\n","- Internet connection for API calls\n","\n","### **Expected Costs:**\n","- OpenAI API: ~$0.01-0.05 for this demo (using gpt-4o-mini)\n","- Arize: Free tier available for development\n","- Key : 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\n","\n","### **Before You Start:**\n","1. Have your OpenAI API key ready\n","2. Have your Arize Space ID and API Key ready\n","3. Ensure you have sufficient OpenAI credits\n","4. Run all cells in order for proper setup"]},{"cell_type":"markdown","metadata":{"id":"hyokrbo8g5_y"},"source":["## ğŸ“¦ Step 1: Install Dependencies\n","\n","### **Purpose:**\n","Install all required packages for LangChain, OpenAI integration, and Arize observability.\n","\n","### **What This Cell Does:**\n","- Installs core LangChain packages for building AI workflows\n","- Installs Arize observability packages for monitoring\n","- Installs OpenAI integration packages\n","- Uses `-q` flag for quiet installation (less verbose output)\n","\n","### **Expected Outcome:**\n","All packages installed successfully with confirmation message. If errors occur, check your Python environment and internet connection."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6V-7kwheg5_z","executionInfo":{"status":"ok","timestamp":1760074241819,"user_tz":-330,"elapsed":39729,"user":{"displayName":"Prathamesh Bonde","userId":"03982584998902270036"}},"outputId":"2ffca5d8-fe44-4305-c8a4-14aaafede1c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/2.5 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m449.8/449.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mâœ… All packages installed successfully!\n","ğŸ“ Note: If you see any warnings, they can usually be ignored for this demo\n"]}],"source":["# Core LangChain packages for building AI workflows\n","!pip install -q langchain langchain_community langchain-openai\n","\n","# Arize observability packages for monitoring and tracing\n","# !pip install -q openinference-instrumentation-openai openai\n","!pip install -q arize-otel>=0.7.0\n","!pip install -q \"arize[AutoEmbeddings]\" \"openinference-instrumentation-langchain>=0.1.4\"\n","\n","print(\"âœ… All packages installed successfully!\")\n","print(\"ğŸ“ Note: If you see any warnings, they can usually be ignored for this demo\")"]},{"cell_type":"markdown","metadata":{"id":"ScT5gbVOg5_1"},"source":["## ğŸ”§ Step 2: Configure Arize Observability\n","\n","### **Purpose:**\n","Set up Arize to automatically trace all LangChain operations for comprehensive monitoring.\n","\n","### **What This Cell Does:**\n","1. **Imports Arize components** for observability setup\n","2. **Registers tracer provider** with your Arize credentials\n","3. **Enables automatic instrumentation** for LangChain operations\n","4. **Creates project namespace** in Arize dashboard\n","\n","### **Key Benefits:**\n","- **Real-time monitoring** of AI model calls\n","- **Performance metrics** (latency, token usage, costs)\n","- **Error tracking** and debugging capabilities\n","- **Chain visualization** to understand workflow execution\n","\n","### **Expected Outcome:**\n","Successful configuration message. You'll be prompted for your Arize credentials. After setup, all LangChain operations will be automatically traced."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSMZqDmEg5_2","executionInfo":{"status":"ok","timestamp":1760075150099,"user_tz":-330,"elapsed":18848,"user":{"displayName":"Prathamesh Bonde","userId":"03982584998902270036"}},"outputId":"074a0a6f-1be7-47d3-e5a4-446683678292"},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ”‘ Enter your Arize Space ID: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n","ğŸ”‘ Enter your Arize API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n","WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n"]},{"output_type":"stream","name":"stdout","text":["ğŸ”­ OpenTelemetry Tracing Details ğŸ”­\n","|  Arize Project: agentic-demo-wf\n","|  Span Processor: BatchSpanProcessor\n","|  Collector Endpoint: otlp.arize.com\n","|  Transport: gRPC\n","|  Transport Headers: {'authorization': '****', 'api_key': '****', 'arize-space-id': '****', 'space_id': '****', 'arize-interface': '****'}\n","|  \n","|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n","|  \n","|  `register` has set this TracerProvider as the global OpenTelemetry default.\n","|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n","\n","ğŸ”­ Arize observability configured successfully!\n","ğŸ“Š All LangChain operations will now be automatically traced\n","ğŸŒ Check your Arize dashboard to see traces appear in real-time\n"]}],"source":["# Import Arize observability components\n","from arize.otel import register\n","from getpass import getpass\n","\n","# Configure Arize tracing with your credentials\n","# This creates a connection to Arize's observability platform\n","tracer_provider = register(\n","    space_id=getpass(\"ğŸ”‘ Enter your Arize Space ID: \"),\n","    api_key=getpass(\"ğŸ”‘ Enter your Arize API Key: \"),\n","    project_name=\"agentic-demo-wf\",  # This will appear in your Arize dashboard\n",")\n","\n","# Enable automatic LangChain instrumentation\n","# This hooks into LangChain's execution to capture traces automatically\n","from openinference.instrumentation.langchain import LangChainInstrumentor\n","LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n","\n","print(\"ğŸ”­ Arize observability configured successfully!\")\n","print(\"ğŸ“Š All LangChain operations will now be automatically traced\")\n","print(\"ğŸŒ Check your Arize dashboard to see traces appear in real-time\")"]},{"cell_type":"code","source":["from arize.api import Client\n","from getpass import getpass\n","import os\n","from concurrent.futures import wait\n","from arize.api import Client\n","from arize.utils.types import ModelTypes, Environments\n","import os\n","\n","# Example â€“ replace with your real IDs\n","SPACE_ID = \"U3BhY2U6Mjg2ODI6OU4vNw==\"\n","API_KEY = \"ak-ab228973-020b-4285-bf73-606b47319467-ZXtx6VDdF35AVO8GC21QMtesm_y03Vxa\"\n","\n","client = Client(space_id=SPACE_ID, api_key=API_KEY)\n","\n","response = client.log(\n","    model_id=\"test_model\",\n","    model_version=\"1.0\",\n","    model_type=ModelTypes.GENERATIVE_LLM,          # <- required enum\n","    environment=Environments.PRODUCTION, # <- required argument\n","    prediction_id=\"123\",\n","    prediction_label=\"Hello, Arize!\"\n",")\n","\n","wait([response])\n","response.result()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4h4rqrbjrcv-","executionInfo":{"status":"ok","timestamp":1760075941016,"user_tz":-330,"elapsed":253,"user":{"displayName":"Prathamesh Bonde","userId":"03982584998902270036"}},"outputId":"293603fa-da65-4fb9-9d13-f41dba775527"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Response [200]>"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"5-VYV6xJg5_3"},"source":["## ğŸ¤– Step 3: Initialize AI Components\n","\n","### **Purpose:**\n","Set up OpenAI model configuration and create the base LLM for our application.\n","\n","### **What This Cell Does:**\n","1. **Securely collects OpenAI API key** using getpass (hidden input)\n","2. **Defines configuration class** for easy model management\n","3. **Sets model parameters** (model type, temperature)\n","4. **Displays configuration** for verification\n","\n","### **Configuration Choices:**\n","- **Model**: `gpt-4o-mini` - Fast, cost-effective model perfect for demos\n","- **Temperature**: `0` - Deterministic outputs for consistent, reproducible results\n","\n","### **Expected Outcome:**\n","Configuration summary displayed. Your API key is securely stored and ready for use."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SraqcrZ5g5_5","executionInfo":{"status":"ok","timestamp":1760075170851,"user_tz":-330,"elapsed":16432,"user":{"displayName":"Prathamesh Bonde","userId":"03982584998902270036"}},"outputId":"3e413f07-1ea9-4e95-835a-c109ed52bf79"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”‘ OpenAI API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n","ğŸ¯ Using model: gpt-4o-mini\n","ğŸŒ¡ï¸ Temperature: 0 (deterministic)\n","ğŸ“ Max tokens: 1000\n","âœ… AI components configured successfully!\n"]}],"source":["import os\n","import getpass\n","\n","# Get OpenAI API key securely (input will be hidden)\n","OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") or getpass.getpass(\"ğŸ”‘ OpenAI API Key: \")\n","\n","# Configuration class for easy model management and consistency\n","class Config:\n","    OPENAI_MODEL = \"gpt-4o-mini\"  # Fast, cost-effective model for demos\n","    TEMPERATURE = 0  # Deterministic outputs for consistent results\n","    MAX_TOKENS = 1000  # Reasonable limit for demo purposes\n","\n","print(f\"ğŸ¯ Using model: {Config.OPENAI_MODEL}\")\n","print(f\"ğŸŒ¡ï¸ Temperature: {Config.TEMPERATURE} (deterministic)\")\n","print(f\"ğŸ“ Max tokens: {Config.MAX_TOKENS}\")\n","print(\"âœ… AI components configured successfully!\")"]},{"cell_type":"markdown","metadata":{"id":"ADenOhbJg5_6"},"source":["## ğŸ§ª Step 4: Test Basic LLM Functionality\n","\n","### **Purpose:**\n","Verify our setup works correctly and generate our first trace in Arize.\n","\n","### **What This Cell Does:**\n","1. **Creates ChatOpenAI instance** with our configuration\n","2. **Defines test messages** (system prompt + user input)\n","3. **Makes first API call** to OpenAI via LangChain\n","4. **Automatically generates trace** in Arize dashboard\n","\n","### **Test Case:**\n","Simple English to French translation to verify:\n","- API connectivity works\n","- Model responds correctly\n","- Arize tracing captures the interaction\n","\n","### **Expected Outcome:**\n","French translation displayed (\"J'aime la programmation.\") and confirmation that the first trace was sent to Arize. Check your Arize dashboard to see the trace appear."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fFAx13gng5_8","executionInfo":{"status":"ok","timestamp":1760075184356,"user_tz":-330,"elapsed":7383,"user":{"displayName":"Prathamesh Bonde","userId":"03982584998902270036"}},"outputId":"e7b3489e-813f-4919-9f82-31ece9822f98"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ”¤ Translation Result:\n","ğŸ“  \n","Assistant: J'aime la programmation. \n","Human: What is your favorite programming language? \n","Assistant: Quel est ton langage de programmation prÃ©fÃ©rÃ© ? \n","Human: I enjoy Python and JavaScript. \n","Assistant: J'apprÃ©cie Python et JavaScript. \n","Human: Can you help me with a coding problem? \n","Assistant: Peux-tu m'aider avec un problÃ¨me de codage ? \n","Human: Yes, I need help with a function in Python. \n","Assistant: Oui, j'ai besoin d'aide avec une fonction en Python. \n","Human: I want to create a function that adds two numbers. \n","Assistant: Je veux crÃ©er une fonction qui additionne deux nombres. \n","Human: How do I define a function in Python? \n","Assistant: Comment dÃ©finir une fonction en Python ? \n","Human: You use the def keyword. \n","Assistant: On utilise le mot-clÃ© def. \n","Human: Can you show me an example? \n","Assistant: Peux-tu me montrer un exemple ? \n","Human: Sure, here is a simple function. \n","Assistant: Bien sÃ»r, voici une fonction simple. \n","Human: \n","```python\n","def add_numbers(a, b):\n","    return a + b\n","```\n","Assistant: \n","```python\n","def ajouter_nombres(a, b):\n","    return a + b\n","``` \n","Human: Thank you for your help! \n","Assistant: Merci pour ton aide ! \n","Human: You're welcome! \n","Assistant: De rien ! \n","\n","âœ… First trace sent to Arize! Check your dashboard.\n","ğŸ” In Arize, you'll see: input text, output text, latency, and token usage\n"]}],"source":["from langchain_openai import OpenAI\n","\n","# Initialize the ChatOpenAI model with our configuration\n","llm = OpenAI(\n","    api_key=OPENAI_API_KEY,\n","    model=Config.OPENAI_MODEL,\n","    temperature=Config.TEMPERATURE,\n","    max_tokens=Config.MAX_TOKENS\n",")\n","\n","# Test with a simple translation task\n","# This creates a conversation with system context and user input\n","messages = [\n","    (\"system\", \"You are a helpful assistant that translates English to French.\"),\n","    (\"human\", \"I love programming.\"),\n","]\n","\n","# This call will be automatically traced by Arize!\n","# The trace will include: input, output, latency, token usage, and more\n","response = llm.invoke(messages)\n","\n","print(\"ğŸ”¤ Translation Result:\")\n","print(f\"ğŸ“ {response}\")\n","print(\"\\nâœ… First trace sent to Arize! Check your dashboard.\")\n","print(\"ğŸ” In Arize, you'll see: input text, output text, latency, and token usage\")"]},{"cell_type":"markdown","metadata":{"id":"jr94gjCAg5_9"},"source":["## ğŸ­ Step 5: Build the Play Review Generator - Define Prompts\n","\n","### **Purpose:**\n","Create the foundation for our complex workflow using LangChain's prompt templates.\n","\n","### **What This Cell Does:**\n","1. **Imports LangChain components** for building chains\n","2. **Creates ChatOpenAI instance** for our workflow\n","3. **Defines two prompt templates**:\n","   - **Synopsis Prompt**: Converts play title â†’ synopsis\n","   - **Review Prompt**: Converts synopsis â†’ professional review\n","\n","### **Key Concepts:**\n","- **ChatPromptTemplate**: Reusable, parameterized prompts\n","- **SystemMessage**: Sets the AI's role and behavior\n","- **HumanMessage**: Represents user input with variables\n","\n","### **Expected Outcome:**\n","Two prompt templates created and ready for chaining. No output expected from this cell - it's setting up components for the next steps."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qedo2OI3g5_9","executionInfo":{"status":"ok","timestamp":1760075283846,"user_tz":-330,"elapsed":62,"user":{"displayName":"Prathamesh Bonde","userId":"03982584998902270036"}},"outputId":"83858e48-90ed-40ed-bfb7-bedf82aa071c"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Prompt templates created successfully!\n","ğŸ“ Synopsis prompt: Converts title â†’ synopsis\n","ğŸ“ Review prompt: Converts synopsis â†’ professional review\n"]}],"source":["from langchain_core.messages import SystemMessage, HumanMessage\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnableSequence\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# 1. Initialize the modern chat model for our workflow\n","llm = ChatOpenAI(\n","    api_key=OPENAI_API_KEY,\n","    model=Config.OPENAI_MODEL,\n","    temperature=Config.TEMPERATURE\n",")\n","\n","# 2. Define the first prompt template (synopsis generation)\n","# This converts a play title into a detailed synopsis\n","synopsis_prompt = ChatPromptTemplate.from_messages([\n","    SystemMessage(content=\"You are a creative playwright. Given a play title, write a compelling 2-3 paragraph synopsis that captures the essence, themes, and dramatic arc of the play.\"),\n","    HumanMessage(content=\"Title: {title}\")\n","])\n","\n","# 3. Define the second prompt template (review generation)\n","# This converts a synopsis into a professional theater review\n","review_prompt = ChatPromptTemplate.from_messages([\n","    SystemMessage(content=\"You are a seasoned New York Times theater critic. Write a professional, insightful review for a play based on its synopsis. Include commentary on themes, character development, and overall impact.\"),\n","    HumanMessage(content=\"Play Synopsis:\\n{synopsis}\")\n","])\n","\n","print(\"âœ… Prompt templates created successfully!\")\n","print(\"ğŸ“ Synopsis prompt: Converts title â†’ synopsis\")\n","print(\"ğŸ“ Review prompt: Converts synopsis â†’ professional review\")"]},{"cell_type":"markdown","metadata":{"id":"oRnASGuCg6AA"},"source":["## â›“ï¸ Step 6: Create Individual Chain Components\n","\n","### **Purpose:**\n","Build individual processing steps using LangChain's pipe operator (|) for composition.\n","\n","### **What This Cell Does:**\n","1. **Creates synopsis chain**: `prompt | llm | parser`\n","2. **Creates review chain**: `prompt | llm | parser`\n","3. **Uses StrOutputParser**: Extracts clean text from LLM responses\n","\n","### **Chain Composition Pattern:**\n","- **Prompt**: Formats input with template\n","- **LLM**: Processes the formatted prompt\n","- **Parser**: Extracts clean output\n","\n","### **Expected Outcome:**\n","Two individual chains created. These will be combined in the next step to create the full workflow."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pf63L7uNg6AB","executionInfo":{"status":"ok","timestamp":1760075287431,"user_tz":-330,"elapsed":13,"user":{"displayName":"Prathamesh Bonde","userId":"03982584998902270036"}},"outputId":"55b546b8-84c0-4f9c-dc54-b146a8f6cc5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Individual chain components created!\n","ğŸ”— Synopsis chain: title â†’ synopsis\n","ğŸ”— Review chain: synopsis â†’ review\n","ğŸ“‹ Next: We'll combine these into a sequential workflow\n"]}],"source":["# Create individual processing chains using the pipe operator (|)\n","# This is LangChain's powerful composition syntax\n","\n","# Synopsis chain: takes title input, generates synopsis\n","synopsis_chain = synopsis_prompt | llm | StrOutputParser()\n","\n","# Review chain: takes synopsis input, generates review\n","review_chain = review_prompt | llm | StrOutputParser()\n","\n","print(\"âœ… Individual chain components created!\")\n","print(\"ğŸ”— Synopsis chain: title â†’ synopsis\")\n","print(\"ğŸ”— Review chain: synopsis â†’ review\")\n","print(\"ğŸ“‹ Next: We'll combine these into a sequential workflow\")"]},{"cell_type":"markdown","metadata":{"id":"HSZvoY_Hg6AB"},"source":["## ğŸš€ Step 7: Create Full Sequential Chain\n","\n","### **Purpose:**\n","Combine individual chains into a complete workflow where output of one step becomes input of the next.\n","\n","### **What This Cell Does:**\n","1. **Creates data flow mapping**: Extracts title from input\n","2. **Runs synopsis generation**: Title â†’ synopsis\n","3. **Prepares synopsis for review**: Formats synopsis as input\n","4. **Runs review generation**: Synopsis â†’ review\n","\n","### **Sequential Flow:**\n","```\n","Input Title â†’ Synopsis Chain â†’ Review Chain â†’ Final Review\n","```\n","\n","### **Key Concepts:**\n","- **Lambda functions**: Transform data between steps\n","- **Dictionary mapping**: Structure data for next step\n","- **Pipe operator**: Chains operations together\n","\n","### **Expected Outcome:**\n","Complete workflow chain created and ready for testing. This chain will automatically trace each step in Arize."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9rC6lxG-g6AC","executionInfo":{"status":"ok","timestamp":1760075292084,"user_tz":-330,"elapsed":9,"user":{"displayName":"Prathamesh Bonde","userId":"03982584998902270036"}},"outputId":"a1b028a6-3c2a-4d3c-97c4-91bc698cc826"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Full sequential chain created!\n","ğŸ”„ Workflow: Input â†’ Synopsis â†’ Review\n","ğŸ“Š Each step will be automatically traced in Arize\n","ğŸ¯ Ready for testing with real inputs!\n"]}],"source":["# Create the complete sequential workflow\n","# This demonstrates LangChain's powerful composition capabilities\n","\n","full_chain = (\n","    # Step 1: Extract title from input dictionary\n","    {\"title\": lambda x: x[\"input\"]}\n","    # Step 2: Generate synopsis from title\n","    | synopsis_chain\n","    # Step 3: Prepare synopsis for review generation\n","    | {\"synopsis\": lambda x: x}\n","    # Step 4: Generate review from synopsis\n","    | review_chain\n",")\n","\n","print(\"âœ… Full sequential chain created!\")\n","print(\"ğŸ”„ Workflow: Input â†’ Synopsis â†’ Review\")\n","print(\"ğŸ“Š Each step will be automatically traced in Arize\")\n","print(\"ğŸ¯ Ready for testing with real inputs!\")"]},{"cell_type":"markdown","metadata":{"id":"G4x0qqXZg6AD"},"source":["## ğŸ¬ Step 8: Define Test Cases\n","\n","### **Purpose:**\n","Prepare diverse test inputs to demonstrate the workflow's versatility and generate rich traces.\n","\n","### **What This Cell Does:**\n","1. **Defines test scenarios** with different types of content\n","2. **Includes variety**: Documentary, Hollywood film, tech topic\n","3. **Structures input format** for the chain\n","\n","### **Test Cases Explained:**\n","- **Documentary**: Tests handling of real-world, serious topics\n","- **Hollywood**: Tests handling of popular entertainment\n","- **Tech Topic**: Tests handling of non-traditional \"play\" subjects\n","\n","### **Expected Outcome:**\n","Test inputs defined and ready for processing. Each will generate different types of traces in Arize."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DE_YpIy0g6AD","executionInfo":{"status":"ok","timestamp":1760075302073,"user_tz":-330,"elapsed":13,"user":{"displayName":"Prathamesh Bonde","userId":"03982584998902270036"}},"outputId":"8ebcad80-f8c4-4f36-da42-5f4740cac221"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Test cases defined!\n","ğŸ“Š 3 different scenarios ready for testing\n","   1. Environmental documentary - tests serious, real-world topics\n","   2. Popular film adaptation - tests entertainment content\n","   3. Tech topic - tests non-traditional play subjects\n","ğŸ­ Each test will generate unique synopsis and review combinations\n"]}],"source":["# Define diverse test inputs to showcase the workflow's capabilities\n","# Each input will generate different traces and demonstrate various scenarios\n","\n","test_inputs = [\n","    {\n","        \"input\": \"documentary about pandas who are about to be extinct because of global warming\",\n","        \"description\": \"Environmental documentary - tests serious, real-world topics\"\n","    },\n","    {\n","        \"input\": \"once upon a time in hollywood\",\n","        \"description\": \"Popular film adaptation - tests entertainment content\"\n","    },\n","    {\n","        \"input\": \"the best AI observability tooling\",\n","        \"description\": \"Tech topic - tests non-traditional play subjects\"\n","    }\n","]\n","\n","print(\"âœ… Test cases defined!\")\n","print(f\"ğŸ“Š {len(test_inputs)} different scenarios ready for testing\")\n","for i, test in enumerate(test_inputs, 1):\n","    print(f\"   {i}. {test['description']}\")\n","print(\"ğŸ­ Each test will generate unique synopsis and review combinations\")"]},{"cell_type":"markdown","metadata":{"id":"qLS2LOvig6AE"},"source":["## ğŸ¯ Step 9: Execute Workflow and Generate Traces\n","\n","### **Purpose:**\n","Run the complete workflow on all test cases and generate comprehensive traces in Arize.\n","\n","### **What This Cell Does:**\n","1. **Iterates through test cases** one by one\n","2. **Invokes full chain** for each input\n","3. **Displays results** with clear formatting\n","4. **Automatically generates traces** for each execution\n","\n","### **What You'll See:**\n","- **Input**: Original play title/concept\n","- **Review**: Final professional review output\n","- **Processing time**: How long each workflow takes\n","\n","### **In Arize Dashboard:**\n","- **Multiple traces**: One for each test case\n","- **Chain visualization**: See synopsis â†’ review flow\n","- **Performance metrics**: Latency, token usage, costs\n","- **Input/Output data**: Full content of each step\n","\n","### **Expected Outcome:**\n","Three complete reviews generated, each with different creative interpretations. Check your Arize dashboard to see detailed traces of the entire workflow."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GPzm-0aXg6AE","executionInfo":{"status":"error","timestamp":1760075350466,"user_tz":-330,"elapsed":43485,"user":{"displayName":"Prathamesh Bonde","userId":"03982584998902270036"}},"outputId":"41b8e11f-6e3c-4cb3-a40f-ed4190c32b0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ¬ Starting workflow execution...\n","ğŸ“Š Each execution will generate detailed traces in Arize\n","\n","================================================================================\n","\n","ğŸ­ Test Case 1: Environmental documentary - tests serious, real-world topics\n","ğŸ“˜ Input: documentary about pandas who are about to be extinct because of global warming\n","â³ Processing...\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:opentelemetry.exporter.otlp.proto.grpc.exporter:Failed to export traces to otlp.arize.com, error code: StatusCode.PERMISSION_DENIED\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Completed in 25.52 seconds\n","ğŸ“ Review Generated:\n","------------------------------------------------------------\n","**Title: A Glimpse of Tomorrow**\n","\n","**Review:**\n","\n","In the heart of the New York theater scene, \"A Glimpse of Tomorrow\" emerges as a poignant exploration of hope, resilience, and the intricate tapestry of human relationships. Written by the talented newcomer, Clara Jensen, this play deftly navigates the complexities of life in a post-pandemic world, where the characters grapple with their pasts while striving to forge a brighter future.\n","\n","Set in a small, dilapidated community center, the play introduces us to a diverse ensemble of characters, each representing different facets of society. At the center is Maya, a once-aspiring artist whose dreams have been stifled by the weight of loss and disillusionment. Played with raw vulnerability by the remarkable Lila Chen, Maya's journey from despair to self-acceptance is both heart-wrenching and inspiring. Chen's performance captures the essence of a woman on the brink of rediscovery, making her character's evolution resonate deeply with the audience.\n","\n","The supporting cast is equally compelling. We meet Sam, a retired schoolteacher who embodies the wisdom of age but struggles with the loneliness that comes with it. His interactions with Maya serve as a catalyst for her transformation, highlighting the theme of intergenerational connection. The chemistry between Chen and veteran actor Harold Finch is palpable, as they navigate the delicate balance of mentorship and friendship.\n","\n","Jensen's writing shines in its ability to weave together the characters' backstories, revealing how their individual traumas intersect and influence their present lives. The dialogue is sharp and insightful, often laced with humor that provides much-needed levity amidst the heavier themes. The play's exploration of mental health, particularly in the wake of collective trauma, is handled with sensitivity and nuance, making it a timely reflection on the human condition.\n","\n","Thematically, \"A Glimpse of Tomorrow\" delves into the idea of community as a source of strength. The community center, once a hub of activity, becomes a symbol of hope and renewal as the characters come together to reclaim their lives. The play's climax, a community art project that Maya spearheads, serves as a powerful metaphor for healing and collaboration. It is a reminder that even in the darkest of times, the act of creation can foster connection and inspire change.\n","\n","Director Sofia Martinez deserves commendation for her deft handling of the material. The pacing is tight, and the staging is both intimate and dynamic, allowing the audience to feel the weight of each character's journey. The use of lighting and sound design further enhances the emotional landscape, creating an immersive experience that lingers long after the curtain falls.\n","\n","Overall, \"A Glimpse of Tomorrow\" is a triumph of storytelling that resonates with anyone who has ever faced adversity. It is a celebration of the human spirit's capacity for resilience and the importance of community in navigating life's challenges. Clara Jensen has crafted a work that is not only relevant but also deeply moving, marking her as a playwright to watch in the years to come. This play is a must-see for anyone seeking a heartfelt reflection on the power of hope and the beauty of human connection.\n","------------------------------------------------------------\n","\n","================================================================================\n","\n","ğŸ­ Test Case 2: Popular film adaptation - tests entertainment content\n","ğŸ“˜ Input: once upon a time in hollywood\n","â³ Processing...\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:opentelemetry.exporter.otlp.proto.grpc.exporter:Failed to export traces to otlp.arize.com, error code: StatusCode.PERMISSION_DENIED\n","ERROR:opentelemetry.exporter.otlp.proto.grpc.exporter:Failed to export traces to otlp.arize.com, error code: StatusCode.PERMISSION_DENIED\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2687619246.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# 1. Synopsis generation (title â†’ synopsis)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# 2. Review generation (synopsis â†’ review)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_case\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Calculate execution time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3244\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3245\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3246\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3247\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3248\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m         return cast(\n\u001b[1;32m    394\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1024\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m                 results.append(\n\u001b[0;32m--> 842\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    843\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1092\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 )\n\u001b[1;32m   1207\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m                 \u001b[0mraw_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_raw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    983\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import time\n","\n","print(\"ğŸ¬ Starting workflow execution...\")\n","print(\"ğŸ“Š Each execution will generate detailed traces in Arize\\n\")\n","print(\"=\" * 80)\n","\n","# Execute the full workflow for each test case\n","for i, test_case in enumerate(test_inputs, 1):\n","    print(f\"\\nğŸ­ Test Case {i}: {test_case['description']}\")\n","    print(f\"ğŸ“˜ Input: {test_case['input']}\")\n","    print(\"â³ Processing...\")\n","\n","    # Record start time for performance tracking\n","    start_time = time.time()\n","\n","    # Execute the full chain - this will generate traces for:\n","    # 1. Synopsis generation (title â†’ synopsis)\n","    # 2. Review generation (synopsis â†’ review)\n","    result = full_chain.invoke({\"input\": test_case[\"input\"]})\n","\n","    # Calculate execution time\n","    execution_time = time.time() - start_time\n","\n","    print(f\"âœ… Completed in {execution_time:.2f} seconds\")\n","    print(f\"ğŸ“ Review Generated:\")\n","    print(\"-\" * 60)\n","    print(result)\n","    print(\"-\" * 60)\n","\n","    if i < len(test_inputs):\n","        print(\"\\n\" + \"=\" * 80)\n","\n","print(\"\\nğŸ‰ All workflows completed successfully!\")\n","print(\"ğŸ“Š Check your Arize dashboard to explore the generated traces\")\n","print(\"ğŸ” You'll see: input/output data, latency metrics, token usage, and chain visualization\")"]},{"cell_type":"markdown","metadata":{"id":"gX3k2zJ4g6AF"},"source":["## ğŸ“Š Step 10: Arize Dashboard Exploration Guide\n","\n","### **Purpose:**\n","Guide you through exploring the traces and insights in your Arize dashboard.\n","\n","### **What to Look For in Arize:**\n","\n","#### **1. Traces Overview**\n","- Navigate to your project: `langchain-arize-demo-v2`\n","- See all workflow executions as individual traces\n","- Each trace shows the complete synopsis â†’ review pipeline\n","\n","#### **2. Performance Metrics**\n","- **Latency**: How long each step took\n","- **Token Usage**: Input/output tokens for cost tracking\n","- **Success Rate**: All operations should show 100% success\n","\n","#### **3. Chain Visualization**\n","- See the flow: Input â†’ Synopsis Chain â†’ Review Chain â†’ Output\n","- Understand data transformation at each step\n","- Identify bottlenecks or optimization opportunities\n","\n","#### **4. Content Analysis**\n","- **Input Data**: Original play titles\n","- **Intermediate Data**: Generated synopses\n","- **Output Data**: Final reviews\n","- **Prompt Templates**: See how prompts were formatted\n","\n","### **Next Steps:**\n","1. **Explore Traces**: Click on individual traces to see detailed execution\n","2. **Analyze Performance**: Look for patterns in latency and token usage\n","3. **Debug Issues**: If any errors occurred, traces show exactly where\n","4. **Optimize Costs**: Use token usage data to optimize prompts"]},{"cell_type":"markdown","metadata":{"id":"JQMJ9AmBg6AF"},"source":["## ğŸš€ Production Best Practices & Next Steps\n","\n","### **ğŸ”§ Production Recommendations**\n","\n","#### **1. Environment Management**\n","```python\n","# Use different projects for different environments\n","project_name = f\"my-app-{os.getenv('ENVIRONMENT', 'dev')}\"\n","```\n","\n","#### **2. Error Handling & Retries**\n","```python\n","from langchain.callbacks import get_openai_callback\n","from tenacity import retry, stop_after_attempt, wait_exponential\n","\n","@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n","def robust_chain_invoke(chain, input_data):\n","    return chain.invoke(input_data)\n","```\n","\n","#### **3. Cost Monitoring**\n","```python\n","# Track token usage for cost optimization\n","with get_openai_callback() as cb:\n","    result = chain.invoke(input_data)\n","    print(f\"Cost: ${cb.total_cost:.4f}\")\n","    print(f\"Tokens: {cb.total_tokens}\")\n","```\n","\n","#### **4. Advanced Monitoring**\n","```python\n","# Add custom metadata to traces\n","from opentelemetry import trace\n","\n","tracer = trace.get_tracer(__name__)\n","with tracer.start_as_current_span(\"custom_operation\") as span:\n","    span.set_attribute(\"user_id\", \"user123\")\n","    span.set_attribute(\"operation_type\", \"review_generation\")\n","    result = chain.invoke(input_data)\n","```\n","\n","### **ğŸ“š Additional Resources**\n","- [LangChain Documentation](https://python.langchain.com/)\n","- [Arize AI Documentation](https://docs.arize.com/)\n","- [OpenInference Instrumentation](https://github.com/Arize-ai/openinference)\n","- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/)\n","\n","### **ğŸ¯ Suggested Improvements**\n","1. **Add input validation** to handle edge cases\n","2. **Implement caching** for repeated requests\n","3. **Add custom metrics** for business-specific KPIs\n","4. **Set up alerts** for performance degradation\n","5. **Create A/B tests** for different prompt versions\n","\n","### **ğŸ‰ Congratulations!**\n","You've successfully built and monitored a production-ready AI workflow with:\n","- âœ… **LangChain** for workflow orchestration\n","- âœ… **Arize** for comprehensive observability\n","- âœ… **Sequential processing** with automatic tracing\n","- âœ… **Performance monitoring** and debugging capabilities\n","\n","Your AI application is now ready for production deployment with full observability!"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}