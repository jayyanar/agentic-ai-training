{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04445cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"../data\")\n",
    "file_path = data_path / \"Efficient Large Language Models- A Survey.pdf\"\n",
    "loader = PyPDFLoader(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349269df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 0, 'page_label': '1'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient Large Language Models: A Survey\\nZhongwei Wan∗ wan.512@osu.edu\\nXin Wang∗ wang.15980@osu.edu\\nChe Liu† che.liu21@imperial.ac.uk\\nSamiul Alam∗ alam.140@osu.edu\\nYu Zheng‡ zhengy30@msu.edu\\nJiachen Liu§ amberljc@umich.edu\\nZhongnan Qu¶‡‡ znqu@amazon.com\\nShen Yan∥ shenyan@google.com\\nYi Zhu†† yi@boson.ai\\nQuanlu Zhang∗∗ quzha@microsoft.com\\nMosharaf Chowdhury§ mosharaf@umich.edu\\nMi Zhang∗ mizhang.1@osu.edu\\n∗The Ohio State University †Imperial College London ‡Michigan State University §University of\\nMichigan ¶Amazon AWS AI ∥Google Research ∗∗Microsoft Research Asia ††Boson AI\\nReviewed on OpenReview:https://openreview.net/forum?id=bsCCJHbO8A\\nAbstract\\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in important\\ntasks such as natural language understanding and language generation, and thus have the\\npotential to make a substantial impact on our society. Such capabilities, however, come with\\nthe considerable resources they demand, highlighting the strong need to develop effective\\ntechniques for addressing their efficiency challenges. In this survey, we provide a systematic\\nand comprehensive review of efficient LLMs research. We organize the literature in a taxon-\\nomy consisting of three main categories, covering distinct yet interconnected efficient LLMs\\ntopics from model-centric, data-centric, and framework-centric perspective, respectively. We\\nhave also created a GitHub repository where we organize the papers featured in this survey\\nat https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain\\nthe repository and incorporate new research as it emerges. We hope our survey can serve as\\na valuable resource to help researchers and practitioners gain a systematic understanding of\\nefficient LLMs research and inspire them to contribute to this important and exciting field.\\n‡‡The work is done outside Amazon.\\n1'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 1, 'page_label': '2'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\n1 Introduction\\nLarge Language Models (LLMs) are a type of advanced AI models designed to understand and generate\\nhuman languages. Recently, we have witnessed a surge in LLMs including those developed by Open AI\\n(GPT-4 (Achiam et al., 2023) and GPT-3 (Brown et al., 2020)), Meta (LLaMA-3 (Meta, 2024), LLaMA-\\n2 (Touvron et al., 2023b), LLaMA-1 (Touvron et al., 2023a)), and Google (Gemini (Team & Google, 2023),\\nPaLM-2 (Anil et al., 2023), PaLM (Chowdhery et al., 2022), GLaM (Du et al., 2022)) as well as many other\\nmodels such as BLOOM (Scao et al., 2023), PanGu-∑ (Ren et al., 2023b), and GLM (Zeng et al., 2023).\\nThese models have demonstrated remarkable performance across a variety of tasks such as natural language\\nunderstanding (NLU), language generation, complex reasoning (Yang et al., 2024), and domain-specific tasks\\nrelated to biomedicine (He et al., 2023; Wan et al., 2023; 2022), law (Eliot, 2021) and code generation (Wei\\net al., 2022b; Chen et al., 2021b). Such performance breakthroughs can be attributed to their massive scales\\nin model sizes and volumes of training data, as they contain billions or even trillions of parameters while\\nbeing trained on a gigantic amount of data from diverse sources.\\nAlthough LLMs are leading the next wave of AI revolution, their remarkable capabilities come at substantial\\nresource demands (Achiam et al., 2023; Du et al., 2022; Chowdhery et al., 2022; Ren et al., 2023b). Figure 1\\nillustrates the relationship between model performance and model training time in terms of GPU hours for\\nLLaMA series, where the size of each circle is proportional to the number of model parameters. As shown,\\nalthough larger models are able to achieve better performance, the amounts of GPU hours used for training\\nthem grow exponentially as model sizes scale up. In addition to training, inference also contributes quite\\nsignificantly to the operational cost of LLMs. Figure 2 depicts the relationship between model performance\\nand inference throughput. Similarly, scaling up the model size enables better performance but comes at\\nthe cost of lower inference throughput (higher inference latency), presenting challenges for these models in\\nexpanding their reach to a broader customer base and diverse applications in a cost-effective way.\\nThe high resource demands of LLMs highlight the strong need to develop techniques to enhance the efficiency\\nof LLMs. As shown in Figure 2, compared to LLaMA-1-33B, Mistral-7B (Jiang et al., 2023a), which uses\\ngrouped-query attention and sliding window attention to speed up inference, achieves comparable perfor-\\nmance and much higher throughput. This superiority highlights the feasibility and significance of designing\\nefficiency techniques for LLMs.\\nThe overarching goal of this survey is to provide a holistic view of the technological advances in efficient\\nLLMs. AsillustratedinFigure3, weorganizetheliteratureinataxonomyconsistingofthreemaincategories,\\ncovering efficient LLMs topics frommodel-centric, data-centric, and framework-centric perspective,\\nrespectively. These three categories cover distinct yet interconnected research topics, collectively providing\\na systematic and comprehensive review of efficient LLMs research. Specifically,\\n• Model-Centric Methods: Model-centric methods focus on bothalgorithm-level and system-\\nlevel efficient techniques where the model itself is the focal point. With billions or even trillions\\nof parameters, LLMs exhibit distinct characteristics (Wei et al., 2022a) compared to smaller-scale\\nmodels, necessitating the development of new techniques to enhance their efficiency. In §2, we survey\\nefficient techniques that cover research directions related to model compression, efficient pre-training,\\nefficient fine-tuning, efficient inference, and efficient architecture design.\\n• Data-Centric Methods: In the realm of LLMs, the importance of data is as crucial as that of\\nthe model itself. Data-centric methods focus on the role of the quality and structure of data in\\nenhancing the efficiency of LLMs. In §3, we survey efficient techniques that cover research directions\\nrelated to data selection and prompt engineering.\\n• LLM Frameworks: The advent of LLMs necessitates the development of specialized frameworks\\nto efficiently handle their training, fine-tuning, inference, and serving. While mainstream AI frame-\\nworks such as TensorFlow and PyTorch provide the foundations, they lack built-in support for spe-\\ncific optimizations and features crucial for LLMs. In §4, we survey existing frameworks specifically\\ndesigned for efficient LLMs, covering their unique features, underlying libraries, and specializations.\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 2, 'page_label': '3'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\n60 62 64 66 68 70 72 74\\nPerformance (Commonsense Reasoning Score)\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\nTraining Time (Million GPU hours)LLaMA-1-7B LLaMA-1-13B\\nLLaMA-1-33B\\nLLaMA-1-65B\\nLLaMA-2-7B\\nLLaMA-2-13B\\nLLaMA-2-34B\\nLLaMA-2-70B5B 10B 25B 50B 75B\\nNumber of model parameters\\nFigure 1: Illustration of model performance and model training time in GPU hours of LLaMA models at dif-\\nferent scales. The reported performance is the average score of several commonsense reasoning benchmarks.\\nThe training time is based on Nvidia A100 80GB GPU. The size of each circle corresponds to the number\\nof model parameters. The original data can be found in Touvron et al. (2023a;b).\\n30 35 40 45 50 55\\nHuggingFace Open LLM Leaderboard Score (%)\\n20\\n30\\n40\\n50\\n60\\n70\\n80Throughput (tokens/s)\\nLLaMA-1-33BOPT-30B\\nGPT-NeoX-20B\\nCodeGen-NL-16B\\nLLaMA-2-13B\\nLLaMA-1-13BOPT-13B\\nXGLM-7.5B\\nMistral-7B\\nCodeGen-NL-6B\\nLLaMA-2-7BLLaMA-1-7B\\nOPT-6.7B\\nCerebras-\\nGPT-6.7B\\nMPT-7B\\nPythia-6.9B\\nXGLM-4.5B\\nOPT-2.7B\\nCerebras-\\nGPT-2.7B\\nCerebras-\\nGPT-1.3B\\n5GB 10GB 50GB 80GB\\nMemory\\nFigure 2: Performance scorevs. inference throughput for various LLMs. The throughputs are measured on\\nNvidia A100 80GB GPU with 16-bit floating point quantization. The size of each circle corresponds to the\\nmemory footprint (in Gigabytes) of each model when running with a batch size of 1, prompt size of 256, and\\ngenerating 1000 tokens. The original data can be found in Ilyas Moutawwakil (2023).\\nIn addition to the survey, we have established aGitHub repositorywhere we compile the papers featured\\nin this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain it\\nand incorporate new research as it emerges.\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 3, 'page_label': '4'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient LLMs Methods\\nModel-Centric(§2)\\nModel Compression(§2.1)\\nQuantization Post-Training QuantizationWeight-Only Quantization\\nWeight-Activation Co-QuantizationQuantization-Aware Training\\nParameter PruningStructured Pruning\\nUnstructured PruningLow-Rank Approximation\\nKnowledge DistillationWhite-Box KD\\nBlack-Box KD\\nEfficient Pre-Training(§2.2)\\nMixed Precision Training\\nScaling Models\\nInitialization Techniques\\nTraining Optimizers\\nSystem-Level Pre-TrainingEfficiency Optimization\\nEfficient Fine-Tuning(§2.3)\\nParameter-EfficientFine-Tuning\\nLow-Rank Adaptation\\nAdapter-based Tuning\\nPrefix Tuning\\nPrompt TuningMemory-Efficient Fine-Tuning\\nEfficient Inference(§2.4)\\nAlgorithm-LevelInference Acceleration\\nSpeculative Decoding\\nKV-Cache Optimization\\nSystem-Level InferenceAcceleration\\nEfficient Architecture(§2.5)\\nEfficient Attention\\nSharing-based Attention\\nKernelization or Low-Rank\\nFixed Pattern Strategies\\nLearnable Pattern Strategies\\nHardware-Assisted Attention\\nMixture of Experts (MoE)\\nMoE-based LLMs\\nAlgorithm-Level MoE Optimization\\nSystem-Level MoE Optimization\\nLong Context LLMs\\nExtrapolation and Interpolation\\nRecurrent Structure\\nSegmentation and Sliding Window\\nMemory-Retrieval Augmentation\\nTransformer-AlternativeArchitectures\\nState Space Models\\nOther Sequential Models\\nData-Centric(§3)\\nData Selection(§3.1)\\nData Selection forEfficient Pre-Training\\nData Selection forEfficient Fine-Tuning\\nPrompt Engineering(§3.2)\\nFew-Shot Prompting\\nDemonstration OrganizationDemonstration Selection\\nDemonstration Ordering\\nTemplate FormattingInstruction Generation\\nMulti-Step ReasoningPrompt Compression\\nPrompt Generation\\nFrameworks(§4) DeepSpeed, Megatron, Colossal-AI, Nanotron, MegaBlocks, FairScale, Pax, Composer,OpenLLM, LLM Foundry, vLLM, TensorRT-LLM, TGI, RayLLM, MLC LLM, Sax, Mosec\\nFigure 3: Taxonomy of efficient large language models (LLMs) literature.\\nAlthough there are a few surveys on LLMs (Zhao et al., 2023a; Chang et al., 2024; Wang et al., 2023i;\\nKaddour et al., 2023), this survey provides a focused review and discussion on the literature related to the\\nefficiency aspect of LLMs. There are also surveys on efficient Transformers (Tay et al., 2022) and their\\ntraining methods (Zhuang et al., 2023). In contrast, this survey specifically focuses on efficiency techniques\\ndesigned for models of more than billions of parameters. We hope this survey together with the GitHub\\nrepository can help researchers and practitioners navigate through the literature and serve as a catalyst for\\ninspiring further research on efficient LLMs.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 4, 'page_label': '5'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nModel Compression\\nQuantization\\nPost-Training Quantization\\nWeight-Only Quantization\\nLLM.int8() (Dettmers et al., 2022), GPTQ (Frantar et al., 2023),\\nOBQ (Frantar & Alistarh, 2022), QuIP (Chee et al., 2023),\\nAWQ (Lin et al., 2023), OWQ (Lee et al., 2023),\\nSpQR (Dettmers et al., 2024), FineQuant (Kim et al., 2023d)\\nWeight-Activation Co-Quantization\\nZeroQuant (Yao et al., 2022b), ZeroQuant-FP (Wu et al., 2023b),\\nZeroQuant-V2 (Yao et al., 2023d), SmoothQuant (Xiao et al., 2023),\\nOliVe (Guo et al., 2023), RPTQ (Yuan et al., 2023a),\\nAhmadian et al. (2023), Outlier Suppression+ (Wei et al., 2023),\\nQLLM (Liu et al., 2024c)\\nQuantization-Aware TrainingQuantGPT (Tao et al., 2022), LLM-QAT (Liu et al., 2023d), BitNet (Wang et al., 2023b)\\nParameter Pruning\\nStructured PruningLLM-Pruner (Ma et al., 2023), Sheared LLaMA (Xia et al., 2023), LoRAPrune (Zhang et al., 2023c),\\nLoRAShear (Chen et al., 2023e), Deja Vu (Liu et al., 2023f)\\nUnstructured PruningSparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2024), Shao et al. (2024)\\nLow-Rank ApproximationTensorGPT (Xu et al., 2023a), LoSparse (Li et al., 2023g), FWSVD (Hsu et al., 2022), ASVD (Yuan et al., 2023b), SVD-LLM (Wang et al., 2024c)\\nKnowledge Distillation\\nWhite-Box KDBaby LLaMA (Timiryasov & Tastet, 2023), MiniLLM (Gu et al., 2024), KPTD (Padmanabhan et al., 2023),\\nTED (Liang et al., 2023), TSLD (Kim et al., 2023b), MiniMA (Zhang et al., 2023a), GKD (Agarwal et al., 2024)\\nBlack-Box KD\\nMetaICL (Min et al., 2022a), Multitask-ICT (Huang et al., 2022), Li et al. (2024b), Lion (Jiang et al., 2023b),\\nDISCO (Chen et al., 2023j), Fu et al. (2023b), Distilling Step-by-Step (Hsieh et al., 2023),\\nFine-tune-CoT (Ho et al., 2023), SOCRATIC CoT (Shridhar et al., 2023), SCOTT (Wang et al., 2023c),\\nSCoTD (Li et al., 2023b), Peng et al. (2023a), Zephyr (Tunstall et al., 2023)\\nFigure 4: Summary of model compression techniques for LLMs.\\n2 Model-Centric Methods\\n2.1 Model Compression\\nModel compression enhances efficiency by reducing the sizes and the amount of arithmetic operations of\\nLLMs. Unlike conventional model compression techniques, most LLM compression approaches are designed\\nunder the post-training setting to avoid resource-intensive retraining. As summarized in Figure 4, model\\ncompression techniques for LLMs can be grouped into four categories: quantization, parameter pruning,\\nlow-rank approximation, and knowledge distillation. These four categories are orthogonal to each other, and\\ncompress LLMs from different perspectives.\\n2.1.1 Quantization\\nQuantization compresses LLMs by converting model weights and/or activations of high-precision data types\\nXH such as 32-bit floating point into low-precision data typesXL such as 8-bit integer (Dettmers et al.,\\n2023) as:\\nXL = Round\\n(\\nabsmax\\n(\\nXL)\\nabsmax (XH) XH\\n)\\n= Round\\n(\\nK·XH)\\n, (1)\\nwhere Round denotes mapping a floating point number into an approximate integer;absmax denotes the\\nabsolute maximum of the input elements; andKdenotes the quantization constant. Quantization techniques\\nfor LLMs can be classified into post-training quantization (PTQ) and quantization-aware training (QAT).\\nCompared to other LLM compression methods such as parameter pruning and low-rank approximation,\\nquantization methods have been shown to achieve superior compression-accuracy trade-offs (Li et al., 2024c).\\nPost-Training Quantization (PTQ).PTQ quantizes LLMs after the model has been trained. To com-\\npensate for the accuracy drop, PTQ uses a small calibration dataset to update the quantized weights and/or\\nactivations. PTQ in general can be grouped into two categories: weight-only quantization, and weight-\\nactivation co-quantization.\\n• Weight-Only Quantization focuses on quantizing model weights only. For example, Dettmers\\net al. (2022) introduce the first multi-billion-scale 8-bit integers (or INT8) weight quantization\\nmethod named LLM.int8() that significantly reduces memory usage during inference while being able\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 5, 'page_label': '6'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTeacher Model\\n(Transparent)\\xa0 Student Model\\xa0\\nWhite-\\nBox KD\\nBlack-\\nBox KD\\nTraining Data Training Data\\nHigh-Precision Weight\\nwith Different Values\\nLow-Precision Weight\\nwith Different Values\\nZero Weight\\xa0 /\\nActivation Value\\nLow-Precision Activation\\nwith Different Values\\nupdate \\nTraining\\nData\\nQAT\\nPTQ (optional) \\n(a) Quantization\\nStructured Unstructured\\n(b) Parameter Pruning\\nX \\nV T \\nDecompose \\nU \\n(c) Low-Rank Approximation\\n (d) Knowledge Distillation\\n+ \\n+ \\nHigh-Precision Activation\\nwith Different Values\\nCalibration\\nData\\nupdate \\nupdate \\nCalibration\\nData\\nupdate \\n+ \\nTeacher Model\\n(Hidden)\\xa0\\nFigure 5: Illustrations of model compression techniques for LLMs.\\nto maintain the performance of the full-precision model. Frantar et al. (2023) push one step further\\nand propose GPTQ, a post-training weight quantization method that compresses LLM weights to\\n3 or 4 bits instead of 8 bits. GPTQ employs layer-wise quantization with Optimal Brain Quanti-\\nzation (OBQ) (Frantar & Alistarh, 2022) to update weights with inverse Hessian information. This\\ntechnique enables quantizing GPT models with 175 billion parameters in roughly four GPU hours\\nwith minimal accuracy drop compared to the original model. Furthermore, driven by the insights\\nthat quantization can be more effective when model weights and proxy Hessian matrices are incoher-\\nent, Chee et al. (2023) propose QuIP, a post-training quantization method that applies incoherence\\nprocessing to quantize LLMs to 2 bits per weight. As another line of research under weight-only\\nquantization, Lin et al. (2023) observe that there exists a small subset of model weights, character-\\nized by larger activation magnitudes, known as salient weights, play a crucial role in determining\\nthe quantization loss. Based on this observation, they propose an approach named activation-aware\\nweight quantization (AWQ) to quantize LLMs while preserving the salient weights in high pre-\\ncision, demonstrating superior performance over GPTQ. Similarly, Lee et al. (2023) observe that\\nactivation outliers amplify weight quantization loss. They propose outlier-aware weight quantiza-\\ntion (OWQ) to identify those vulnerable weights with activation outliers and allocate high-precision\\nto them. Dettmers et al. (2024) introduce Sparse-Quantized Representation (SpQR) to separate\\noutlier weights that are prone to large quantization errors. These outlier weights are preserved at\\nhigher precision, while the remaining weights are compressed to 3-4 bits. Additionally, they intro-\\nduce a decoding scheme tailored for the SpQR format that enhances the efficiency of inference on\\na token-by-token basis. Lastly, Kim et al. (2023d) aim to address the issue of outliers that distort\\nthe distribution of quantized weights, and propose FineQuant that employs an empirically crafted,\\nheuristic-based approach to allocate varying levels of granularity to different weight matrices.\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 6, 'page_label': '7'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\n• Weight-Activation Co-Quantizationdiffers from weight-only quantization in the sense that it\\nquantizes both model weights and activations. For instance, Yao et al. (2022b) propose ZeroQuant,\\nwhich combines group-wise quantization for model weights and token-wise quantization for activa-\\ntions. However, ZeroQuant falls short in maintaining accuracy for models with more than 175 billion\\nparameters. To address this issue, Yao et al. (2023d) and Wu et al. (2023b) propose ZeroQuant-FP\\nand ZeroQuant-V2 respectively, both of which utilize low-rank matrices to recover the accuracy\\ndrop. A key challenge of weight-activation co-quantization is that due to the existence of outliers,\\nactivations are more difficult to quantize than model weights (Bondarenko et al., 2021). To address\\nthis challenge, Xiao et al. (2023) propose SmoothQuant which introduces a per-channel scaling trans-\\nformation that migrates the quantization difficulty from activations to weights to achieve lossless\\nquantization of weights and activations to 8 bits for LLMs up to 530 billion parameters. Guo\\net al. (2023) pinpoint that outliers are critical in weight-activation co-quantization but their nearby\\nnormal values are not. Given that, they propose OliVe, which prunes normal values adjacent to\\nthe outliers so that the outliers can be encoded with higher precision. Yuan et al. (2023a) iden-\\ntify the challenge of quantizing activations when different channels have disparate ranges. They\\npropose RPTQ, which groups channels in activations that have similar value ranges and applies\\nuniform quantization parameters to the values in each group. Ahmadian et al. (2023) demonstrate\\nthat it is possible to suppress large activation outliers at scales as large as 52B. Given the right\\noptimization choices during pre-training, they can quantize models ranging in size from 410M to\\n52B with minimal accuracy degradation. Wei et al. (2023) observe that the activation outliers in\\nLLMs are asymmetric and tend to cluster in particular channels. Based on this observation, they\\npropose Outlier Suppression+, which introduces operations that shift and scale channels individually\\nto neutralize asymmetric outliers. Lastly, Liu et al. (2024c) propose QLLM, an adaptive channel\\nreassembly method that tackles activation outliers and utilizes calibration data to offset the in-\\nformation loss incurred from quantization. Experimental result shows that QLLM achieves better\\ncompression performance than SmoothQuant and Outlier Suppression+ on LLaMA model family.\\nQuantization-Aware Training (QAT).Different from PTQ, QAT quantizes LLMs during the training\\nprocess, allowing LLMs to learn quantization-friendly representations. Since QAT requires training using\\nthe complete training dataset, it is much more expensive and time consuming than PTQ. Tao et al. (2022)\\npropose QuantGPT, which combines contrastive distillation from a full-precision teacher model and logit\\ndistillation to a quantized student model during autoregressive pretraining. QuantGPT achieves 14.4× and\\n13.4× compression rates on GPT-2 and BART with comparable performance with the full-precision models.\\nLLM-QAT (Liu et al., 2023d) uses data generated by LLMs itself to distill knowledge with the objective of\\nquantizing a student model. Specifically, LLM-QAT retains the original output distribution and is capable of\\nquantizing a model irrespective of its initial training data. Besides quantizing weights and activations, LLM-\\nQAT also quantizes the key-value cache, a crucial step for enhancing throughput and accommodating long\\nsequence dependencies in LLMs. Experimental results show that LLM-QAT achieves better performance\\nover training-free methods especially in low-bit settings. Lastly, BitNet (Wang et al., 2023b) pioneers QAT\\nfor 1-bit LLMs. It proposes to use low-precision binary weights and quantized activations while keeping\\noptimizer states and gradients high-precision during training. Experimental results show that compared to\\nFP16 Transformer baselines, BitNet is able to achieve competitive performance while substantially reducing\\nmemory footprint and energy consumption.\\n2.1.2 Parameter Pruning\\nParameter pruning compresses LLMs by removing redundant or less important model weights. Parameter\\npruning methods for LLMs can be categorized into structured pruning and unstructured pruning.\\nStructured Pruning.Structured pruning focuses on pruning structured patterns such as groups of consec-\\nutive parameters or hierarchical structures such as rows, columns, or sub-blocks of the LLM weight matrices.\\nFor instance, LLM-Pruner (Ma et al., 2023) introduces a task-agnostic structured pruning strategy that se-\\nlectively eliminates non-essential interconnected structures using gradient information. LLM-Pruner utilizes\\na small amount of data to obtain the weight, parameter, and group importance of the coupled structure\\nfor LLaMA (Touvron et al., 2023a), and uses LoRA (Hu et al., 2022) to recover accuracy after pruning,\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 7, 'page_label': '8'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nshowing competitive zero-shot performance. Sheared LLaMA (Xia et al., 2023), on the other hand, proposes\\ntwo techniques to improve the performance of LLM-Pruner. The first technique, named targeted struc-\\ntured pruning, prunes a larger model to a designated target shape by eliminating layers, heads, intermediate\\nand hidden dimensions in an end-to-end manner. The second technique, named dynamic batch loading,\\ndynamically configures the composition of sampled data in each training batch based on losses in various\\ndomains. Through these two techniques, Sheared LLaMA is able to prune LLaMA2-7B down to 1.3B pa-\\nrameters, achieving superior compression ratio compared to LLM-Pruner. LoRAPrune (Zhang et al., 2023c)\\nintroduces a LoRA-based pruning criterion using LoRA’s weights and gradients for importance estimation.\\nBy employing an iterative structure pruning process to eliminate excess channels and heads, LoRAPrune\\nachieves better efficiency over LLM-Pruner at 50% compression rate. Lastly, given the input, Deja Vu (Liu\\net al., 2023f) predicts a small set of attention heads and MLP parameters, referred to as contextual sparsity,\\nyields approximately the same output as the dense model. By exploiting such contextual sparsity, Deja Vu\\nis able to achieve much lower latency compared to FasterTransformer without accuracy drop.\\nUnstructured Pruning.Unstructured pruning, on the other hand, focuses on pruning model weights indi-\\nvidually. Compared to structured pruning, unstructured pruning has much more pruning flexibility and thus\\nenjoys a lower accuracy drop. However, unstructured pruning incurs irregular sparsification, which in general\\nmakes the resulting pruned models difficult to be deployed on hardware except specific types of hardware\\nsuch as Nvidia Ampere GPUs (Busato & Pool, 2020). For instance, Frantar & Alistarh (2023) introduce\\nSparseGPT, an one-shot LLM unstructured pruning approach that does not require retraining. SparseGPT\\nformulates pruning as a sparse regression problem and solves it by utilizing an approximate solver based\\non the inversion of the Hessian matrix. In doing so, SparseGPT reaches about 60% unstructured sparsity\\non models such as OPT-135B while experiencing only a slight performance drop. Sun et al. (2024) propose\\nWanda, which prunes weights based on the product values of weight magnitudes and their respective input\\nactivations. Compared to SparseGPT, Wanda neither relies on second-order information nor necessitates\\nweight update, and is able to achieve competitive performance. Shao et al. (2024) improve the performance\\nof SparseGPT in another way. Specifically, instead of performing the unstructured pruning with a unified\\nratio for every layer, they propose to utilize Hessian sensitivity-aware mixed sparsity pruning to achieve a\\nminimum of 50% sparsity in LLMs without retraining. This method adaptively assigns sparsity based on\\nsensitivity to minimize the error induced by pruning while preserving the overall level of sparsity.\\n2.1.3 Low-Rank Approximation\\nLow-rank approximation compresses LLMs by approximating the LLM weight matrixWm×n with smaller\\nlow-rank matricesU and V such thatW ≈UV⊤, where U ∈Rm×r, V ∈Rn×r, and r is typically much\\nsmaller than m,n. In doing so, low-rank approximation reduces the number of parameters and enhances\\nefficiency. For example, Xu et al. (2023a) introduce TensorGPT which compresses the embedding layers of\\nLLMs using Tensor-Train Decomposition (TTD). It transforms and breaks down each token embedding and\\ncreates an efficient embedding format named Matrix Product State (MPS) that can be efficiently computed\\nin a distributed manner. LoSparse (Li et al., 2023g) improves the performance of TensorGPT by compressing\\nthe coherent and expressive components within neurons through low-rank approximation while eliminating\\nthe incoherent and non-expressive elements through pruning. As another line of research, FWSVD (Hsu\\net al., 2022) compresses the weight matrix of an LLM instead of the token embedding matrix via low-\\nrank approximation. Specifically, instead of using vanilla singular value decomposition (SVD), FWSVD\\nproposes a weighted SVD approach which uses Fisher information to weigh the importance of the weights for\\ncompression. While FWSVD demonstrates competitive compression results under low compression ratios, it\\nrequires calculating the gradients based on the training dataset of the target task to estimate the importance\\nscores, which is task-specific and demands significant computation resources. In contrast, ASVD (Yuan\\net al., 2023b) proposes a training-free SVD-based approach. It scales the weight matrix based on the\\nactivation distribution that enhances the decomposition accuracy and efficiency for model compression.\\nHowever, neither FWSVD nor ASVD directly correlate singular values with compression loss. Consequently,\\ntruncating the smaller singular values might result in increased compression loss. SVD-LLM (Wang et al.,\\n2024c) addresses this drawback by incorporating a truncation-aware data whitening strategy that establishes\\na direct mapping between singular values and compression loss. Experimental results demonstrate the\\nsuperiority of SVD-LLM over FWSVD and ASVD in terms of compression performance and speed.\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 8, 'page_label': '9'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\n2.1.4 Knowledge Distillation\\nKnowledge Distillation (KD) compresses LLMs by transferring knowledge from a large teacher LLM to a\\nsmaller student LLM. Though effective, compared to other LLM compression methods, knowledge distillation\\nmethods incur a resource-demanding distillation process. In general, KD for LLMs can be categorized into\\nwhite-box KD methods and black-box KD methods.\\nWhite-Box Knowledge Distillation. White-box KD refers to KD techniques where the parameters\\nor logits of the teacher LLM are used in the distillation process (Gou et al., 2021). For example, as a\\npioneering effort in this direction, Baby LLaMA (Timiryasov & Tastet, 2023) trains an ensemble of GPT-2\\nand a collection of smaller LLaMA models using the BabyLM dataset of 10M words. This ensemble is\\nthen distilled into a compact LLaMA model with 58 million parameters, which outperforms both its original\\nteachermodelsaswellasacomparablemodelthatwastrainedwithouttheuseofdistillation. Guetal.(2024)\\nobserve that conventional KD objectives, such as Kullback-Leibler divergence (KLD), may not be suited for\\nopen text generation tasks due to their complex output spaces compared to classification tasks. To address\\nthis issue, they propose MiniLLM which minimizes reverse KLD using the gradient of the objective function\\nthrough policy gradient techniques (Sutton et al., 1999). Experimental results show that MiniLLM achieves\\nbetter accuracy than conventional KD or directly fine-tuning student models. KPTD (Padmanabhan et al.,\\n2023) demonstrates that white-box KD can transfer and disseminate knowledge from entity definitions into\\nthe parameters of a pre-trained language model. Specifically, KPTD creates a transfer set by prompting\\nthe language model to generate text based on the definition of the entity. The model parameters are then\\nupdated to align the distribution of the student model with that of the teacher model. TED (Liang et al.,\\n2023) introduces a technique for layer-specific task distillation. It uses specially designed filters to align the\\ninternal states of both student and teacher models in each layer. These filters extract relevant knowledge\\nfrom the internal states that is beneficial for the specific task. TED shows considerable and steady gains in\\nperformance on both continual pre-training and fine-tuning. TSLD (Kim et al., 2023b) leverages token-level\\ndistillation to enhance QAT. It addresses the limitations of layer-to-layer KD in token prediction recovery\\nby reforming intermediate representation and has successfully applied QAT to LLMs. MiniMA (Zhang\\net al., 2023a) proposes a viewport towards the capacity gap in distilling LLMs, converting it into a principle\\nthrough analysis and introducing a 3B Language Model that sets a new benchmark for compute-performance\\npareto frontier. Experimental results show that MiniMA achieves the best accuracy compared to other 3B\\ndistilled LLMs. Lastly, Generalized knowledge distillation (GKD) (Agarwal et al., 2024) addresses the issue\\nof distribution mismatch by drawing output sequences from the student model during training. GKD can\\nbe applied in combination with other distillation methods to improve their compression performance.\\nBlack-Box Knowledge Distillation.Different from white-box KD, in black-box KD, only the outputs\\ngenerated from the teacher LLM are used in the distillation process. Inspired by ICT (Chen et al., 2022c) and\\nMetaICL (Min et al., 2022a), where the language model is meta-trained under a wide range of tasks using\\nin-context learning objectives and then fine-tuned for unseen tasks through in-context learning, Multitask-\\nICT (Huang et al., 2022) introduces a concept known as in-context learning distillation to transfer the\\nfew-shot learning capabilities from the teacher model to the student model. Experimental results show\\nthat under Multitask-ICT, in-context learning objectives achieve the best performance when combined with\\nlanguage modeling objectives. Similarly, Li et al. (2024b) introduce a hybrid prompting technique that\\nemploys multi-task learning along with explanations generated by GPT-3 text-davinci-002 version (OpenAI,\\n2023). This method is used to distill explanations into smaller models, achieving consistent and significant\\nimprovements over strong single-task fine-tuning benchmarks in different scenarios. Experiments on multiple\\nreasoning tasks show that this method even perform better than finetuning or prompting a 60x larger\\nGPT-3 (175B) model by up to 9.5% in accuracy. Lion (Jiang et al., 2023b) introduces an adversarial\\ndistillation architecture aimed at enhancing the efficiency of knowledge transfer by incrementally improving\\nthe skill level of the student model. Specifically, it prompts LLMs to recognize challenging instructions\\nand creates new complex instructions for the student model, thereby establishing a three-phase adversarial\\ncycle involving imitation, discrimination, and generation. Experimental results show that Lion-13B not only\\nachieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional instruction-\\ntuned models. DISCO (Chen et al., 2023j) prompts a general LLM to produce phrasal perturbations. These\\ngenerated perturbations are then filtered by a specialized teacher model to distill high-quality counterfactual\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 9, 'page_label': '10'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTable 1: Pre-training costs of representative LLMs.\\nModel Parameter SizeData ScaleGPUs CostTraining Time\\nGPT-3 (Brown et al., 2020) 175B 300B tokens - -\\nGPT-NeoX-20B (Black et al., 2022)20B 825GB corpus96 A100-40G -\\nOPT (Zhang et al., 2022a) 175B 180B tokens992 A100-80G -\\nBLOOM (Scao et al., 2023) 176B 366B tokens384 A100-80G 105 days\\nGLM (Zeng et al., 2023) 130B 400B tokens786 A100-40G 60 days\\nLLaMA (Touvron et al., 2023a)65B 1.4T tokens2048 A100-80G 21 days\\nLLaMA-2 (Touvron et al., 2023b)70B 2T tokens A100-80G 71,680 GPU days\\nGopher (Rae et al., 2022) 280B 300B tokens 1024 A100 13.4 days\\nLaMDA (Thoppilan et al., 2022)137B 768B tokens 1024 TPU-v3 57.7 days\\nGLaM (Du et al., 2022) 1200B 280B tokens 1024 TPU-v4 574 hours\\nPanGu-α(Zeng et al., 2021) 13B 1.1TB corpus2048 Ascend 910 -\\nPanGu-∑(Ren et al., 2023b) 1085B 329B tokens512 Ascend 910 100 days\\nPaLM (Chowdhery et al., 2022)540B 780B tokens 6144 TPU-v4 -\\nPaLM-2 (Anil et al., 2023) - 3.6T tokens TPUv4 -\\nWeLM (Su et al., 2023a) 10B 300B tokens128 A100-40G 24 days\\nFlan-PaLM (Chung et al., 2022)540B - 512 TPU-v4 37 hours\\nAlexaTM (Soltan et al., 2022)20B 1.3 tokens 128 A100 120 days\\nCodegeex (Zheng et al., 2023)13B 850 tokens1536 Ascend 910 60 days\\nMPT-7B (Team, 2023) 7B 1T tokens - -\\ndata into smaller student models, allowing the smaller models to learn causal representations more reliably.\\nAs another line of research, some studies have shown that chain-of-thought (CoT) prompting can elicit\\nlanguage models to solve complex reasoning tasks step by step, with the aim to transfer such ability from\\nlarge models into smaller ones through black-box KD. For example, to enhance the CoT math reasoning\\ncapabilities of smaller models, Fu et al. (2023b) propose a method for instruct-tuning a student model\\n(FlanT5) by distilling the reasoning pathways found in the GSM8K dataset from a teacher model (GPT-\\n3.5 code-davinci-002 (Chen et al., 2021b)). Fine-tuning and distilling smaller models require substantial\\namounts of training data to match the performance of the large model. To address this issue, Hsieh et al.\\n(2023) propose Distilling Step-by-Step, a technique that uses CoT prompting to extract LLM rationales\\nfor extra guidance in training smaller models within a multi-task setting. Experimental results show that\\nDistilling Step-by-Step achieves better performance with much fewer labeled or unlabeled training examples\\ncompared to both fine-tuning and standard distillation. Fine-tune-CoT (Ho et al., 2023) utilizes existing\\nzero-shot CoT prompting techniques (Kojima et al., 2022) to create rationales from LLMs. These rationales\\nare then used to fine-tune smaller student models. It also introduces diverse reasoning, a method that\\nemploys stochastic sampling to generate a variety of reasoning solutions from teacher models, which serves\\nto enrich the training data for the student models. SOCRATIC CoT (Shridhar et al., 2023) breaks down\\nthe original problem into a series of smaller sub-problems and utilizes this decomposition to direct the\\nintermediate steps of reasoning. It is used to train a pair of smaller, distilled models: one specializes in\\ndissecting the problem and the other focuses on solving these sub-problems. SOCRATIC COT is shown to\\nbe an effective alternative to CoT, enabling a much smaller model (GPT-2 large) to outperform a 10x larger\\nmodel (GPT-3 6B). SCOTT (Wang et al., 2023c) uses rationales generated by LLMs to train a student\\nmodel under a counterfactual reasoning framework. It ensures that the student model does not overlook the\\nprovided rationales, thereby preventing it from making inconsistent predictions. Experimental results show\\nthat SCOTT can generate CoT rationales that are more faithful than original CoT prompting. Li et al.\\n(2023b) present a method called symbolic CoT distillation (SCoTD) that draws CoT rationales from a LLM\\nusing unlabeled data instances. A smaller model is then trained to predict both the sampled rationales and\\nthe associated labels. Lastly, Peng et al. (2023a) utilize GPT-4 as a teacher model to generate English and\\nChinese instruction-based datasets to refine student LLMs. They show that the 52K data points generated\\nby GPT-4 are able to improve zero-shot performance compared to instruction-following data generated from\\nprevious state-of-the-art models.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 10, 'page_label': '11'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient Pre-Training\\nMixed Precision TrainingAMP (Micikevicius et al., 2018; Facebook AI Research (FAIR), 2023; Rae et al., 2022),\\nBrain Floating Point (BFLOAT16) (Kalamkar et al., 2019; Burgess et al., 2019)\\nScaling Models\\nProgressive Stacking (Gong et al., 2019), MSLT (Yang et al., 2020), CompoundGrow (Gu et al., 2021), bert2BERT (Chen et al., 2022b),\\nKnowledge Inheritance (Qin et al., 2022), Staged Training (Shen et al., 2022), LiGO (Wang et al., 2023d), Mango (Pan et al., 2023),\\nYao et al. (2024), Growth Strategy (Li et al., 2023e)\\nInitialization TechniquesKumar (2017), Fixup (Zhang et al., 2019), ZerO (Zhao et al., 2022), SkipInit (De & Smith, 2020),\\nReZero (Bachlechner et al., 2021), T-Fixup (Huang et al., 2020), DeepNet (Wang et al., 2024a)\\nTraining OptimizersLion (Chen et al., 2023g), Sophia (Liu et al., 2024a)\\nSystem-Level Pre-Training\\nEfficiency Optimization\\nZeRO (Rajbhandari et al., 2020), FSDP (Zhao et al., 2023c), ZeRO-Offload (Ren et al., 2021), ZeRO-Infinity (Rajbhandari et al., 2021),\\nZeus (You et al., 2023), Perseus (Chung et al., 2023)\\nFigure 6: Summary of efficient pre-training techniques for LLMs.\\nUpdate Backward\\nTraining Data New Layer\\nOld Layer\\nProgressive Update\\nForward Forward\\nHigh-Precision Weight / Gradient\\n0 0 0\\n1 1 1\\nTraining\\nTraining Data\\nInitial Model Weight:\\nGradient:\\nEfficient \\nOptimizer \\n(a)\\xa0Mixed Precision Training (b)\\xa0Scaling Models \\n(c)\\xa0Initialization\\xa0Techniques (d)\\xa0Training Optimizers \\nLow-Precision Weight / Activation / Gradient\\nActivation:\\nConvert\\nFigure 7: Illustrations of efficient pre-training techniques for LLMs.\\n2.2 Efficient Pre-Training\\nAs shown in Table 1, pre-training LLMs incurs significant costs. Efficient pre-training techniques focus on\\nreducing the costs of the LLM pre-training process in terms of compute resources, training time, memory and\\nenergy consumption. As summarized in Figure 6, enhancing the efficiency of pre-training can be achieved\\nthrough different and complementary techniques, including mixed precision acceleration, scaling models,\\ninitialization techniques, training optimizers, and system-level pre-training efficiency optimization.\\nMixed Precision Training. Mixed precision training enhances pre-training efficiency by using low-\\nprecision models for forward and backward propagation and then converting the calculated low-precision\\ngradients to high-precision ones for updating the original high-precision weights. For example, Micikevicius\\net al. (2018) propose Automatic Mixed Precision (AMP) to keep a master copy of weights in full-precision\\n(FP32) for updates, whereas weights, activations, and gradients are stored in FP16 for arithmetic operations.\\nNotably, the improved version of AMP (Facebook AI Research (FAIR), 2023) has eliminated the copy of\\nFP32 weights, but the optimizer (AdamW) still uses FP32 internally. Meanwhile, Rae et al. (2022) demon-\\nstrate that FP16 in AMP results in accuracy loss due to the restricted numerical range. To address this\\nissue, Brain Floating Point (BFLOAT16), which has a greater dynamic range — i.e., number of exponent\\nbits — than FP16, was proposed (Kalamkar et al., 2019; Burgess et al., 2019) to achieve better training\\nperformance.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 11, 'page_label': '12'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nScaling Models.Techniques based on scaling models accelerate pre-training convergence and reduce train-\\ning costs by leveraging the weights of a smaller model to upscale to a larger one. For example, Gong et al.\\n(2019) introduce a technique named progressive stacking to transfer knowledge from a simpler model to a\\nmore complex one to enhance model training efficiency. Meanwhile, Yang et al. (2020) observe that as the\\ndepth of the model increases through progressive stacking, the training speed however decreases. To address\\nthis issue, they propose multi-stage layer training (MSLT), which only updates the output and newly intro-\\nduced top encoder layers while keeping the previously trained layers unchanged. Once all the layers have\\nbeen trained, MSLT fine-tunes the entire model by updating each layer with 20% of the total steps, making\\nit more time-efficient than the traditional progressive stacking approach. Similarly, Gu et al. (2021) intro-\\nduce CompoundGrow, which begins with training a small model and then incrementally expands it using\\na mix of model growth techniques, including increasing input length, model breadth and depth, leading to\\nan acceleration in the pre-training process in wall-clock time compared to progressive stacking. Chen et al.\\n(2022b) propose bert2BERT, which applies function-preserving initialization (FPI) and advanced knowledge\\ninitialization (AKI) to transfer the knowledge of a smaller pre-trained model to a large model to improve the\\npre-training efficiency of the large model. Specifically, FPI enforces the initialized larger model to closely\\nmirror the behavior of the smaller model, laying a strong basis for later optimization; and AKI promotes\\nfaster convergence by replicating weights from higher layers. Experimental results show that bert2BERT is\\nable to save a significant amount of training cost over MSLT. Qin et al. (2022) propose Knowledge Inheri-\\ntance which employs knowledge distillation as an auxiliary supervision during pre-training. This facilitates\\ntraining a larger model from a smaller teacher model, thereby enhancing both the pre-training speed and\\nthe generalization ability. Shen et al. (2022) introduce Staged Training that begins with a small model and\\nprogressively increases its depth and breadth through a growth operator. By starting each stage with the\\nresults from the previous one, it effectively reuses computation, leading to a more efficient training process\\ncompared to previous techniques like CompoundGrow and progressive stacking. Wang et al. (2023d) propose\\nLinear Growth Operator (LiGO) that linearly maps the parameters of a smaller model to initiate a larger one.\\nBy using a composition of width-and depth-growth operators further enhanced with Kronecker factorization\\nto capture architectural knowledge, LiGO outperforms bert2BERT which saves about 30% computational\\ncosts. Pan et al. (2023) introduce a technique named Mango which establishes a linear relationship between\\neach weight of the target model and all weights of the pretrained model to boost acceleration capabilities.\\nIt also employs multi-linear operators to decrease computational and spatial complexity during pre-training,\\nachieving 59.9% acceleration ratio compared to Chen et al. (2022b) and LiGO. Drawing from these scaling\\ntechniques and the progressive pre-training (Yao et al., 2024), recent LLMs like FLM-101B (Li et al., 2023e)\\nintroduce a growth strategy to cut LLM training costs by expanding model structures offline and resuming\\nfrom the previous stage’s smaller model checkpoint.\\nInitialization Techniques.Initialization plays a key role in enhancing the efficiency of LLM pre-training\\nbecause a good initialization can accelerate the convergence of the model. Most LLMs employ initialization\\ntechniques that were adopted in training smaller-scale models. For example, initialization method introduced\\nby Kumar (2017) balances input and output variances. Fixup (Zhang et al., 2019) and ZerO (Zhao et al.,\\n2022) set the backbone to zero, preserving signal identity. SkipInit (De & Smith, 2020) substitutes batch\\nnormalization with a zero-value multiplier. ReZero (Bachlechner et al., 2021) adds zero-valued parameters\\nto maintain identity which leads to faster convergence. T-Fixup (Huang et al., 2020) follows Fixup to adopt\\nrescaling schemes for the initialization of the residual blocks of Transformer models. DeepNet (Wang et al.,\\n2024a) adjusts the residual connection in deep Transformers using Post-LN-init, ensuring stable inputs to\\nlayer normalization and mitigating gradient vanishing for stable optimization.\\nTraining Optimizers. Popular LLMs such as GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022a),\\nBLOOM (Scao et al., 2023), and Chinchilla (Hoffmann et al., 2022) are predominately pre-trained using\\nAdam (Kingma & Ba, 2017) or AdamW (Loshchilov & Hutter, 2019) as optimizers. However, both Adam\\nand AdamW are memory hungry and computationally expensive. Some studies (Chen et al., 2023g; Liu et al.,\\n2024a) propose new optimizers to accelerate LLM pre-training. Specifically, Chen et al. (2023g) propose to\\nleverage search techniques to traverse a large and sparse program space to discover optimizers for model\\ntraining. The discovered optimizer, named Lion (EvoLved Sign Momentum), is more memory-efficient than\\nAdam as it only keeps track of the momentum. Liu et al. (2024a), on the other hand, propose Sophia\\nas a lightweight second-order optimizer that outpaces Adam with doubling the pre-training speed. Sophia\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 12, 'page_label': '13'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient Fine-Tuning\\nParameter-Efficient Fine-Tuning\\nLow-Rank Adaptation\\nLoRA (Hu et al., 2022), LoRA-FA (Zhang et al., 2023b),\\nLoraHub (Huang et al., 2023), LongLoRA (Chen et al., 2023i),\\nMulti-Head Routing (Caccia et al., 2023), AdaLoRA (Zhang et al., 2023d),\\nDyLoRA (Valipour et al., 2023), CEPT (Zhao et al., 2023b),\\nTied-LoRA (Renduchintala et al., 2023)\\nAdapter-based Tuning\\nLLM-Adapters (Hu et al., 2023b), Compacter (Karimi Mahabadi et al., 2021),\\n(IA)3(Liu et al., 2022a), Meta-Adapters (Bansal et al., 2022),\\nAdaMix (Wang et al., 2022c), OpenDelta (Hu et al., 2023a),\\nSparseAdapter (He et al., 2022b)\\nPrefix TuningPrefix-Tuning (Li & Liang, 2021), LLaMA-Adapter (Zhang et al., 2024)\\nHyperTuning (Phang et al., 2023)\\nPrompt Tuning\\nPrompt Tuning (Lester et al., 2021), P-Tuning (Liu et al., 2023b),\\nP-Tuning v2 (Liu et al., 2022c), Tam et al. (2023), MP2(Sun et al., 2023a),\\nPPT (Gu et al., 2022b), Multitask Prompt Tuning (Wang et al., 2023j),\\nXu et al. (2023b)\\nMemory-Efficient Fine-Tuning\\nQLoRA (Dettmers et al., 2023), QA-LoRA (Xu et al., 2024b), LoftQ (Li et al., 2024d), PEQA (Kim et al., 2023a),\\nSelective Fine-Tuning (Simoulin et al., 2023), LOMO (Lv et al., 2023), MeZO (Malladi et al., 2023),\\nLiu et al. (2023g)\\nFigure 8: Summary of efficient fine-tuning methods for LLMs.\\ncalculates the moving average of gradients and the estimated Hessian, dividing the former by the latter and\\napplying element-wise clipping. It effectively moderates update sizes, addresses non-convexity and rapid\\nhessian changes, enhancing both memory utilization and efficiency.\\nSystem-Level Pre-Training Efficiency Optimization.Due to high demand on memory and compute\\nresources, LLMs are usually pre-trained across multiple compute nodes in a distributed manner. Therefore,\\nmost system-level optimization techniques are designed in the setting of large-scale distributed training. For\\ninstance, Zero Redundancy Data Parallelism (ZeRO) (Rajbhandari et al., 2020) provides three stages of\\noptimization to partition various training states across different devices. Specifically, ZeRO-1 only partitions\\nthe optimizer states, whereas ZeRO-2 partitions both the optimizer states and the gradients. ZeRO-3\\nfurther partitions the model parameters across devices compared with ZeRO-1 and ZeRO-2. Although\\nruntime memory is further reduced through ZeRO-3, there is about 50% increase in communication volume.\\nTherefore, it is recommended to use ZeRO-3 within a node to minimize the communication time while\\nusing ZeRO-1 and ZeRO-2 across nodes. Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023c) shares a\\nsimilar idea for optimization, and designs a hybrid sharding strategy to allow users to define which nodes or\\nprocesses to partition the gradients, model parameters, and optimizer states across different nodes. In the\\ncase when the weight memory exceeds the aggregated memory that can be provided by all of the compute\\nnodes, ZeRO-Offload (Ren et al., 2021) enables offloading any stage of ZeRO to CPU memory, whereas\\nZeRO-Infinity (Rajbhandari et al., 2021) provides a mechanism to offload to NVMe drives in addition to\\nCPU memory. However, it is quite difficult to maintain performance using these two alternatives, as the data\\nmovement between CPU and GPU is slow. Lastly, training LLMs on numerous GPUs consumes a massive\\namount of energy, Zeus (You et al., 2023) and Perseus (Chung et al., 2023) are proposed to optimize energy\\nconsumption by finding the best GPU-level configurations based on the unique LLM characteristics. The\\nevaluation shows that Perseus reduces energy consumption of large model training by up to 30%.\\n2.3 Efficient Fine-Tuning\\nEfficient fine-tuning techniques focus on reducing the costs of the LLM fine-tuning process. As summarized\\nin Figure 8, efficient fine-tuning techniques can be grouped into parameter-efficient fine-tuning (PEFT) and\\nmemory-efficient fine-tuning (MEFT).\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 13, 'page_label': '14'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nfixed\\ntuned (added)\\nLLMs \\nreduce\\nreduce\\nbackward\\nLarge ActivationSmall\\nActivation\\nLarge GradientSmall\\nGradient\\nupdate\\nforward\\nFine-T uning \\nData \\nFine-T uning \\nData \\nfixed\\n+ X \\ntuned\\nLLMs \\nFine-T uning \\nData \\nfixed\\n(d) Prompt Tuning\\n(b) Adapter-based Tuning\\nEmbedding\\n+ \\ntuned\\nFine-T uning \\nData \\nfixed\\ntuned\\n(a) Low-Rank Adaptation\\n(c) Preﬁx Tuning\\n(e) Memory-Efﬁcient Fine-Tuning\\nFigure 9: Illustrations of parameter-efficient fine-tuning (a)-(d) and memory-efficient fine-tuning (e).\\n2.3.1 Parameter-Efficient Fine-Tuning\\nParameter-efficient fine-tuning (PEFT) adapts an LLM to downstream tasks by freezing the whole LLM\\nbackbone and only updating a small set of newly added extra parameters. In general, PEFT methods can be\\ngrouped into four categories: low-rank adaptation, adapter-based tuning, prefix tuning, and prompt tuning.\\nLow-Rank Adaptation.Low-rank adaptation (LoRA) (Hu et al., 2022) is a widely used PEFT approach\\nfor LLMs. The hypothesis is that the change in weights during model adaptation has a low “intrinsic rank”.\\nHence, LoRA introduces two trainable low-rank matricesA ∈Rm×r and B ∈Rr×n and adjusts the weight\\nmatrix byW ←W+∆W = W+A·B. As such, only the small matricesA and B are updated during fine-\\ntuning, while the original large weight matrix remains frozen, making the fine-tuning process more efficient.\\nTo enhance the efficiency of LoRA, LoRA-FA (Zhang et al., 2023b) keeps the projection-down weights of\\nA fixed while only updating the projection-up weights of B in each LoRA adapter so that the weight\\nmodifications during fine-tuning are confined to a low-rank space, thereby eliminating the need to store the\\nfull-rank input activations. It achieves comparable accuracy related to full parameter fine-tuning and LoRA.\\nBuilding on top of LoRA, LoraHub (Huang et al., 2023) explores the composability of LoRA for the purpose\\nof generalizing across different tasks. It combines LoRA modules that have been trained on various tasks\\nwith the goal of attaining good performance on tasks that have not been seen before. LongLoRA (Chen et al.,\\n2023i), on the other hand, extends LoRA to the long-context fine-tuning scenario. It introduces shift short\\nattention (S2-Attn), which effectively facilitates context expansion, showing that LoRA is effective for long\\ncontext when utilizing trainable embedding and normalization. Multi-Head Routing (MHR) (Caccia et al.,\\n2023) extends LoRA to Mixture-of-Experts (MoE) architectures. It outperforms Polytropon (Ponti et al.,\\n2023) when operating with a similar parameter allocation. Notably, it achieves competitive performance\\nwhile focusing on fine-tuning the routing function alone, without making adjustments to the adapters,\\ndemonstrating remarkable parameter efficiency. Zhang et al. (2023d) observe that many PEFT techniques\\nneglect the differing significance of various weight parameters. To address this, they propose AdaLoRA\\nwhich employs singular value decomposition to parameterize incremental updates and adaptively distributes\\nthe parameter budget based on the importance score of each weight matrix. The rank in LoRA is static and\\ncannot be adaptively adjusted during fine-tuning. Valipour et al. (2023) propose DyLoRA to introduce a\\ndynamic low-rank adaptation method that trains LoRA blocks across multiple ranks rather than just one by\\norganizing the representations learned by the adapter module based on their ranks. Different from the above-\\nmentioned methods that apply LoRA-based methods to full-size LLMs, CEPT (Zhao et al., 2023b) introduces\\naframeworkthatutilizescompressedLLMs. Specifically, itassesseshowprevalentLLMcompressionmethods\\naffect PEFT performance and subsequently implements strategies for knowledge retention and recovery to\\ncounteract the loss of knowledge induced by compression. Lastly, Tied-LoRA (Renduchintala et al., 2023)\\nuses weight tying and selective training to further increase parameter efficiency of LoRA.\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 14, 'page_label': '15'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nAdapter-based Tuning.Adapters are bottleneck-like trainable modules integrated into LLMs, which first\\ndown-project the input feature vector followed by a non-linear layer and then up-project back to the original\\nsize (Houlsby et al., 2019). Adapter-based tuning includes both series adapters and parallel adapters. In\\nseries adapters, each LLM layer has two adapter modules added after its attention and feed-forward modules;\\nwhereas parallel adapters position two adapter modules alongside the attention and feed-forward modules\\nwithin each layer of the LLM. In particular, Hu et al. (2023b) propose LLM-Adapters, which integrates series\\nor parallel adapters into LLMs for fine-tuning on different tasks. Karimi Mahabadi et al. (2021) propose\\nCompacter, which unifies adapters, low-rank techniques, and the latest hyper-complex multiplication layers\\nto achieve a balanced trade-off between the amount of trainable parameters and task performance compared\\nto original Adapter method (Houlsby et al., 2019). Furthermore, (IA)3 (Liu et al., 2022a) introduces a\\ntechnique that scales activations using learned vectors. It outperforms Adapter (Houlsby et al., 2019)\\nand Compacter on few-shot setting with better accuracy and computational efficiency. Following meta-\\nlearning principles, Meta-Adapters (Bansal et al., 2022) designs a resource-efficient fine-tuning technique\\nfor the few-shot scenario where it incorporates adapter layers that have been meta-learned into a pre-\\ntrained model, transforming the fixed pre-trained model into an efficient few-shot learning framework. Meta-\\nAdapters outperforms Adapter (Houlsby et al., 2019) at few-shot fine-tuning with less parameters to fine-\\ntune. AdaMix (Wang et al., 2022c) takes inspiration from sparsely-activated mixture-of-experts (MoE)\\nmodels (Zuo et al., 2022) and proposes a mixture of adaptation modules to learn multiple views of the given\\ntask. Compared to Adapter, it demonstrates better results on both natural language understanding and\\ngeneration tasks with less learnable parameters. Lastly, OpenDelta (Hu et al., 2023a) is an open-source\\nsoftware library that offers a versatile and plug-and-play framework for implementing a range of adapter-\\nbased techniques, and is designed to be compatible with various LLMs architectures.\\nPrefix Tuning.Prefix-Tuning (Li & Liang, 2021) adds a series of trainable vectors, known as prefix tokens,\\nto each layer in an LLM. These prefix tokens are tailored to specific tasks and can be treated as virtual\\nword embeddings. Building on top of Prefix-Tuning, LLaMA-Adapter (Zhang et al., 2024) incorporates a\\nset of trainable adaptation embeddings and attaches them to the word embeddings in the upper layers of the\\nLLMs. A zero-initialized attention scheme with zero gating is also introduced. It dynamically incorporates\\nnew guiding signals into LLaMA-1 while retaining its pre-trained knowledge. Different from conventional\\nprefix tuning, HyperTuning (Phang et al., 2023) employs a hyper-model to produce task-specific parameters\\nsuch as soft prefixes for a downstream model, showing improved performance through initialization from\\nhypermodel-generated parameters for subsequent fine-tuning.\\nPrompt Tuning. Different from prefix tuning, prompt tuning incorporates trainable prompt tokens only\\nat the input layer. These tokens can be inserted either as a prefix or anywhere within the input tokens.\\nPrompt Tuning (Lester et al., 2021) keeps the entire pre-trained model fixed while adding an extraktrainable\\ntokens at the beginning of the input text for each downstream task. It outperforms few-shot prompts and\\nnarrows the performance gap compared to full-model fine-tuning. P-Tuning (Liu et al., 2023b) utilizes a\\nsmall number of parameters as prompts, which are processed by a prompt encoder before being used as\\ninput for pre-trained LLMs. Instead of searching for discrete prompts, P-Tuning fine-tunes these prompts\\nthrough gradient descent and improves performance on a wide range of natural language understanding tasks\\ncompared to Prompt Tuning. Liu et al. (2022c) observe that earlier versions of prefix tuning struggle with\\ncomplex sequence labeling tasks. To address this, they propose P-Tuning v2, which borrows the ideas from\\nprefix tuning by introducing continuous prompts at each layer of the pre-trained model. This modification\\nhas proven effective in boosting performance across various parameter sizes for tasks related to natural\\nlanguage understanding. Tam et al. (2023) introduce efficient prompt tuning for text retrieval, updating just\\n0.1% of parameters and outperforming traditional full-parameter update methods in diverse domains. Sun\\net al. (2023a) claim that prompt tuning tends to struggle in few-shot learning scenarios, and thus propose\\nMP2 that pre-trains a collection of modular prompts using multitask learning. These prompts are then\\nselectively triggered and assembled by a trainable routing mechanism for specific tasks. As a result, MP2\\ncan quickly adapt to downstream tasks by learning how to merge and reuse pretrained modular prompts.\\nDifferent from MP2, PPT (Gu et al., 2022b) attributes the performance degradation of prompt tuning in\\nfew-shot learning to the poor initialization of soft prompt, and thus proposes to add the soft prompt into\\nthe pre-training stage for a better initialization. Lastly, Multitask Prompt Tuning (Wang et al., 2023j)\\nextends Prompt Tuning and harnesses the knowledge of the various tasks through the use of prompt vectors\\n15'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 15, 'page_label': '16'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nin a multitask learning settings. Specifically, it initially learns a single, transferable prompt by extracting\\nknowledge from various task-specific source prompts, and then applies multiplicative low-rank updates to\\nthis prompt to effectively tailor it for each downstream task. By doing this, Multitask Prompt Tuning is\\nable to attain performance levels that are competitive compared to full-model fine-tuning methods.\\n2.3.2 Memory-Efficient Fine-Tuning\\nDifferent from PEFT methods which focus on parameter efficiency, MEFT methods focus on memory savings\\nduring the LLMs fine-tuning process. For instance, Dettmers et al. (2023) propose QLoRA which first\\nquantizes the model into a 4-bit NormalFloat data type, and then fine-tunes this quantized model with\\nadded low-rank adapter (LoRA) weights (Hu et al., 2022). In doing so, QLoRA reduces memory usage\\nduring fine-tuning without performance degradation compared to standard full-model fine-tuning. QA-\\nLoRA (Xu et al., 2024b) improves QLoRA by introducing group-wise operators that improve quantization\\nflexibility (each group is quantized separately) while reducing adaptation parameters (each group utilizes\\nshared adaptation parameters). Similarly, LoftQ (Li et al., 2024d) combines model quantization with singular\\nvalue decomposition (SVD) to approximate the original high-precision pre-trained weights. As a result, it\\noffers a favorable initialization point for subsequent LoRA fine-tuning, leading to enhancements over QLoRA\\non both natural language understanding and generation tasks. PEQA (Kim et al., 2023a) introduces a two-\\nstage approach to quantization-aware fine-tuning. In the first stage, the parameter matrix for each fully\\nconnected layer is quantized into a matrix of low-bit integers along with a scalar vector. In the second\\nstage, the low-bit matrix remains unchanged, while fine-tuning is focused solely on the scalar vector for each\\nspecific downstream task. Employing this two-stage approach, PEQA not only minimizes memory usage\\nduring fine-tuning but also speeds up inference time by maintaining weights in a low-bit quantized form,\\nshowing better perplexity than GPTQ (Frantar et al., 2023) with LoRA. Different from above-mentioned\\nMEFT methods that combine LoRA with quantization to reduce fine-tuning memory footprints, as another\\nline of research, some studies propose MEFT methods based on gradient optimization. Specifically, Simoulin\\net al. (2023) propose Selective Fine-Tuning which minimizes memory usage by specifically preserving a subset\\nof intermediate activations from the forward pass for which the calculated gradients are nonzero. Notably,\\nthis approach delivers performance equivalent to full-model fine-tuning while using just up to one-third of the\\nGPU memory required otherwise. Lv et al. (2023) introduce LOMO, which minimizes memory consumption\\nduring fine-tuning by combining gradient calculation and parameter updating into a single step. As such,\\nLOMO eliminates all components of the optimizer state, lowering the memory requirements for gradient\\ntensors to O(1). Lastly, MeZO (Malladi et al., 2023) improves the zeroth-order method (Spall, 1992) for\\ngradient estimation using only two forward passes. This enables efficient fine-tuning of LLMs with memory\\nrequirements similar to inference and supports both full-parameter and PEFT methods like LoRA (Hu et al.,\\n2022) and prefix tuning (Li & Liang, 2021), enabling MeZO to train a 30-billion parameter model on a single\\nA100 80GB GPU.\\n2.4 Efficient Inference\\nEfficient inference techniques focus on reducing the costs of the LLMs inference process. As summarized in\\nFigure 10, efficient inference techniques can be grouped into techniques at algorithm level and system level.\\nAlgorithm-Level Inference Efficiency Optimization. Techniques that enhance LLM inference effi-\\nciency at the algorithm level include speculative decoding and KV-cache optimization.\\n• Speculative Decoding.Speculative decoding (i.e., speculative sampling) (Leviathan et al., 2023)\\nis a decoding strategy for autoregressive language models that speeds up the sampling process by\\ncomputing tokens using a smaller draft model in parallel to create speculative prefixes for the large\\ntarget model. Chen et al. (2023a) focus on the distributed serving setting for LLMs and propose\\nto run a faster autoregressive modelK times and then evaluate the preliminary output with the\\nlarge target model. A tailored rejection sampling strategy is employed to approve a selection of\\nthe draft tokens in a left-to-right order, thereby recapturing the distribution of the large target\\nmodel during the procedure. Staged Speculative (Spector & Re, 2023) transforms the speculative\\nbatch into a tree structure representing potential token sequences. This restructuring expedites the\\n16'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 16, 'page_label': '17'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient Inference\\nAlgorithm-Level Inference\\nEfficiency Optimization\\nSpeculative Decoding\\nSpeculative Decoding (Leviathan et al., 2023), Chen et al. (2023a),\\nStaged Speculative (Spector & Re, 2023), BiLD (Kim et al., 2023c),\\nSpecInfer (Miao et al., 2024), LLMA (Yang et al., 2023b),\\nMedusa (Cai et al., 2024), Santilli et al. (2023), PaSS (Monea et al., 2023)\\nKV-Cache Optimization\\nKIVI (Zirui Liu et al., 2023), KVQuant (Hooper et al., 2024),\\nHeavy-Hitter Oracle (H2O) (Zhang et al., 2023f),\\nScissorhands (Liu et al., 2023e), StreamingLLM (Xiao et al., 2024)\\nSystem-Level Inference\\nEfficiency Optimization\\nFlexGen (Sheng et al., 2023), Pope et al. (2023), S3(Jin et al., 2023), Orca (Yu et al., 2022), vLLM (Kwon et al., 2023),\\nDeepSpeed-Inference (Aminabadi et al., 2022), Flash-Decoding (Dao et al., 2023), FlashDecoding++ (Hong et al., 2023)\\nFigure 10: Summary of efficient inference techniques for LLMs.\\nLarge | language | model | has | witnessed | a | huge | advance\\nCheck & Regenerate\\nGenerate x x x \\nLLMs\\nSmall LMs\\nKV cache [T oken] \\nLLMs\\nnew query\\nold key, value\\nnew key,\\nvalue\\n1 \\n2 \\n3 \\n4 \\n(a)\\xa0Speculative Decoding (b)\\xa0KV-Cache Optimization\\nFigure 11: Illustrations of algorithm-level efficiency optimization techniques for LLM inference.\\ngeneration of larger and improved speculative batches. It also introduces an additional phase for\\nspeculative decoding of the initial model, thereby enhancing overall performance, showing 1.36x over\\nstandard speculative decoding. BiLD (Kim et al., 2023c) optimizes speculative decoding through\\ntwo innovative techniques: the fallback policy that permits the smaller draft model to waive control\\nto the larger target model when it lacks sufficient confidence; and the rollback policy that enables\\nthe target model to revisit and rectify any inaccurate predictions made by the smaller draft model.\\nSpecInfer (Miao et al., 2024) extends speculative decoding and speeds up inference by employing\\nspeculative inference techniques and token tree validation. Its core idea involves merging a range\\nof small speculative models that have been fine-tuned collectively to collaboratively forecast the\\noutput of the large target model, which is then used to validate all the predictions. Different\\nfrom speculative decoding that needs to introduce an additional efficient drafter model to generate\\na draft for checking, LLMA (Yang et al., 2023b) chooses a text segment from a closely related\\nreference and duplicates its tokens into the decoder. It then concurrently assesses the suitability of\\nthese tokens as the decoding output within a single decoding step. This approach results in a speed\\nincrease of more than two times while maintaining the same generated results as traditional greedy\\ndecoding. Similarly, instead of using a separate draft model to sequentially generate candidate\\noutput, Medusa (Cai et al., 2024) proposes to freeze the LLM backbone, fine-tune additional heads,\\nand use a tree-based attention mechanism to process predictions in parallel to speed up the decoding\\nprocess. Lastly, Santilli et al. (2023) propose parallel decoding including the Jacobi and Gauss-Seidel\\nfixed-point iteration methods for speculative decoding. Among these methods, Jacobi decoding was\\nextended into Lookahead decoding (Fu et al., 2023c) to further enhance the efficiency.\\n• KV-Cache Optimization.During inference, LLMs need to store the Key-Value (KV) pairs of the\\npast tokens into the cache for future token generation. The size of KV cache needed enlarges mas-\\nsively with the increase of generated token length, resulting in considerable memory consumption\\nand long inference latency. Therefore, reducing the size of KV cache is key to enhancing inference\\nefficiency. Existing KV-cache optimization techniques can in general be grouped into two categories.\\nThe first category is to compress the KV cache. For example, Zirui Liu et al. (2023) propose KIVI, a\\ntuning-free 2bit KV cache quantization algorithm which quantizes the key cache per-channel and the\\nvalue cache per-token, achieving 2.6x less peak memory usage during inference. Similarly, Hooper\\n17'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 17, 'page_label': '18'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\net al. (2024) conduct an empirical study on the impact of per-channel quantization and other types of\\nquantization such as quantization before rotary positional embedding. Based on their findings, they\\npropose KVQuant which combines these quantization methods to quantize the KV cache of LLaMA\\nto 3-bit. The second category of KV-Cache optimization techniques is to evict some KVs from the\\ncache. For instance, Zhang et al. (2023f) propose Heavy-Hitter Oracle (H2O), a KV cache eviction\\nstrategy that formulates the KV cache eviction as a dynamic submodular problem and dynami-\\ncally retains a balance between recent and performance-critical tokens, improving the throughput\\nfor LLMs inference. Similarly, Liu et al. (2023e) propose a hypothesis named the persistence of\\nimportance, suggesting that only tokens that were crucial at an earlier phase will have a significant\\nimpact on subsequent stages. Based on this hypothesis, they design Scissorhands which significantly\\nreduces the KV cache without compromising model quality. Lastly, StreamingLLM (Xiao et al.,\\n2024) incorporates window attention, where only the most recent KVs are cached into a fixed-size\\nsliding window, into their algorithm design. Through evicting the outdated KVs, StreamingLLM\\nensures constant memory usage and decoding speed after the cache is initially filled.\\nSystem-Level Inference Efficiency Optimization. The efficiency of LLM inference can also be op-\\ntimized at the system level under a specific hardware architecture. For example, FlexGen (Sheng et al.,\\n2023) is a high-throughput inference engine that enables the execution of LLMs on GPUs with limited\\nmemory. It uses a linear programming-based search approach to coordinate various hardware, combining\\nthe memory and computation from GPU, CPU, and disk. Furthermore, FlexGen quantizes the weights\\nand attention cache to 4 bits, which increases the inference speed of OPT-175B (Zhang et al., 2022a) on a\\nsingle 16GB GPU. Pope et al. (2023) develop a simple analytical framework to partition a model in order\\nto scale Transformer inference based on the application requirements. By combining it with scheduling and\\nmemory optimizations, they are able to achieve better efficiency on PaLM (Chowdhery et al., 2022) in com-\\nparison to FasterTransformer (NVIDIA, 2023a). Orca (Yu et al., 2022) employs iteration-level scheduling\\nto serve batched sequences with variable output sequence length. When a sequence in a batch is com-\\npleted, it is returned to the user so that a new sequence can be served immediately. As a result, Orca\\nimproves GPU utilization compared to static batching, showing 36.9× throughput improvement under the\\nsame level of latency compared to FasterTransformer. S3 (Jin et al., 2023) creates a system that is aware\\nof the output sequence beforehand. It can anticipate the length of the sequence and arrange generation\\nrequests accordingly, optimizing the utilization of device resources and increasing the rate of production,\\nshowing higher throughput than Orca with the same number of GPUs. However, both Orca and S3 lead to\\nmemory fragmentation due to their inaccurate memory provisioning for each request. vLLM (Kwon et al.,\\n2023) addresses the memory efficiency problem with PagedAttention, which enables the storage of contin-\\nuous keys and values in non-contiguous memory space. Specifically, PagedAttention divides the KV cache\\nof each sequence into blocks, each containing the keys and values for a fixed number of tokens. During\\nattention computation, PagedAttention kernel manages these blocks efficiently by maintaining a block table\\nto reduce memory fragmentation. Specifically, the contiguous logical blocks of a sequence are mapped to\\nnon-contiguous physical blocks via the table and the table automatically allocates a new physical block for\\nevery newly generated token. This reduces the amount of memory wasted when generating new tokens, thus\\nimproving its efficiency, showing that PagedAttention improves the throughput of popular LLMs by 2-4×\\nwith the same level of latency compared to FasterTransformer (NVIDIA, 2023a) and Orca (Yu et al., 2022).\\nOn the other hand, infinitely optimizing server-side aggregated metrics does not necessarily lead to good\\nuser experience or Quality of Experience (QoE) especially under high server load. Andes (Liu et al., 2024b)\\nfirst defines QoE for the LLM-based text streaming services and proposes a QoE-aware serving systems to\\noptimize QoE by prioritizing requests based on their resource demand and service acquired. DeepSpeed-\\nInference (Aminabadi et al., 2022) is a multi-GPU inference approach designed to enhance the efficiency\\nof both dense and sparse Transformer models when they are contained within the collective GPU memory.\\nFurthermore, it provides a mixed inference technique that utilizes CPU and NVMe memory, in addition\\nto GPU memory and computation, enabling high-throughput inference even for models that are too large\\nto fit in the combined GPU memory. This approach demonstrates lower latency than FasterTransformer\\nunder the same throughput. Flash-Decoding (Dao et al., 2023) boosts the speed of long-context inference\\nby breaking down keys/values into smaller pieces, computing attention on these pieces in parallel, and then\\ncombining them to generate the final output. It outperforms FasterTransformer and FlashAttention (Dao\\n18'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 18, 'page_label': '19'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient Architecture Design\\nEfficient Attention\\nSharing-based AttentionMQA (Shazeer, 2019), GQA (Ainslie et al., 2023)\\nKernelization or Low-RankSumformer (Alberti et al., 2023), FluRKA (Gupta et al., 2023), Scatterbrain (Chen et al., 2021a),LRT (Winata et al., 2020), Performer (Choromanski et al., 2021), RFA (Peng et al., 2021),Linear Transformer (Katharopoulos et al., 2020), Linformer (Wang et al., 2020)\\nFixed Pattern StrategiesPagliardini et al. (2023), Big Bird (Zaheer et al., 2020), Poolingformer (Zhang et al., 2021),Longformer (Beltagy et al., 2020), Blockwise Transformer (Qiu et al., 2020), Sparse Transformer (Child et al., 2019),Lightning Attention-2 (Qin et al., 2024)\\nLearnable Pattern StrategiesHyperAttention (Han et al., 2024), Reformer (Kitaev et al., 2020), Sparse Sinkhorn Attention (Tay et al., 2020),Clustered Attention (Vyas et al., 2020), ClusterFormer (Wang et al., 2022b), Routing Transformer (Roy et al., 2021)\\nHardware-Assisted AttentionFlashAttention (Dao et al., 2022), vAttention (Prabhu et al., 2024)\\nMixture of Experts (MoE)\\nMoE-based LLMsGShard (Lepikhin et al., 2021), Switch Transformer (Fedus et al., 2022), Artetxe et al. (2022),BASE Layer (Lewis et al., 2021), PanGu-∑(Ren et al., 2023b), Mixtral 8x7B (Jiang et al., 2023a)\\nAlgorithm-Level MoE OptimizationExpert Choice (Zhou et al., 2022), StableMoE (Dai et al., 2022), X-MoE (Chi et al., 2022),Lifelong-MoE (Chen et al., 2023f), Flan-MoE (Shen et al., 2024)\\nSystem-Level MoE OptimizationFastMoE (He et al., 2021), FasterMoE (He et al., 2022a), DeepSpeed-MoE (Rajbhandari et al., 2022),TA-MoE (Chen et al., 2022a), EdgeMoE (Yi et al., 2023), Tutel (Hwang et al., 2023),SmartMoE (Zhai et al., 2023), MegaBlocks (Gale et al., 2023)\\nLong Context LLMs\\nPositional Extrapolation and InterpolationALiBi (Press et al., 2022), xPOS (Sun et al., 2023c), CLEX (Chen et al., 2024a),RoPE-PI (Chen et al., 2023d), NTK Interpolation (bloc97, 2023),YaRN Interpolation (Peng et al., 2024), FIRE (Li et al., 2024a), PoSE (Zhu et al., 2024)\\nRecurrent StructureTransformer-XL (Dai et al., 2019), Memformer (Wu et al., 2022a),∞-former (Martins et al., 2022),RMT (Bulatov et al., 2022), Block-Recurrent Transformer (Hutchins et al., 2022), Retentive Network (Sun et al., 2023b)\\nSegmentation and Sliding WindowMistral (Jiang et al., 2023a), StreamingLLM (Xiao et al., 2024), PCW (Ratner et al., 2023),LongNet (Ding et al., 2023a), SLED (Ivgi et al., 2023), MemWalker (Chen et al., 2023c),RAPTOR (Sarthi et al., 2024)\\nMemory-Retrieval AugmentationMemorizing Transformer (Wu et al., 2022c), Landmark Attention (Mohtashami & Jaggi, 2023),LongMem (Wang et al., 2023e), Unlimiformer (Bertsch et al., 2023),Focused Transformer (Tworkowski et al., 2023), Xu et al. (2024a)\\nTransformer-Alternative Architecture\\nState Space ModelsStructured State Space (Gu et al., 2022a), Diagonal State Space (Gupta et al., 2022), H3 (Fu et al., 2023a),Gated State Space (Mehta et al., 2023), Block-State Transformer (Pilault et al., 2023),Mamba (Gu & Dao, 2023), SMA (Ren et al., 2023a)\\nOther Sequential ModelsRWKV (Peng et al., 2023b), Hyena (Poli et al., 2023), MEGABYTE (YU et al., 2023)\\nFigure 12: Summary of efficient architecture designs for LLMs.\\net al., 2022) in decoding speed for very large sequences. FlashDecoding++ (Hong et al., 2023) supports\\nmainstream language models and hardware backends through asynchronous softmax, double buffering for\\nflat GEMM optimization, and heuristic dataflow, resulting in up to 4.86x and 2.18x acceleration on Nvidia\\nand AMD GPUs respectively compared to HuggingFace implementations, showing higher speedup compared\\nto Flash-Decoding under the same throughput.\\n2.5 Efficient Architecture Design\\nEfficient architecture design for LLMs refers to the strategic optimization of model architecture and compu-\\ntational processes to enhance performance and scalability while minimizing resource consumption. Figure 12\\nprovides a summary of existing efforts on designing efficient architectures for LLMs.\\n2.5.1 Efficient Attention\\nThe quadratic time and space complexity of attention modules considerably slows down the pre-training,\\ninference, and fine-tuning of LLMs (Duman Keles et al., 2023). Many techniques have been proposed to\\nmake attention lightweight for more efficient execution. These techniques can be generally categorized as\\nsharing-based attention, kernelization or low-rank, fixed pattern strategies, learnable pattern strategies, and\\nhardware-assisted attention.\\nSharing-based Attention. Sharing-based attention accelerates attention computation during inference\\nthrough KV heads sharing. For example, LLaMA-2 (Touvron et al., 2023b) optimizes the autoregres-\\nsive decoding process by using multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention\\n(GQA) (Ainslie et al., 2023). In traditional multi-head attention (MHA), each head has distinct linear\\n19'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 19, 'page_label': '20'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nW 1 W 2 x \\nx x x\\n= (1,1) \\n(2,2) \\n(3,3) \\n(4,4) \\nValueIndexOriginal Weight\\nU 1 V 1 U 2 V 2 \\n≈\\n=\\nOriginal FeatureLearned Pattern\\nML\\n(b) Kernelization or\\nLow-Rank\\n(c) Fixed Pattern\\nStrategies\\n(d)\\xa0Learnable Pattern\\nStrategies\\xa0\\nKey \\nValue \\nQuery \\nMulti-Head Sharing-based \\n(a) Sharing-based\\nAttention\\xa0\\nFigure 13: Illustrations of attention optimizations.\\nFlashAttention\\nMemory Hierarchy with\\nBandwidth & Memory Size\\nAttention on GPT-2\\nFlashAttentionPyTorch\\nTime (ms)\\nMatmul\\nMask\\nSoftmax\\nDropout\\nMatmul\\nFused\\nKernel\\nQ: N x d V: N X d\\nKT: d x N\\nQKT: N x N\\nsm(QKT)V: N x d\\nOuter Loop\\nCopy Block to SRAM\\nCopy\\nOuter Loop\\nCopy\\nInner Loop\\nCompute Block\\non SRAM\\nOutput to HBM\\nInner Loop\\nInner Loop\\nOuter Loop\\nGPU\\nSRAM\\nGPU\\nHBM\\nMain Memory\\n(CPU DRAM)\\nSRAM: 19 TB/s (20 MB)\\nHBM: 1.5 TB/s (40 GB)\\nDRAM: 12.8 GB/s\\n                (>1 TB)\\n0\\n5\\n10\\n15 Figure14: DesignofFlashAttention(Dao\\net al., 2022).\\ntransformations for input matrix queries (Q), keys (K) and values (V), allowing for diverse representations\\nand attention mechanisms across different subspaces. In MQA, all heads share a single set of key and value\\nweights across all query heads. Thus, MQA speeds up the inference but could compromise output quality.\\nTo address this drawback, GQA interpolates MQA and MHA by employing one key and value heads for each\\ngroup of query heads to enhance inference quality.\\nKernelization or Low-Rank. Kernelization or low-rank techniques adopted by models such as Sum-\\nformer (Alberti et al., 2023), FluRKA (Gupta et al., 2023), Scatterbrain (Chen et al., 2021a), Low-Rank\\nTransformer (LRT) (Winata et al., 2020), Performer (Choromanski et al., 2021), Random Feature Attention\\n(RFA) (Peng et al., 2021), Linear Transformer (Katharopoulos et al., 2020), and Linformer (Wang et al.,\\n2020) enhance the efficiency by utilizing low-rank representations of the self-attention matrix or by adopting\\nattention kernelization techniques. Specifically, low-rank methods focus on compacting the dimensions of at-\\ntention keys and values. For example, Linformer (Wang et al., 2020) proposes to segment scaled dot-product\\nattention into smaller units via linear projection. Kernelization, a variant of low-rank techniques, focuses\\non approximating the attention matrix (Choromanski et al., 2020). For example, Performer (Choromanski\\net al., 2021) condenses softmax attention-kernels using positive orthogonal random features, outperforming\\nReformerandLinformeronlongproteinsequencebenchmark. Sumformer (Albertietal.,2023)approximates\\nthe equivariant sequence-to-sequence function, offering a universal solution for Linformer and Performer.\\nFixed Pattern Strategies.Fixed pattern strategies adopted by models such as (Pagliardini et al., 2023),\\nBig Bird (Zaheer et al., 2020), Poolingformer (Zhang et al., 2021), Longformer (Beltagy et al., 2020), Block-\\nwise Transformer (Qiu et al., 2020), and Sparse Transformer (Child et al., 2019) improve efficiency by\\nsparsifying the attention matrix. This is achieved by confining the attention scope to predetermined pat-\\nterns, such as local windows or fixed-stride block patterns. For instance, the attention mechanism adopted\\nby Longformer (Beltagy et al., 2020), designed as an alternative to conventional self-attention, merges local\\nwindowed attention with globally oriented attention tailored to specific tasks. Pagliardini et al. (2023) ex-\\npand FlashAttention (Dao et al., 2022) to support a broad spectrum of attention sparsity patterns, including\\nkey-query dropping and hashing-based attention techniques, achieving a multi-fold runtime speedup on top\\nof FlashAttention on long text benchmark.\\nLearnable Pattern Strategies. Different from fixed pattern strategies, learnable pattern strategies\\nadoptedbymodelssuchasHyperAttention(Hanetal.,2024), Reformer(Kitaevetal.,2020), SparseSinkhorn\\nAttention (Tay et al., 2020), Clustered Attention (Vyas et al., 2020), ClusterFormer (Wang et al., 2022b),\\nand Routing Transformer (Roy et al., 2021) improve efficiency by learning token relevance and subsequently\\ngrouping tokens into buckets or clusters. As an example, HyperAttention (Han et al., 2024) proposes a\\nparameterization for spectral approximation and employs two key metrics: the maximal column norm in the\\n20'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 20, 'page_label': '21'}, page_content=\"Published in Transactions on Machine Learning Research (May/2024)\\nLLMs \\nRouter Network 2\\nRouter Network 1\\nInput Output \\nExperts Experts Input \\nsentence 1\\nsentence 1\\nsentence 2\\nsentence 3\\nsentence 4\\nsentence 1\\nsentence 2\\nsentence 3\\nsentence 4\\nIt's short. I can do it.\\nIt's long. I can't do it.\\nIt's long. But I can do it.\\ngood generation\\nBad Generation\\ngood generation\\n(2) window & stream structure\\n(4) recurrent\\xa0 structure\\n(1) Extrapolation\\nand Interpolation\\n(3) Memory-\\nRetrieval\\nAugmentation\\n(a) Mixture of Experts (MoE) (b) Long Context LLMs\\nFigure 15: Illustrations of Mixture of Experts (MoE) and long context LLMs.\\nnormalized attention matrix and the row norm ratio in the unnormalized matrix after large entry removal.\\nIt also utilizes the learnable sort locality-sensitive hashing (sortLSH) technique and fast matrix multiplica-\\ntion via row norm sampling. The experiment results show that HyperAttention enhances both inference and\\ntraining speeds for LLMs with only minimal performance degradation, giving significant speed improvements\\ncompared to FlashAttention (Dao et al., 2022) on long contexts.\\nHardware-AssistedAttention. Hardware-assistedattentionfocusesondevelopinghardware-specifictech-\\nniques to enhance attention efficiency. For example, FlashAttention (Dao et al., 2022) reduces the number\\nof memory access between GPU high-bandwidth memory (HBM) and GPU on-chip SRAM when calculating\\nthe attention module in LLMs. Instead of transmitting the values and results between HBM and SRAM\\nmultiple times as is done in the standard attention mechanism, FlashAttention combines all the attention\\noperations into one kernel and tiles the weight matrices into smaller blocks to better fit the small SRAM as\\nshown in Figure 14. As a result, only one communication is required to process each attention block, which\\nsignificantly enhances the efficiency for processing the entire attention block. vAttention (Prabhu et al.,\\n2024) is proposed to store KV cache in contiguous virtual memory without committing physical memory\\nahead-of-time. It avoids the software complexity to store KV cache by leveraging CUDA support of low-level\\nvirtual memory APIs.\\n2.5.2 Mixture of Experts (MoE)\\nMixture of Experts (MoE) represents a sparse methodology utilized prominently in large-scale models like\\nLLMs. It operates on the principle of segmenting a designated task into several sub-tasks, and then de-\\nveloping numerous smaller, specialized models, dubbedexperts, with each honing in on a distinct sub-task.\\nSubsequently, these experts collaborate to deliver a consolidated output. For pre-traning or fine-tuning, MoE\\nrequires developers to manage a huge number of parameters efficiently, enhancing the model’s capacity and\\npotentially its performance while keeping the computational and memory requirements relatively manage-\\nable. For inference, MoE decreases the inference time by not engaging all experts simultaneously, but rather\\nactivating only a select few. Additionally, MoE is capable of minimizing communication between devices in\\nmodel-distributed scenarios by allocating each expert to an individual accelerator; communication is only\\nnecessary between the accelerators that host the router and the relevant expert model (Kaddour et al., 2023).\\nMoE-based LLMs.Several MoE-based LLMs have been proposed. For example, GShard (Lepikhin et al.,\\n2021) is a MoE-based LLM that offers a refined method to articulate a variety of parallel computation\\nframeworks with minor modifications to the existing model code. It also amplifies a multilingual neural\\nmachine translation Transformer model with Sparsely-Gated MoE beyond 600 billion parameters through\\nautomatic sharding. Switch Transformer (Fedus et al., 2022) brings forth a switch routing algorithm and\\ncraftsintuitivelyenhancedmodels, loweringcommunicationandcomputationalexpenditures. Itencompasses\\nup to one trillion parameters, dividing tasks among up to 2,048 experts, thereby illustrating the scalability\\n21\"), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 21, 'page_label': '22'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nand efficacy of the MoE framework. Artetxe et al. (2022) scale sparse language models to 1.1T parameters,\\ndiscerning superior performance up to this scale in language modeling, zero-shot and few-shot learning\\nin comparison to dense models. This suggests that sparse MoE models are a computationally efficient\\nsubstitute for traditionally employed dense architectures. Its largest MoE model outperforms its dense\\ncounterpart where the latter requires twice as much computation. BASE Layer (Lewis et al., 2021) defines\\ntoken-to-expert allocation as a linear assignment problem, allowing an optimal assignment where each expert\\nacquires an equal number of tokens, achieving lower validation perplexity during training relative to Switch\\nTransformer. PanGu-Σ (Ren et al., 2023b) is a MoE-based LLM with 1.085T parameters, transitioned from\\nthe dense Transformer model to a sparse one with Random Routed Experts (RRE), and effectively trains\\nthe model over 329B tokens utilizing Expert Computation and Storage Separation (ECSS). It outperforms\\ndense model like ERNIE 3.0 Titan Wang et al. (2021) on zero-shot test of Chinese downstream task. Lastly,\\nMixtral 8x7B (Jiang et al., 2023a) is a MoE with 46.7B total parameters. By leveraging the advantage of\\nMoE architecture, Mixtral 8x7B outperforms LLaMA-2 70B on most benchmarks such as MMLU, MBPP,\\nand GSM-8K with 6x faster inference by only using 12.9B parameters of the model per token for inference.\\nAlgorithm-Level MoE Optimization. The efficiency of MoE-based LLMs can be improved at the al-\\ngorithm level. Expert Choice (Zhou et al., 2022) allows experts to pick the top-k tokens instead of having\\ntokens choose the top-k experts, implying that each token can be directed to a variable number of experts\\nwhile each expert maintains a fixed bucket size. This method demonstrates higher performance in the GLUE\\nand SuperGLUE benchmarks, and outperforms T5 dense model in seven out of the 11 tasks. StableMoE\\n(Dai et al., 2022) identifies the issue of altering target experts for identical input during training and ad-\\ndresses this by creating two training phases. Initially, it cultivates a balanced routing strategy, which is\\nthen distilled into a decoupled lightweight router. In the following phase, this distilled router is used for a\\nfixed token-to-expert assignment, ensuring a stable routing strategy. StableMoE shows better results than\\nSwitch Transformer and BASE Layer with lower validation perplexity on language modeling. X-MoE (Chi\\net al., 2022) notes that earlier routing mechanisms foster token clustering around expert centroids, indicating\\na tendency toward representation collapse. It proposes to estimate the routing scores between tokens and\\nexperts on a low-dimensional hyper-sphere, showing improvements over Switch Transformer on multilingual\\nmulti-task benchmark. Lifelong-MoE (Chen et al., 2023f) observes that MoE increases the capacity of the\\nmodel to adapt to different corpus distributions in online data streams without extra computational cost,\\nsimply by incorporating additional expert layers and suitable expert regularization. This facilitates contin-\\nuous pre-training of a MoE-based LLM on sequential data distributions without losing previous knowledge.\\nIt outperforms other MoE models such as GShard on natural language generation and understanding tasks.\\nLastly, Shen et al. (2024) observe that compared to dense models, MoE gains more from instruction tuning.\\nBased on this observation, they propose Flan-MoE, which combines MoE and instruction tuning to en-\\nlarge language models without increasing demands in memory and compute resources while showing better\\nzero-shot and few-shot performance compared to FLAN-T5 dense model.\\nSystem-Level MoE Optimization.The efficiency of MoE-based LLMs can also be improved at the sys-\\ntem level. For example, FastMoE (He et al., 2021) is a distributed MoE training system built on PyTorch,\\ncompatible with common accelerators. This system offers a hierarchical interface that allows both flexi-\\nble model design and easy adaptation to various applications, such as Transformer-XL and Megatron-LM.\\nFasterMoE (He et al., 2022a) tries to address the challenges of dynamic load imbalance, inefficient syn-\\nchronous execution mode, and congested all-to-all communication during MoE training. It first introduces\\na performance model that predicts latency and analyzes end-to-end performance through a roofline-like\\nmethodology. Utilizing this model, it presents a dynamic shadowing technique for load balancing, a concur-\\nrent fine-grained schedule for operations, and a strategy to alleviate network congestion by adjusting expert\\nselection for model training. It outperforms FastMoE and achieves 1.37× - 17.87× speedup compared to\\nmethods including ZeRO, GShard, and BASE Layer. Lina (Li et al., 2023a) also improves training efficiency\\nand inference time by optimizing communication and load balancing in distributed MoE. Lina first prioritizes\\nall-to-all over allreduce using tensor partitioning and pipelining to improve its bandwidth in training, and\\nthen dynamically balances the workload with token-level expert selection pattern in inference. DeepSpeed-\\nMoE (Rajbhandari et al., 2022) has designed a Pyramid-Residual MoE (PR-MoE) architecture to enhance\\nboth the training and the inference efficiency of the MoE model parameter. PR-MoE is a dense-MoE hybrid\\nthat employs residual connections to optimally utilize experts, managing to reduce the parameter size by\\n22'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 22, 'page_label': '23'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nup to 3x without sacrificing quality or compute requirements. It serves massive MoE models with up to\\n4.5x faster and 9x cheaper inference compared to quality-equivalent dense models. DeepSpeed-MoE also\\nproposes a highly optimized MoE inference system which enables efficient scaling of inference workloads on\\nhundreds of GPUs, providing up to 7.3x reduction in inference latency and cost when compared with existing\\nMoE inference solutions. TA-MoE (Chen et al., 2022a) highlights that current MoE dispatch patterns do\\nnot fully leverage the underlying heterogeneous network environment and thus introduces a topology-aware\\nrouting strategy for large-scale MoE training that dynamically modifies the MoE dispatch pattern based\\non the network topology, making it outperform FastMoE, FasterMoE, and DeepSpeed-MoE. EdgeMoE (Yi\\net al., 2023) presents an on-device inference engine tailored for MoE-based LLMs. It optimizes memory and\\ncomputation for inference by distributing the model across different storage levels. Specifically, non-expert\\nmodel weights are stored directly on the edge device, while expert weights are kept externally and only\\nloaded into the device’s memory when necessary. Tutel (Hwang et al., 2023) proposes adaptive parallelism\\nand pipelining features to adapt to the dynamic workload. It employs a consistent layout for MoE param-\\neters and input data, supporting switchable parallelism and dynamic pipelining without any mathematical\\ninconsistencies or tensor migration costs, thus enabling free run-time optimization, achieving up to 5.75×\\nspeedup for a single MoE layer. SmartMoE (Zhai et al., 2023) focuses on the automatic parallelization for\\nMoE distributed training. In the offline stage, SmartMoE constructs a search space of hybrid parallelism\\nstrategies. In the online stage, it incorporates light-weight algorithms to identify the optimal parallel strat-\\negy. It achieves up to 1.88× speedup in end-to-end training over FasterMoE on a distributed training setting.\\nLastly, MegaBlocks (Gale et al., 2023) transforms MoE-oriented computation with block-sparse operations\\nand creates block-sparse GPU kernels to optimize MoE computation on hardware. This leads to training\\ntime up to 40% shorter compared to Tutel and 2.4x shorter than dense models trained with Megatron-LM.\\n2.5.3 Long Context LLMs\\nIn many real-world applications, such as multi-turn conversations and meeting summarization, existing LLMs\\nare often required to comprehend or generate context sequences that are much longer than what they have\\nbeen pre-trained with and may result in a degradation in accuracy due to the poor memorization for the long\\ncontext. One direct way to address this issue is to fine-tune LLMs with similar long-sequence data, which,\\nhowever, is time consuming and computation-intensive. To fill this gap, new methods have been developed\\nto enable LLMs to adapt to longer context lengths in a more efficient way. These methods can be in general\\ngrouped into four categories: extrapolation and interpolation, recurrent structure, segmentation and sliding\\nwindow, and memory-retrieval augmentation.\\nPositional Extrapolation and Interpolation. Standard positional encoding methods such as abso-\\nlute positional embeddings (APE) (Vaswani et al., 2017), learned positional embeddings (LPE) (Wang\\net al., 2022a), relative positional embeddings (RPE) (Shaw et al., 2018), and rotary position embeddings\\n(RoPE) (Su et al., 2023b) have advanced the integration of positional information in LLMs. For example,\\nLPE has been used by GPT-3 and OPT, RPE was used by Gopher (Rae et al., 2022) and Chinchilla (Hoff-\\nmann et al., 2022), whereas RoPE was used by LLaMA-1 and GLM-130B. However, it is still challenging to\\ntrain LLMs on sequences with a limited maximum length while still ensuring them to generalize well on sig-\\nnificantly longer sequences during inference. Given that, techniques based on positional extrapolation (Press\\net al., 2022; Sun et al., 2023c; Chen et al., 2024a) and positional interpolation (Chen et al., 2023d; Peng\\net al., 2024; Li et al., 2024a) have been proposed.\\nPositional extrapolation strategies extend the encoding of positional information beyond what the model\\nhas explicitly learned during training. For example, ALiBi (Press et al., 2022) applies attention with linear\\nbiases to attain extrapolation for sequences that exceed the maximum length seen during training. By ap-\\nplying negatively biased attention scores with a linearly diminishing penalty based on the distance between\\nthe pertinent key and query, it facilitates efficient length extrapolation. Different from ALiBi, xPOS (Sun\\net al., 2023c) characterizes attention resolution as a marker for extrapolation and utilizes a relative posi-\\ntion embedding to enhance attention resolution, thereby improving length extrapolation. However, these\\ntechniques have not been implemented in some of the recent LLMs such as GPT-4, LLaMA, and LLaMA-\\n2. CLEX (Chen et al., 2024a) proposes to generalize position embedding scaling with ordinary differential\\n23'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 23, 'page_label': '24'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nequations to model continuous dynamics over length scaling factors. In doing so, CLEX gets rid of the\\nlimitations of existing positional extrapolation scaling methods to enable long-sequence generation.\\nPositional interpolation strategies, on the other hand, reduce the scale of input position indices and extend\\nthe context window sizes, allowing LLMs to maintain their performance over longer text sequences. For\\nexample, Chen et al. (2023d) observe that extending beyond the trained context length could impair the\\nself-attention mechanism. They propose RoPE-PI to reduce the position indices through linear interpolation,\\naligning the maximum position index with the prior context window limit encountered during pre-training.\\nNTK interpolation (bloc97, 2023) modifies the base of the RoPE, effectively changing the rotational velocity\\nof each RoPE dimension. YaRN interpolation (Peng et al., 2024) uses a ramp function to blend linear\\nand NTK interpolation in varying proportions across dimensions and incorporates a temperature factor to\\ncounteract distribution shifts in the attention matrix caused by long inputs. Experimental results on long-\\ntext modeling shows that YaRN outperforms existing RoPE interpolation methods including RoPE-PI and\\nNTK. FIRE (Li et al., 2024a) proposes a functional relative position encoding using learnable mapping of\\ninput positions to biases and progressive interpolation, ensuring bounded input for encoding functions across\\nall sequence lengths to enable length generalization. It demonstrates competitive results compared to ALiBi,\\nRoPE, and RoPE-PI on long text benchmarks. Lastly, PoSE (Zhu et al., 2024) proposes positional skip-wise\\ntraining that simulates long inputs using a fixed context window and designs distinct skipping bias terms\\nto manipulate the position indices of each chunk. This strategy reduces memory and time consumption\\ncompared to full-length fine-tuning.\\nRecurrent Structure.LLMs’ ability to manage long sequences can also be enhanced through recurrence\\nstructure. For example, Transformer-XL (Dai et al., 2019) presents a segment-level recurrence mechanism\\nand utilizes enhanced relative positional encoding to capture long-term dependencies and address the long-\\ncontext fragmentation issue. Memformer (Wu et al., 2022a) leverages an external dynamic memory for\\nencoding and retrieving past information, achieving linear time and constant memory space complexity\\nfor long sequences. It also proposes Memory Replay Back-Propagation (MRBP) to facilitate long-range\\nback-propagation through time with significantly lower memory requirements, achieving better results than\\nTransformer-XL on language modeling and image generation benchmarks.∞-former (Martins et al., 2022)\\npresentsaTransformermodelaugmentedwithunboundedlong-termmemory(LTM).Itemploysacontinuous\\nspace attention framework to balance the quantity of information units accommodated in memory against\\nthe granularity of their representations, showing better results than Transformer-XL on long text sorting\\nand modeling. Recurrent Memory Transformer (RMT) (Bulatov et al., 2022) uses a recurrence mechanism\\nto retain information from the past segment level by incorporating special memory tokens into the input\\nor output sequence, and demonstrates superior performance compared to Transformer-XL in long context\\nmodeling. Block-Recurrent Transformer (BRT) (Hutchins et al., 2022) utilizes self-attention and cross-\\nattention to execute a recurrent function across a broad set of state vectors and tokens so as to model long\\nsequences through parallel computation. Lastly, Retentive Network (Sun et al., 2023b) introduces a multi-\\nscale retention mechanism as an alternative to multi-head attention. By leveraging parallel and chunk-wise\\nrecurrent representations, it enables effective scaling, achieves training parallelization and constant inference\\ncost, and offers linear long-sequence memory complexity compared to other Transformer models.\\nSegmentation and Sliding Window.Segmentation and sliding window techniques tackle the issue of\\nlong-context processing by dividing the input data into smaller segments, or applying a moving window to\\nslide through the long sequence. For instance, Mistral (Jiang et al., 2023a) uses sliding window attention\\nto handle sequences of arbitrary length with a reduced inference cost. StreamingLLM (Xiao et al., 2024)\\nidentifies an attention sink phenomenon, noting that retaining the Key-Value of initial tokens significantly\\nrestores the performance of window attention. Based on this observation, it suggests an efficient frame-\\nwork via merging window context and the first token, allowing LLMs trained with a finite length attention\\nwindow, but have the ability to generalize to infinite sequence lengths without any fine-tuning. Parallel\\nContext Windows (PCW) (Ratner et al., 2023) segments a long context into chunks, limiting the attention\\nmechanism to function only within each window, and then re-deploys the positional embeddings across these\\nwindows. LongNet (Ding et al., 2023a) proposes dilated attention, which exponentially expands the attentive\\nfield as the distance increases, enabling the handling of sequence lengths of more than one billion tokens.\\nSLED (Ivgi et al., 2023) handles long sequences by partitioning the long text into small chunks and leveraging\\n24'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 24, 'page_label': '25'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\npretrained short-text language models for encoding and decoding. Different from the methods mentioned\\nabove, MemWalker (Chen et al., 2023c) transforms lengthy texts into segmented summaries within a tree\\nstructure, leveraging LLMs as an interactive entity for guided reading through iterative prompts. It out-\\nperforms traditional methods based on extended context, recurrence, and retrieval. Similar to MemWalker,\\nRAPTOR (Sarthi et al., 2024) utilizes text chunks to construct a recursive tree to enable information in-\\ntegration from extensive documents across various abstraction levels during inference, achieving superior\\nperformance in multi-step reasoning.\\nMemory-Retrieval Augmentation.Lastly, several studies tackle the inference of extremely long text by\\nemployingmemory-retrievalaugmentationstrategies. AnotableexampleistheMemorizingTransformer(Wu\\net al., 2022c), which extends the attention context size by utilizing k-nearest-neighbor (KNN) lookup to fetch\\npreviously similar context embeddings. Additionally, Landmark Attention (Mohtashami & Jaggi, 2023) em-\\nploys a landmark token to represent each block of input and trains the attention mechanism to utilize it for\\nchoosing relevant blocks. This allows the direct retrieval of blocks through the attention mechanism while\\nmaintaining the random access flexibility of the previous context, demonstrating comparable perplexity as\\nTransformer-XL while reducing FLOPs for long-context modeling. LongMem (Wang et al., 2023e) proposes\\na decoupled network architecture with the original backbone LLM as a memory encoder and an adaptive\\nresidual side network as a memory retriever and reader, efficiently caching and updating long-term past\\ncontexts to prevent knowledge staleness. It outperforms Memorizing Transformer on long text modeling and\\nnatural language understanding tasks. Unlimiformer (Bertsch et al., 2023) enhances the KNN-augmented\\nTransformer by outputting attention dot-product scores as KNN distances to enable indexing of virtually\\nunlimited input sequences. Experimental results show that Unlimiformer outperforms Memorizing Trans-\\nformer on long document summarization benchmarks. Tworkowski et al. (2023) observe that the ratio of\\nrelevant keys to irrelevant ones diminishes as context length increases. Based on this observation, they\\npropose Focused Transformer (FoT), a contrastive learning-based technique to refine the structure of the\\nKey-Value space. Unlike Memorizing Transformer and Transformer-XL, FoT does not require training on\\nlong sequences and shows better performance on both long context and short context tasks. Lastly, Xu et al.\\n(2024a) discover that an LLM with a 4K context window, when augmented with simple retrieval during\\ngeneration, can match the performance of a fine-tuned LLM with a 16K context window using positional\\ninterpolation (Chen et al., 2023d) on long context tasks while requiring significantly less computation.\\n2.5.4 Transformer-Alternate Architectures\\nWhile Transformer-based architectures are now at the forefront of LLMs, some studies propose new archi-\\ntectures to supplant Transformer-based architectures.\\nState Space Models. A promising approach that aims to substitute the attention mechanism is state space\\nmodels (SSMs). SSM is formulated asx′(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t), which maps a single-\\ndimension input signalu(t) to an N-dimension latent statex(t) before projecting to a single-dimension output\\nsignal y(t), where A, B, C, D are parameters learned by gradient descent (Gu et al., 2022a). Compared\\nto attention that has quadratic complexity, SSMs provide near-linear computational complexity related to\\nthe length of the sequence. Given such advantage, a series of techniques have been proposed to improve\\nSSMs. For example, the Structured State Space (S4) sequence model (Gu et al., 2022a) refines SSMs by\\nconditioning matrix A with a low-rank correction. This enables stable diagonalization and simplifies the\\nSSM to the well-studied computation of a Cauchy kernel. Diagonal State Space (DSS) (Gupta et al., 2022)\\nimproves SSMs by proposing fully diagonal parameterization of state spaces instead of a diagonal plus\\nlow rank structure. To bridge the gap between SSMs and attention while adapting to modern hardware,\\nH3 (Hungry Hungry Hippo) (Fu et al., 2023a) stacks two SSMs to interact with their output and input\\nprojection, allowing it to log tokens and facilitate sequence-wide comparisons. Mehta et al. (2023) introduce\\na more efficient layer called Gated State Space (GSS), which has been empirically shown to be 2 to 3 times\\nfaster than DSS while maintaining the perplexity on multiple language modeling benchmarks. Block-State\\nTransformer (BST) (Pilault et al., 2023) designs a hybrid layer that combines an SSM sublayers for extended\\nrange contextualization with a Block Transformer sublayer for short-term sequence representation. Gu &\\nDao (2023) propose Mamba to enhance SSMs by designing a selection mechanism to eliminate irrelevant\\ndata and develop a hardware-aware parallel algorithm for recurrent operation, achieving 5x throughput than\\n25'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 25, 'page_label': '26'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nData Selection\\nData Selection for Efficient Pre-TrainingSSPT (Glass et al., 2020), Yao et al. (2022a), DSIR (Xie et al., 2023b), DoReMi (Xie et al., 2023a)\\nData Selection for Efficient Fine-TuningIvison et al. (2023), Instruction Mining (Cao et al., 2023), TS-DShapley (Schoch et al., 2023),\\nLTD Instruction Tuning (Chen et al., 2023b), AlpaGasus (Chen et al., 2024b), LIMA (Zhou et al., 2023a)\\nFigure 16: Summary of data selection techniques for LLMs.\\nLLMs \\nsubset subset\\nTraining Data Fine-Tuning Data\\nSelect Select Training Fine- \\nTuning \\n(a) Data Selection for Efﬁcient Pre-Training (b) Data Selection for Efﬁcient Fine-Tuning\\nFigure 17: Illustrations of data selection techniques for LLMs.\\nTransformers. Ren et al. (2023a) extend SSMs and propose a general modular activation mechanism named\\nSparse Modular Activation (SMA), which unifies MoE, adaptive computation, dynamic routing and sparse\\nattention, and further applies SMA to develop a novel architecture, SeqBoat, to achieve state-of-the-art\\nquality-efficiency trade-off.\\nOther Sequential Models. Some other architectures have been proposed to replace the Transformer\\nlayer. For instance, Receptance Weighted Key Value (RWKV) model (Peng et al., 2023b) combines the\\nadvantages of recurring neural networks (RNN) and Transformers. Such combination utilizes the effective\\nparallelizable training feature of Transformers coupled with the efficient inference ability of RNNs, thereby\\neffectively tackling the challenges associated with long sequence processing, outperforming Transformer-\\nbased models such as BLOOM and OPT. Poli et al. (2023) propose Hyena, a sub-quadratic alternative to\\nthe attention mechanism to mitigate the quadratic cost in long sequences. This operator includes two efficient\\nsub-quadratic primitives: an implicit long convolution and multiplicative element-wise gating of the input.\\nHyena facilitates the development of larger, more efficient convolutional language models for long sequences\\nand outperforms RWKV and GPT-Neo on SuperGLUE tasks (Wang et al., 2019). Lastly, MEGABYTE (YU\\net al., 2023) breaks down long sequences into fixed-sized patches akin to tokens, comprising a patch embedder\\nfor encoding, a global module acting as a large autoregressive Transformer for patch representations, and a\\nlocal module for predicting bytes within a patch.\\n3 Data-Centric Methods\\n3.1 Data Selection\\nData selection is a fundamental technique for enhancing efficiency. As summarized in Figure 16, in the\\ncontext of LLMs, data selection techniques have been primarily used for enhancing the efficiency of pre-\\ntraining and fine-tuning.\\n3.1.1 Data Selection for Efficient Pre-Training\\nData selection enhances LLMs pre-training efficiency by strategically selecting informative and diverse data\\nsamples during training. For example, SSPT (Glass et al., 2020) is a pre-training task based on the principles\\nof reading comprehension. It involves selecting answers from contextually relevant text passages, which has\\nshown notable improvements in performance across various Machine Reading Comprehension benchmarks.\\nYao et al. (2022a) propose a meta-learning-based method for selecting linguistically informative sentences\\nwhich significantly elevates the quality of machine-generated translations. Xie et al. (2023b) propose DSIR,\\n26'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 26, 'page_label': '27'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\na data selection method based on importance resampling for both general-purpose and specialized LLMs. It\\ncalculates how important different pieces of data are within a simpler set of features and chooses data based\\non these importance calculations. Experimental results demonstrate that DSIR achieves similar performance\\nto expert curation across eight different target distributions. In the context of pre-training general-domain\\nmodels, DSIR outperforms random selection and heuristic filtering baselines by 2–2.5% on the GLUE bench-\\nmark. Different from DSIR, Xie et al. (2023a) design DoReMi to address the distribution shift between\\npre-training and downstream tasks, which is also a critical problem for training data selection.\\n3.1.2 Data Selection for Efficient Fine-Tuning\\nData selection can also boost LLM fine-tuning efficiency given that only a curated subset of examples is\\nemployed to refine the model. For example, Ivison et al. (2023) propose to use a few unlabeled data samples\\nto retrieve similar labeled ones from a larger multitask dataset, improving task-specific model training. This\\nmethod outperforms standard multitask data sampling for fine-tuning and enhances few-shot fine-tuning,\\nyielding an 2-23% relative improvement. With the success of instruction tuning, many studies start focusing\\non the selection of high-quality instruction data to fine-tune LLMs. For example, Instruction Mining (Cao\\net al., 2023) presents a linear evaluation method to assess data quality in instruction-following tasks. It\\nhighlights the importance of high-quality data, showing that models trained with Instruction Mining-curated\\ndatasets outperform those trained on generic datasets in 42.5% of the considered cases. TS-DShapley (Schoch\\net al., 2023) is introduced to address the computational challenges of applying Shapley-based data valuation\\ntofine-tuningLLMs. Itemploysanefficientsampling-basedmethodthataggregatesShapleyvaluescomputed\\nfrom subsets to evaluate the entire training set. Low Training Data Instruction Tuning (LTD Instruction\\nTuning) (Chen et al., 2023b) challenges the need for large datasets in fine-tuning, showing that less than 0.5%\\nof the original dataset is able to effectively train task-specific models without compromising performance.\\nThis approach enables more resource-efficient practices in data-scarce environments, combining selective\\ndata strategies with tailored training protocols for optimal data efficiency. AlpaGasus (Chen et al., 2024b)\\nis a model fine-tuned on a mere 9K high-quality data samples, which are meticulously filtered from a larger\\ndataset of 52K. It outperforms the original model trained on the full dataset and reduces the fine-tuning time\\nby 5.7x, demonstrating the power of high-quality data in instruction-fine-tuning. Lastly, LIMA (Zhou et al.,\\n2023a) fine-tunes LLMs with a small, selected set of examples, showing strong performance and challenging\\nthe need for extensive tuning. It generalizes well to new tasks, matching or exceeding GPT-4 in 43% of the\\nconsidered cases.\\n3.2 Prompt Engineering\\nPrompt engineering (Liu et al., 2023a) focuses on designing effective inputs (i.e., prompts) to guide LLMs\\nin generating desired outputs. It enhances inference efficiency by tailoring the input prompts or queries\\nto better suit the capabilities of a specific language model. When used for some simple tasks, such as\\nsemantic classification, prompt engineering can even substitute fine-tuning to achieve high accuracy (Liu\\net al., 2022a). As summarized in Figure 18, prompt engineering techniques can in general be grouped into\\nfew-shot prompting, prompt compression, and prompt generation.\\n3.2.1 Few-Shot Prompting\\nFew-shot prompting aims to provide a LLM with a limited set of examples, referred to as demonstrations,\\nto steer its understanding to a task it is required to execute (Wei et al., 2022a). These demonstrations are\\nselectedfromthetrainingcorpusbasedontheirsimilaritytothetestexample, andtheLLMisexpectedtouse\\nthe knowledge gained from these similar demonstrations to make the correct prediction (Dong et al., 2023).\\nFew-shot prompting provides an efficient mechanism to use LLM by guiding the LLM to perform a wide\\nvariety of tasks without the need for additional training or fine-tuning. Furthermore, an effective few-shot\\nprompting approach can make the created prompt concise enough to allow LLMs to quickly adjust to the task\\nin high accuracy with only a slight increase of extra context, thus significantly improving inference efficiency.\\nAs illustrated in Figure 19, few-shot prompting techniques can in general be grouped into demonstration\\norganization and template formatting.\\n27'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 27, 'page_label': '28'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nPrompt Engineering\\nFew-Shot Prompting\\nDemonstration Organization\\nDemonstration Selection\\nKATE (Liu et al., 2022b), VoteK (SU et al., 2023), Wang et al. (2023f),IDS (Qin et al., 2023a), Min et al. (2022b), LENS (Li & Qiu, 2023),MDL (Wu et al., 2023c), Zhang et al. (2022b), EPR (Rubin et al., 2022),UDR (Li et al., 2023f), Wang et al. (2024b), Luo et al. (2023)\\nDemonstration OrderingLu et al. (2022)\\nTemplate Formatting\\nInstruction GenerationInstruction Induction (Honovich et al., 2023),Automatic Prompt Engineer (Zhou et al., 2023c), Self-Instruct (Wang et al., 2023h),OPRO (Yang et al., 2023a), TeGit (Chen et al., 2023h)\\nMulti-Step Reasoning\\nChain-of-Thought (Wei et al., 2022b), Auto-CoT (Zhang et al., 2023g), Self-Ask (Press et al., 2023),ReAct (Yao et al., 2023b), Least-to-Most Prompting (Zhou et al., 2023b),Tree-of-Thought (Yao et al., 2023a), CoT-SC (Wang et al., 2023h),Graph of Thoughts (Besta et al., 2024), Contrastive CoT (Chia et al., 2023),XoT (Ding et al., 2023b), Skeleton-of-Thought (Ning et al., 2024)\\nPrompt CompressionGisting (Mu et al., 2023), AutoCompressors (Chevalier et al., 2023), PCRL (Jung & Kim, 2023),ICAE (Ge et al., 2024), Nugget 2D (Qin et al., 2023b), LongLLMLingua (Jiang et al., 2024)\\nPrompt GenerationAutoPrompt (Shin et al., 2020), TempLM (Zhang et al., 2023e), PromptGen (Zhang et al., 2022c)\\nFigure 18: Summary of prompt engineering techniques for LLMs.\\nThis food tastes good.\\nThe review is __\\nTraining Data\\nIt is a nice restaurant.\\nThe review is positive.\\nThis t-shirt looks cool.\\nThe review is positive.\\nThe dining room is dirty.\\nThe review is negative.\\nembedding \\ngeneration \\nselect \\nInput\\nIt is a nice restaurant.\\nThe review is positive. \\nThe dining room is dirty.\\nThe review is\\xa0negative.\\nThis t-shirt looks cool.\\nThe review is\\xa0positive.\\nIs the review positive or negative?\\xa0\\nIt is a nice restaurant. The review is positive.\\nThe dining room is dirty. The review is\\xa0negative.\\nThis t-shirt looks cool. The review is\\xa0positive.\\nIs the review positive or negative? \\nIt is a nice restaurant. The sentence contains a positive\\nword \"nice\". The review is positive. \\nThe dining room is dirty. The sentence contains a negative\\nword \"dirty\".The review is negative. \\nThis t-shirt looks cool. The sentence contains a positive\\nword \"cool\". The review is positive.LLMs \\nThis food tastes good.\\nThe review is positive\\nOutput\\nDemonstration Selection Demonstration Ordering\\nInstruction GenerationMulti-Step Reasoning\\n(a) Demonstration Organization\\n(b) Template Formatting\\nembedding \\nFigure 19: Illustrations of few-shot prompting techniques for LLMs.\\nDemonstration Organization.Demonstration organization refers to organizing the demonstrations in an\\nappropriate way so as to form a suitable prompt for inference. Demonstration organization has a significant\\nimpact on the inference efficiency since improper organization may result in the processing of a considerable\\namount of unnecessary information, leading to significant slowdown. The optimization of demonstration\\norganization comes from its two main steps: demonstration selection and demonstration ordering.\\n• Demonstration Selection. Demonstration selection aims to choose the good examples for few-\\nshot prompting (Dong et al., 2023). In order to generate a satisfactory result, a good selection of\\ndemonstrations may only require a few number of demonstrations to be used for the prompt, thus\\nmakingthepromptconciseandstraightforwardforamoreefficientinference. Existingdemonstration\\nselection techniques can be grouped into unsupervised methods (Liu et al., 2022b; SU et al., 2023;\\nWang et al., 2023f; Qin et al., 2023a; Min et al., 2022b; Li & Qiu, 2023; Wu et al., 2023c; Zhang\\net al., 2022b) and supervised methods (Rubin et al., 2022; Li et al., 2023f; Wang et al., 2024b; Luo\\n28'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 28, 'page_label': '29'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\net al., 2023). Unsupervised methods aim to select the nearest examples from the training set using\\na predefined similarity function, such as L2 distance, cosine distance, and minimum description\\nlength (MDL) (Wu et al., 2023c). For example, KATE (Liu et al., 2022b) is an unsupervised\\nselection method that directly uses the nearest neighbors of a given test sample as the corresponding\\ndemonstrations. VoteK (SU et al., 2023) is an improved version of KATE to resolve its limitation\\nthatrequiresalarge setofexamplestoachievegoodperformance. Unlike KATE,VoteKincreasesthe\\ndiversity of the demonstrations by penalizing examples similar to those already selected. In contrast,\\nsupervised methods require training a domain-specific retriever from the training set and using it for\\ndemonstration selection. For example, EPR (Rubin et al., 2022) is trained to select demonstrations\\nfrom a small set of candidates initialized by the unsupervised retriever such as BM25 from the\\ntraining corpus. UDR (Li et al., 2023f) further enhances EPR by adopting a unified demonstration\\nretriever to unify the demonstration selection across different tasks. Compared to unsupervised\\nmethods, supervised methods often lead to a more satisfying generation result but require frequent\\nadjustment of the retriever for handling the out-of-domain data, making them relatively less efficient\\nfor inference.\\n• Demonstration Ordering.After selecting representative samples from the training set, the next\\nstep is ordering these samples in the prompt. The order of the demonstrations also has a significant\\nimpact on the performance of the model. Therefore, selecting the right order of demonstrations\\ncan help the model quickly reach a good generation quality with fewer samples, thus improving\\nthe inference efficiency. To date, only a few studies have delved into this area. For example, Liu\\net al. (2022b) suggest arranging demonstrations based on their distance from the input, placing the\\nclosest demonstration furthest to the right. Lu et al. (2022) propose to develop both global and\\nlocal entropy metrics and use the entropy metrics to set up the demonstration order.\\nTemplate Formatting.Template formatting aims to design a suitable template to form the prompt. A\\ngood template typically compiles all the information needed by LLMs into a brief statement, making the\\nprompt and the entire input context as succinct as possible, thus leading to a higher inference efficiency.\\nTemplate formatting can be divided into two parts: instruction generation and multi-step reasoning.\\n• Instruction Generation.The instruction of the template refers to a short description of the task.\\nBy adding instructions to the prompt, LLMs can quickly understand the context and the task they\\nare currently performing, and thus may require fewer demonstrations to create a desirable prompt.\\nThe performance of a given task is highly affected by the quality of the instructions. The instructions\\nvary not only between different datasets for the same task but also between different models. Unlike\\ndemonstrations that are usually included in traditional datasets, the generation of instructions is\\nheavily dependent on human efforts. To enhance the efficiency of instruction generation, automatic\\ninstructiongenerationtechniqueshavebeenproposed. Forexample, InstructionInduction(Honovich\\net al., 2023) and Automatic Prompt Engineer (Zhou et al., 2023c) demonstrate that LLMs can\\ngenerate task instructions. Wang et al. (2023h) propose Self-Instruct, an approach that allows LLMs\\nto align with self-generated instructions, highlighting their inherent adaptability. Experimental\\nresults show that it achieves an 33% improvement over the original model when applied on vanilla\\nGPT3. Yang et al. (2023a) also discover that LLMs can be treated as an optimizer to iteratively\\ngenerate better instructions for the target LLM and have applied this technique to various LLMs.\\nExperiments demonstrate that the best prompts optimized by this method outperform human-\\ndesigned prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Chen et al.\\n(2023h) develop TeGit for training language models as task designers, which can automatically\\ngenerate inputs and outputs together with high-quality instructions to better filter the noise based\\non a given human-written text for fine-tuning LLMs. Despite the promise of automatic instruction\\ngeneration methods, their complexity is still a major bottleneck for their real-world adoption.\\n• Multi-Step Reasoning. Multi-step reasoning (Huang & Chang, 2023) refers to techniques that\\nguide the LLMs to produce a sequence of intermediate steps before outputting the final answer.\\nCompared to fine-tuning, conducting specific task reasoning directly through this way is a more effi-\\ncient approach. At the same time, its accuracy improvement is often not as good as fine-tuning. One\\n29'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 29, 'page_label': '30'}, page_content=\"Published in Transactions on Machine Learning Research (May/2024)\\nLMs \\nCompress \\nA B C T est \\nA' C' B' T est \\nDemonstrations Input\\nT est \\nInput\\nA' T est LMs \\nGenerate \\n(a) Prompt Compression (b) Prompt Generation\\nFigure 20: Illustrations of prompt compression (a) and prompt generation (b) for LLMs.\\nof the widely used techniques in multi-step reasoning is Chain-of-Thought (CoT) prompting (Wei\\net al., 2022b), which adds a series of reasoning steps into the prompt to make it more informative\\nand comprehensive. Despite its advantages, it is still difficult for CoT to ensure the accuracy of\\nevery intermediate step (Dong et al., 2023). A number of techniques have been proposed to address\\nthis issue. For example, Auto-CoT (Zhang et al., 2023g) proposes to generate the CoT step by\\nstep from LLMs. Self-Ask (Press et al., 2023) incorporates the self-generated questions of each step\\ninto CoT. ReAct (Yao et al., 2023b) performs dynamic reasoning to create, maintain, and adjust\\nhigh-level plans for acting, while interacting with external environments to incorporate additional\\ninformation into reasoning. Least-to-Most Prompting (Zhou et al., 2023b) is a new milestone in\\nCoT. It breaks down the complex question into smaller ones and answers them iteratively within the\\ncontext of former questions and answers. Experimental results show that Least-to-Most Prompting\\ncan boost the accuracy of GPT-3 code-davinci-002 model with CoT prompting to 99.7% by using\\njust 14 exemplars. Tree-of-Thought (ToT) (Yao et al., 2023a) expends CoT to include exploration\\nover coherent units of text and deliberates decision-making processes. It outperforms GPT-4 with\\nCoT prompting in the Game of 24 benchmark. CoT-SC (Wang et al., 2023h) introduces a novel\\ndecoding approach called self-consistency to replace the greedy decoding in CoT prompting. It\\nstarts by sampling various reasoning paths instead of just the greedy one and then determines the\\nmost consistent answer by considering all the sampled paths. Graph of Thoughts (GoT) (Besta\\net al., 2024) represents information produced by an LLM as a generic graph, with “LLM thoughts”\\nas vertices and edges indicating dependencies between these vertices. Experimental results show\\nthat GoT increases the quality of sorting by 62% over ToT while reducing the cost by over 31%.\\nContrastive CoT (Chia et al., 2023) proposes to enhance language model reasoning by providing\\nboth valid and invalid reasoning demonstrations. XoT (Ding et al., 2023b) utilizes pretrained rein-\\nforcement learning and Monte Carlo Tree Search (MCTS) to integrate external domain knowledge\\ninto the thought processes of LLMs, thereby boosting their ability to efficiently generalize to new,\\nunseen problems. Lastly, Skeleton of Thought (SoT) (Ning et al., 2024) proposes a method that first\\nprompts the LLM to organize the output and then parallelizes the generation of different segments.\\nThrough splitting the serial generation into two different steps, it makes the generation workload\\nmore parallelizable. Therefore, it improves hardware utilization and provides speedups across LLM,\\nrevealing the possibility of exploiting data-level organization to enhance efficiency.\\n3.2.2 Prompt Compression\\nPrompt compression (Figure 20(a)) accelerates the processing of LLM inputs through either condensing\\nlengthy prompt inputs or learning compact prompt representations. Mu et al. (2023) propose to train LLMs\\nto distill prompts into a more concise set of tokens, referred to as gist tokens. These gist tokens encapsulate\\nthe knowledge of the original prompt and can be stored for future use. In doing so, it is able to compress\\nprompts by up to 26x, leading to a reduction in floating-point operations per second (FLOPs) by up to\\n30\"), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 30, 'page_label': '31'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\n40%. Chevalier et al. (2023) propose AutoCompressors to condense long textual contexts into compact\\nvectors, known as summary vectors, which can then be used as soft prompts for the language model. These\\nsummary vectors extend the model’s context window, allowing it to handle longer documents with much less\\ncomputational cost. In particular, AutoCompressors can utilize long contexts to improve perplexity of both\\nfine-tuned OPT and LLaMA-2 on sequences of up to 30,720 tokens. Jung & Kim (2023) propose Prompt\\nCompression with Reinforcement Learning (PCRL) that employs a policy network to directly edit prompts,\\naiming to reduce token count while preserving performance. It achieves an average reduction of 24.6% in\\ntoken count across various instruction prompts. Ge et al. (2024) propose In-context Autoencoder (ICAE),\\nwhich consists of a learnable encoder and a fixed decoder. The encoder compresses a long context into a\\nlimited number of memory slots, which the target language model can then condition on. With such design,\\nICAE is able to obtain 4x context compression. Nugget 2D (Qin et al., 2023b) represents the historical\\ncontext as compact “nuggets” that are trained to enable reconstruction. Furthermore, it has the flexibility\\nto be initialized using readily available models like LLaMA. Eperimental results show that Nugget 2D\\ncompresses context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly\\nlossless encoding. Lastly, LongLLMLingua (Jiang et al., 2024) introduces a prompt compression technique\\ncontaining question-aware coarse-to-fine compression, document reordering, dynamic compression ratios, and\\npost-compression sub-sequence recovery to enhance LLMs’ key information perception. Experimental results\\nshow that LongLLMLingua achieves 17.1% better performance over the original prompt with 4x fewer tokens\\nas input to GPT-3.5-Turbo.\\n3.2.3 Prompt Generation\\nPrompt generation (Figure 20(b)) enhances efficiency by automatically creating effective prompts that guide\\nthe model in generating specific and relevant responses. For instance, AutoPrompt (Shin et al., 2020)\\nproposes an automated method to generate prompts for a diverse set of tasks based on a gradient-guided\\nsearch. It underscores the significance of human-written text in refining the quality and authenticity of\\ndata, emphasizing its pivotal role in optimizing LLM performance. Experimental results demonstrate that\\nAutoPrompt outperforms manually created prompts on the LAMA benchmark in eliciting more precise\\nfactual knowledge from LLM. TempLM (Zhang et al., 2023e) proposes to combine generative and template-\\nbased methodologies to distill LLMs into template-based generators, offering a harmonized solution for\\ndata-to-text tasks. TempLM not only reduces the unfaithfulness rate of a fine-tuned BART model from 83%\\nto 0%, but also substantially improves upon human-written ones in BERTScore. Lastly, PromptGen (Zhang\\net al., 2022c) considers dynamic prompt generation for knowledge probing based on pre-trained LLMs. It\\nautomatically generates prompts conditional on the input sentence and outperforms AutoPrompt on the\\nLAMA benchmark.\\n4 LLM Frameworks\\nLLM frameworks can be in general grouped based on whether they support the tasks of training, fine-\\ntuning, and inference. Specifically, frameworks that support training and/or fine-tuning aim to provide\\nscalable, efficient, and flexible infrastructure that improves computation efficiency, reduces memory footprint,\\noptimizes communication efficiency, and ensures reliability of the training/fine-tuning process. Frameworks\\nthat support inference focus on optimizing inference throughput and reducing memory footprint and latency.\\nThese frameworks offer a variety of deployment features to serve LLM requests. Table 2 provides a summary\\nof existing LLM frameworks along with their key features.\\nDeepSpeed. Developed by Microsoft, DeepSpeed (Rasley et al., 2020) is an integrated framework for both\\ntraining and serving LLMs. It has been used to train large models like Megatron-Turing NLG 530B (Smith\\net al., 2022) (in a joint effort with Nvidia Megatron framework) and BLOOM (Scao et al., 2023). Within\\nthis framework, DeepSpeed-Inference is the foundational library. A pivotal feature of DeepSpeed-Inference is\\nZeRO-Inference (Rajbhandari et al., 2020; 2021), an optimization technique created to address GPU memory\\nconstraints for large model inference. ZeRO-Inference distributes model states across multiple GPUs and\\nCPUs, providing an approach to managing the memory constraints of GPUs.\\n31'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 31, 'page_label': '32'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTable 2: Comparison of LLM frameworks.\\nFramework Training Fine-Tuning Inference Key Features\\nDeepSpeed /check-circle /check-circle /check-circle3D Parallelism with ZeRO (Rasley et al., 2020),\\nZeRO-2 (Rajbhandari et al., 2020), ZeRO In-\\nfinity (Rajbhandari et al., 2021) and ZeRO-\\nOffload (Ren et al., 2021), Expert Parallelism (Ra-\\njbhandari et al., 2022), FlashAttention (Dao et al.,\\n2022), PagedAttention (Kwon et al., 2023), Dy-\\nnamic SplitFuse (Holmes et al., 2024), Continuous\\nBatching (Yu et al., 2022), ZeroQuant (Wu et al.,\\n2023b; Yao et al., 2023d), INT4 Quantization (Wu\\net al., 2023a), XTC (Ternary quantization) (Wu\\net al., 2022b), RLHF (Yao et al., 2023c), Kernel\\nOptimizations, Diverse Hardware Support.\\nMegatron /check-circle /check-circle /check-circle3D Parallelism (Shoeybi et al., 2020; Narayanan\\net al., 2021), Sequence Paralellism (Korthikanti\\net al., 2023; Li et al., 2023d), Expert Par-\\nallelism (Singh et al., 2023), FasterTrans-\\nformer (NVIDIA, 2023a), FlashAttention (Dao\\net al., 2022), Selective Activation Recomputa-\\ntion (Korthikanti et al., 2023).\\nColossal-AI /check-circle /check-circle /check-circle3D Parallelism (Xu & You, 2023; Wang et al.,\\n2023a; Bian et al., 2021), Sequence Parallelism (Li\\net al., 2023d), ZeRO Optimizer (Zhao et al.,\\n2022), Auto-Parallelism (Liu et al., 2023c), Het-\\nerogeneous Memory Management (Fang et al.,\\n2023), Expert Parallelism (Singh et al., 2023;\\nXue et al., 2023), PagedAttention (Kwon et al.,\\n2023), FlashAttention-2 (Dao, 2024), Quan-\\ntization (GPTQ (Frantar et al., 2023) and\\nSmoothQuant (Xiao et al., 2023)), RLHF (Grif-\\nfith et al., 2013; Singh et al., 2023).\\nNanotron /check-circle /check-circle /check-circle3D Parallelism (Narayanan et al., 2021; Huang\\net al., 2019), Expert Parallelism (Singh et al.,\\n2023), ZeRO Optimizer (Zhao et al., 2022),\\nSSM (Gu & Dao, 2023) Support, Spectral\\nµTransfer Parametrization (Yang et al., 2022),\\nDoReMi (Xie et al., 2023a).\\nMegaBlocks /check-circle /check-circle /check-circleFSDP (Zhao et al., 2023c), dropless-MoE (Gale\\net al., 2023), Integration with Megatron and\\nvLLM.\\nFairScale /check-circle /check-circle /check-circleFSDP (Zhao et al., 2023c), Pipeline Paral-\\nlelism (Huang et al., 2019), AdaScale opti-\\nmizer (Johnson et al., 2020).\\nPax /check-circle /check-circle /check-circleData Parallelism, Tensor Parallelism (Shoeybi\\net al., 2020).\\nComposer /check-circle /check-circle /check-circleFSDP (Zhao et al., 2023c), Elastic Sharded Check-\\npointing.\\nContinued on next page\\n32'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 32, 'page_label': '33'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTable 2: Comparison of LLM frameworks. (Continued)\\nOpenLLM /times-circle/check-circle /check-circleFSDP (Zhao et al., 2023c), Quantization\\n(GPTQ (Frantar et al., 2023), AWQ (Lin\\net al., 2023), SqueezeLLM (Kim et al.,\\n2024), SpQR (Dettmers et al., 2024),\\nLLM.int8 (Dettmers et al., 2022)), LangChain,\\nTransformers Agents, Prometheus Metrics.\\nLLM\\nFoundry\\n/times-circle/check-circle /check-circleFSDP (Zhao et al., 2023c), Continuous Batch-\\ning (Yu et al., 2022).\\nvLLM /times-circle /times-circle/check-circleData Parallelism, Tensor Parallelism (Shoeybi\\net al., 2020), PagedAttention (Kwon et al., 2023),\\nContinuous Batching (Yu et al., 2022), Quanti-\\nzation (GPTQ (Frantar et al., 2023), AWQ (Lin\\net al., 2023), SqueezeLLM (Kim et al., 2024)),\\nMulti-LoRa (Wang et al., 2023g).\\nTensorRT-\\nLLM\\n/times-circle /times-circle/check-circle3D Parallelism, PagedAttention (Kwon et al.,\\n2023), Continuous Batching (Yu et al., 2022),\\nQuantization (GPTQ (Frantar et al., 2023),\\nAWQ (Linet al.,2023), SmoothQuant(Xiao etal.,\\n2023)).\\nTGI /times-circle /times-circle/check-circleData Parallelism, Tensor Parallelism (Shoeybi\\net al., 2020), PagedAttention (Kwon et al.,\\n2023), Continuous Batching (Yu et al., 2022),\\nQuantization (BitsAndBytes Dettmers (2023),\\nGPTQ (Frantar et al., 2023)).\\nRayLLM /times-circle /times-circle/check-circleData Parallelism, Tensor Parallelism (Shoeybi\\net al., 2020), Continuous Batching (Yu et al.,\\n2022), Quantization (GPTQ (Frantar et al., 2023),\\nAWQ (Lin et al., 2023), SqueezeLLM (Kim et al.,\\n2024)), Prometheus Metrics.\\nMLC LLM /times-circle /times-circle/check-circleTVM-based Compiler Acceleration (Feng et al.,\\n2023; Chen et al., 2018), Continuous Batching (Yu\\net al., 2022), Quantized Model Support.\\nSax /times-circle /times-circle/check-circleData Parallelism, Tensor Parallelism (Shoeybi\\net al., 2020), Serves Pax, JAX, and PyTorch mod-\\nels, Slice Serving, Prometheus Metrics.\\nMosec /times-circle /times-circle/check-circleData Parallelism, Continuous Batching (Yu et al.,\\n2022), Rust-based Task Coordinator, Prometheus\\nMetrics.\\nAnother important feature of DeepSpeed-Inference is its deep fusion mechanism, which allows for the fu-\\nsion of operations without the necessity for global synchronization by tiling computations across iteration\\nspace dimensions (Ren et al., 2021; Tang et al., 2021; Li et al., 2022; Lu et al., 2023). Building on this,\\nthe DeepSpeed Model Implementations for Inference (DeepSpeed MII) module introduces Dynamic Split-\\nFuse (Holmes et al., 2024), which leverages continuous batching (Yu et al., 2022) and non-contiguous KV\\ncaches to enable increased occupancy and higher responsivity for LLM serving. It also supports scaling\\nbeyond tensor, data, and pipeline parallelism (3D Parallelism) with Zero Infinity (Rajbhandari et al., 2021)\\nand efficient post-training quantization to Int8 with ZeroQuant (Yao et al., 2022b), Int4 (W4A4) with Wu\\net al. (2023a) and ternary quantization with XTC (Wu et al., 2022b). Furthermore, the introduction of\\nDeepSpeed-Chat (Yao et al., 2023c) adds chat support to the framework. This module focuses on train-\\ning chatbot models across different scales, integrating techniques like Reinforcement Learning from Human\\nFeedback (RLHF) (Griffith et al., 2013) with the DeepSpeed training system. Notably, its integration of the\\n33'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 33, 'page_label': '34'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nZeRO-Offload optimizer (Ren et al., 2021) facilitates training on both CPUs and GPUs, irrespective of their\\nmemory capacities.\\nMegatron. Megatron (Shoeybi et al., 2020) is Nvidia’s efforts to streamline training and serving of LLMs\\nsuch as GPT (Radford et al., 2019) and T5 (Raffel et al., 2020). It is the underlying framework used for\\nNvidia Megatron models (Shoeybi et al., 2020; Narayanan et al., 2021; Korthikanti et al., 2023). Central to\\nMegatron’s design is the strategic decomposition of the model’s tensor operations, distributed across multiple\\nGPUs, to optimize both processing speed and memory utilization, thus enhancing training throughput\\nwithout compromising model quality (Shoeybi et al., 2020). In conjunction with 3D Parallelism, it also\\nimplements sequence parallelism and selective activation recomputation (Korthikanti et al., 2023), which\\nenhances training efficiency. Megatron also uses FasterTransformer (NVIDIA, 2023a) for optimizing the\\ninference process for large Transformer models and handling varying precision modes like FP16 and INT8,\\ncatering to diverse operational needs.\\nColossal-AI. Colossal-AI (Li et al., 2023c) is a framework mainly designed for large-scale distributed train-\\ning (Wang et al., 2023a). Colossal-AI unifies a wide range of parallelism techniques (Xu & You, 2023; Wang\\net al., 2023a; Bian et al., 2021) including sequence parallelism (Li et al., 2023d), auto-parallelism (Liu et al.,\\n2023c), and Zero Redundancy Optimizer (Zhao et al., 2022). It also implements heterogeneous memory\\nmanagement (Fang et al., 2023) through a streamlined API. This integrated approach mitigates the steep\\nlearning curve often associated with orchestrating large-scale training in distributed environments. In ad-\\ndition, the framework integrates several other features like quantization (Frantar et al., 2023; Xiao et al.,\\n2023), RLHF (Griffith et al., 2013), OpenMoE, and mixed-precision training.\\nNanotron. Nanotron (HuggingFace, 2023a), introduced by Huggingface, is anLLM trainingframeworkwith\\na primary focus on providing functionality with minimal overhead. As such, the library only subjectively\\nincorporates the best optimizations required for modern LLM training requirements. Some highlights are\\nits implementation of tensor, data, and pipeline parallelism with one-forward-one-backward pipeline engine,\\nZeRO-1 optimizer (Zhao et al., 2022), FP32 gradient accumulation, parameter tying/sharding and spectral\\nµTransfer parametrization (Yang et al., 2022) for scaling up neural networks. Nanotron also incorporates\\nDoReMi (Xie et al., 2023a) to further speed up training.\\nMegaBlocks. Developed by Databricks, MegaBlocks (Gale et al., 2023) is an LLM training framework for\\ntraining Mixture-of-Experts (MoE) models. The core of MegaBlocks is dropless-MoE, a reformulation of\\nMoE in terms of block-sparse operations, that avoids token dropping without sacrificing hardware efficiency.\\nThis design simplifies and accelerates training as it does not require the capacity factor as a hyper-parameter.\\nFairScale. Developed by Meta, FairScale (FairScale authors, 2021) is an extension library to PyTorch,\\ndedicated to high-performance and large-scale training. As a highlight, FairScale uses Fully Sharded Data\\nParallel (FSDP) (Zhao et al., 2023c) as the preferred method for scaling the training operations of large\\nneural networks. It uses AdaScale optimizer (Johnson et al., 2020) as its distributed optimizer.\\nPax. Developed by Google, Pax (Google, 2023a) is a JAX-based distributed training framework. Pax has\\nbeen used to train PaLM-2 (Anil et al., 2023) and Bard (Hsiao et al., 2023). It targets scalability and\\nhas reference examples for large model training, including across modalities (e.g., text, vision, speech). It\\nsupports data and tensor parallelism, and is heavily integrated with JAX and uses many libraries in the JAX\\necosystem. Several key components Pax contains include SeqIO to handle sequential data processing, Optax\\nfor optimization, Fiddle for configuration, Orbax for checkpointing, PyGLove for automatic differentiation,\\nand Flax for creating high-performance neural networks.\\nComposer. Composer (MosaicML, 2023a) is an LLM framework designed by Mosaic ML. It has been used\\nto train Mosaic ML’s MPT 7B and MPT 30B models and Replit’s Code V-1.5 3B. The framework is built\\non top of PyTorch and provides a collection of acceleration methods that users can incorporate into their\\ntraining loops or use with the Composer trainer. Composer supports FSDP for efficient parallelism, elastic\\nshared checkpointing for robust intermittent training, and a dataset streaming implementation allowing the\\ndownload of datasets from cloud blob storage on the fly during training. Composer also provides a functional\\nAPI for integrating methods directly into its training loops, as well as a Trainer API which automatically\\nimplements a PyTorch-based training loop, reducing the workload for ML developers.\\n34'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 34, 'page_label': '35'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nOpenLLM. OpenLLM (Pham et al., 2023) was made by BentoML to serve and fine-tune LLMs within\\nproduction environments. Anchored within the BentoML ecosystem, OpenLLM makes it easy to self-host\\nLLMs and integrate them with other cloud services. OpenLLM emphasizes on flexibility, SOTA LLM\\nsupport, and streamlined APIs for self-hosting. Recognizing the diverse needs of production environments,\\nOpenLLM supports the automatic generation of docker images. It also supports serving models as serverless\\nendpoints using BentoML’s cloud platform. OpenLLM further integrates advanced quantization techniques\\n(GPTQ (Frantar et al., 2023), AWQ (Lin et al., 2023), SqueezeLLM (Kim et al., 2024), SpQR (Dettmers\\net al., 2024), LLM.int8 (Dettmers et al., 2022)) for efficient inference. OpenLLM’s design also incorporates\\nrobust monitoring with Prometheus metrics and logging tools, ensuring that operational insights are readily\\navailable for performance tuning and troubleshooting.\\nLLM Foundry. LLM Foundry (MosaicML, 2023b) is a library developed by MosaicML for fine-tuning,\\nevaluating, and serving LLMs. It supports distributed inference via FSDP and continuous batching (Yu\\net al., 2022) for efficient serving. It also has cloud integration with the MosaicML platform.\\nvLLM. vLLM (Kwon et al., 2023) is an open-source library for LLM inference and serving. It adopts a\\ndifferent design in how KV cache is stored in memory. Central to this design is PagedAttention, a mechanism\\nthat segments the attention key and value (KV) cache for a set number of tokens. Unlike contiguous\\nspace storage, PagedAttention’s blocks for the KV cache are stored flexibly, similar to the virtual memory\\nmanagement in operating systems. This facilitates memory sharing at a block level across various sequences\\ntiedtothesamerequestorevendifferentrequests, thusenhancingmemorymanagementefficiencyinhandling\\nattention mechanisms. Hence, vLLM can reduce the total memory usage when using complex decoding\\ntechniques such as parallel sampling and beam search as the memory blocks can be shared across different\\ncandidate samples. It also allows on-demand buffer allocation, while also eliminating external fragmentation\\nas the blocks are uniformly sized. Furthermore, vLLM incorporates safeguards that prevent GPU memory\\noverflow due to the increasing size of KV cache by evicting and recovering blocks as needed via swapping\\nand recomputation. vLLM also supports Multi-LoRA (Wang et al., 2023g), continuous batching (Yu et al.,\\n2022) and quantization (GPTQ (Frantar et al., 2023), AWQ (Lin et al., 2023) and SqueezeLLM (Kim et al.,\\n2024)). Lastly, it implements efficient kernels for both Nvidia and AMD GPUs.\\nTensorRT-LLM.TensorRT-LLM (NVIDIA, 2023b) is a streamlined library to serve LLMs. Built on top of\\nthe TensorRT engine, TensorRT-LLM integrates optimized kernels from FasterTransformer (Timonin et al.,\\n2022) and employs tensor parallelism, facilitating efficient inference at scale across multiple GPUs and servers\\nwithout necessitating developer intervention or model changes. It also integrates seamlessly with the Nvidia\\nTriton Inference Server for serving LLMs. Additionally, it offers features like tensor parallelism, continuous\\nbatching (Yu et al., 2022), and PagedAttention (Kwon et al., 2023) as well as supports a wide range of\\nquantization modes and techniques.\\nText-Generation-Inference (TGI).Text-Generation-Inference (TGI) (HuggingFace, 2023b) is a high-\\nperformance LLM serving library developed by Huggingface and is used to power Hugging Chat. TGI\\nsupports a large variety of LLMs, and offers a wide range of features like tensor parallelism, continuous\\nbatching (Yu et al., 2022), efficient attention mechanisms like FlashAttention (Dao et al., 2022) and Page-\\ndAttention (Kwon et al., 2023), and quantization (BitsAndBytes (Dettmers, 2023), GPTQ (Frantar et al.,\\n2023)) support.\\nRayLLM.RayLLM (Ray Project, 2023) is an LLM serving framework as part of the Ray ecosystem (Moritz\\net al., 2018). Built with Ray Serve and the Ray library developed by AnyScale, RayLLM eases LLM\\nserving in multi-GPU and multi-node systems. At the core of RayLLM is the leveraging of Ray’s inherent\\ndistributed computing capabilities. RayLLM integrates Ray’s distributed task scheduling and execution\\nmechanisms, ensuring that LLM tasks are efficiently distributed across available resources. RayLLM also\\nsimplifies adding new LLMs for custom use cases, and offers auto-scaling support and high-performance\\nfeatures like continuous batching (Yu et al., 2022), quantization (GPTQ (Frantar et al., 2023), AWQ (Lin\\net al., 2023), SqueezeLLM (Kim et al., 2024)), and monitoring via Prometheus metrics. It also comes with\\nadvanced monitoring support as well which includes a CLI and a web frontend that can be used to compare\\nand rank models and get cost and latency estimates.\\n35'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 35, 'page_label': '36'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nMLC LLM.MLC LLM (Machine Learning Compilation for Large Language Models) (MLC-LLM, 2023)\\nallows individuals to develop, optimize, and deploy LLMs on a wide range of platforms such as mobile phones\\nand web browsers. Central to MLC LLM is its focus on machine learning compilation techniques. MLC-\\nLLM compiles LLMs and deploys them in a process that is inherently tailored to the specific capabilities\\nand constraints of each platform and hardware (Chen et al., 2018; Shao et al., 2022; Feng et al., 2023).\\nThis platform-native approach ensures that LLMs are not only efficient but also highly optimized for the\\nplatforms in which they operate.\\nSax. Sax (Google, 2023b) is a platform designed by Google for serving Pax, JAX, and PyTorch models for\\ninference tasks. It supports distributed inference with tensor and data parallelism. It also integrates easily\\nwith Google Cloud and cloud monitoring with Prometheus metrics.\\nMosec. Mosec (Yang et al., 2021) is a serving framework built to streamline the serving of machine learning\\nmodels into backend services and microservices. Mosec’s key features include continuous batching (Yu\\net al., 2022), pipelined stages for handling mixed workloads, and essential cloud features like model warmup,\\ngraceful shutdown, and Prometheus monitoring metrics, making it easily manageable by Kubernetes or other\\ncontainer systems.\\n5 Concluding Remarks\\nIn this survey, we provide a systematic review of efficient LLMs, an important area of research aimed at\\ndemocratizing LLMs. We start with motivating the necessity for efficient LLMs. Guided by a taxonomy, we\\nreview algorithm-level and system-level efficient techniques for LLMs from model-centric and data-centric\\nperspectives respectively. Furthermore, we review LLM frameworks with specific optimizations and features\\ncrucial for efficient LLMs. We believe that efficiency will play an increasingly important role in LLMs and\\nLLMs-oriented systems. We hope this survey could enable researchers and practitioners to quickly get started\\nin this field and act as a catalyst to inspire new research on efficient LLMs.\\n6 Acknowledgement\\nWe would like to thank the action editor Greg Durrett and anonymous reviewers of Transactions on Machine\\nLearning Research for their helpful and constructive comments.\\nReferences\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Bal-\\naji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine,\\nGabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-\\nLuisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Camp-\\nbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis\\nChantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey\\nChu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien\\nEcoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston\\nForte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh,\\nRapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,\\nShixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-\\nhannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny\\nHsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger\\nJiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,\\nAli Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook\\nKim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz\\nKondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael\\n36'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 36, 'page_label': '37'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nLampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini,\\nSam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob Mc-\\nGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok\\nMehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa,\\nDaniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Ra-\\njeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub\\nPachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy\\nParparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,\\nMichael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong,\\nTolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya\\nRamesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,\\nNick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,\\nPranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Ben-\\njamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever,\\nJie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-\\nston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\\nChelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\\nCJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\\nClemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu,\\nKai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,\\nMarvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4\\ntechnical report, 2023,arXiv preprint arXiv:2303.08774.URL http://arxiv.org/abs/2303.08774.\\nRishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and\\nOlivier Bachem. Generalized knowledge distillation for auto-regressive language models. InThe Twelfth\\nInternational Conference on Learning Representations, 2024. URL https://openreview.net/forum?i\\nd=3zKtaqxLhW.\\nArash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen Gou, Phil Blunsom, Ahmet\\nÜstün, and Sara Hooker. Intriguing properties of quantization at scale. InThirty-seventh Conference on\\nNeural Information Processing Systems, 2023. URLhttps://openreview.net/forum?id=IYe8j7Gy8f.\\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai.\\nGQA: Training generalized multi-query transformer models from multi-head checkpoints. InProceedings\\nof the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore, 2023. URL\\nhttps://aclanthology.org/2023.emnlp-main.298.\\nSilas Alberti, Niclas Dern, Laura Thesing, and Gitta Kutyniok. Sumformer: Universal approximation for\\nefficient transformers. In Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry in\\nMachine Learning (TAG-ML), volume 221, 2023. URLhttps://proceedings.mlr.press/v221/alber\\nti23a.html.\\nReza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng,\\nOlatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. Deepspeed-inference: En-\\nabling efficient inference of transformer models at unprecedented scale. InProceedings of the International\\nConference on High Performance Computing, Networking, Storage and Analysis, Dallas, Texas, 2022. URL\\nhttps://dl.acm.org/doi/abs/10.5555/3571885.3571946.\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey,\\nYanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson,\\nSebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Jun-\\nwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks,\\nMichele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clé-\\nment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer,\\n37'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 37, 'page_label': '38'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nVlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lu-\\ncas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey\\nHui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,\\nMaxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li,\\nYaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,\\nAroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric\\nNi, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan\\nQiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar\\nSamuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha\\nValter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John\\nWieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report,\\n2023, arXiv preprint arXiv:2305.10403.URL http://arxiv.org/abs/2305.10403.\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman, Xian Li, Shuohui Chen,\\nHalil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo, Jeffrey Wang,\\nLuke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Veselin Stoyanov. Efficient large scale language\\nmodeling with mixtures of experts. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.),Pro-\\nceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11699–11732,\\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi:\\n10.18653/v1/2022.emnlp-main.804. URL https://aclanthology.org/2022.emnlp-main.804.\\nThomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley.\\nRezero is all you need: fast convergence at large depth. InProceedings of the Thirty-Seventh Conference\\non Uncertainty in Artificial Intelligence, volume 161, 2021. URLhttps://proceedings.mlr.press/v1\\n61/bachlechner21a.html.\\nTrapit Bansal, Salaheddin Alzubi, Tong Wang, Jay-Yoon Lee, and Andrew McCallum. Meta-adapters:\\nParameter efficient few-shot fine-tuning through meta-learning. InInternational Conference on Automated\\nMachine Learning, Baltimore, US, 2022. URLhttps://openreview.net/forum?id=BCGNf-prLg5.\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020,\\narXiv preprint arXiv:2004.05150.URL http://arxiv.org/abs/2004.05150.\\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range trans-\\nformers with unlimited length input. In Thirty-seventh Conference on Neural Information Processing\\nSystems, 2023. URLhttps://openreview.net/forum?id=lJWUJWLCJo.\\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna\\nGajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts:\\nSolving elaborate problems with large language models.Proceedings of the AAAI Conference on Artificial\\nIntelligence, 38, 2024. URLhttps://ojs.aaai.org/index.php/AAAI/article/view/29720.\\nZhengda Bian, Qifan Xu, Boxiang Wang, and Yang You. Maximizing parallelism in distributed training for\\nhuge neural networks, 2021,arXiv preprint arXiv:2105.14450.URL http://arxiv.org/abs/2105.14450.\\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\\nConnor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit,\\nLaria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source\\nautoregressive language model. In Proceedings of BigScience Episode #5 – Workshop on Challenges &\\nPerspectives in Creating Large Language Models, virtual+Dublin, 2022. URLhttps://aclanthology.o\\nrg/2022.bigscience-1.9.\\nbloc97. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-\\ntuning and minimal perplexity degradation.https://www.reddit.com/r/LocalLLaMA/comments/14lz7\\nj5/ntkaware_scaled_rope_allows_llama_models_to_have/, 2023.\\n38'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 38, 'page_label': '39'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges\\nof efficient transformer quantization. In Proceedings of the 2021 Conference on Empirical Methods in\\nNatural Language Processing, Online and Punta Cana, Dominican Republic, 2021. URLhttps://aclant\\nhology.org/2021.emnlp-main.627.\\nTomBrown, BenjaminMann, NickRyder, MelanieSubbiah, JaredDKaplan, PrafullaDhariwal, ArvindNee-\\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-\\npher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\\nare few-shot learners. In Advances in Neural Information Processing Systems, volume 33, 2020. URL\\nhttps://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac14\\n2f64a-Paper.pdf.\\nAydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. Recurrent memory transformer. InAdvances in Neural\\nInformation Processing Systems, 2022. URLhttps://openreview.net/forum?id=Uynr3iPhksa.\\nNeil Burgess, Jelena Milanovic, Nigel Stephens, Konstantinos Monachopoulos, and David Mansell. Bfloat16\\nprocessing for neural networks. InIEEE Symposium on Computer Arithmetic, Kyoto, 2019. URLhttps:\\n//ieeexplore.ieee.org/document/8877390.\\nFederico Busato and Jeff Pool. Exploiting nvidia ampere structured sparsity with cusparselt. https:\\n//developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt , 2020.\\nLucas Caccia, Edoardo Ponti, Zhan Su, Matheus Pereira, Nicolas Le Roux, and Alessandro Sordoni. Multi-\\nhead adapter routing for cross-task generalization. InThirty-seventh Conference on Neural Information\\nProcessing Systems, 2023. URLhttps://openreview.net/forum?id=qcQhBli5Ho.\\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao.\\nMedusa: Simple llm inference acceleration framework with multiple decoding heads, 2024,arXiv preprint\\narXiv:2401.10774. URL http://arxiv.org/abs/2401.10774.\\nYihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. Instruction mining: When data mining meets large\\nlanguage model finetuning, 2023,arXiv preprint arXiv:2307.06290.URL http://arxiv.org/abs/2307\\n.06290.\\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\\nCunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing\\nXie. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and\\nTechnology, 15, 2024. URLhttps://doi.org/10.1145/3641289.\\nJerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit quantization of large\\nlanguagemodelswithguarantees. In Thirty-seventh Conference on Neural Information Processing Systems,\\n2023. URL https://openreview.net/forum?id=xrk9g5vcXR.\\nBeidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying\\nsparse and low-rank attention. Advances in Neural Information Processing Systems, 34, 2021a. URL\\nhttps://openreview.net/forum?id=SehIKudiIo1.\\nChang Chen, Min Li, Zhihua Wu, Dianhai Yu, and Chao Yang. TA-moe: Topology-aware large scale\\nmixture-of-expert training. InAdvances in Neural Information Processing Systems, 2022a. URLhttps:\\n//openreview.net/forum?id=FRDiimH26Tr.\\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John\\nJumper. Accelerating large language model decoding with speculative sampling, 2023a,arXiv preprint\\narXiv:2302.01318. URL http://arxiv.org/abs/2302.01318.\\n39'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 39, 'page_label': '40'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nCheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan\\nLiu, and Qun Liu. bert2BERT: Towards reusable pretrained language models. In Proceedings of the\\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin,\\nIreland, 2022b. URLhttps://aclanthology.org/2022.acl-long.151.\\nGuanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. CLEX: Continuous length\\nextrapolation for large language models. InThe Twelfth International Conference on Learning Represen-\\ntations, 2024a. URLhttps://openreview.net/forum?id=wXpSidPpc5.\\nHao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo\\nZhao. Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning,\\n2023b, arXiv preprint arXiv:2305.09246.URL http://arxiv.org/abs/2305.09246.\\nHoward Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory\\nmaze: Beyond context limit through interactive reading, 2023c,arXiv preprint arXiv:2310.05029.URL\\nhttp://arxiv.org/abs/2310.05029.\\nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vi-\\njay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training a better alpaca\\nwith fewer data. In The Twelfth International Conference on Learning Representations, 2024b. URL\\nhttps://openreview.net/forum?id=FdVXgSJhvz.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,\\nMichael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,\\nMikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Fe-\\nlipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-\\nVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir\\nBalaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam,\\nVedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie\\nMayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\\nZaremba. Evaluating large language models trained on code, 2021b,arXiv preprint arXiv:2107.03374.\\nURL http://arxiv.org/abs/2107.03374.\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large\\nlanguage models via positional interpolation, 2023d,arXiv preprint arXiv:2306.15595.URL http://arxi\\nv.org/abs/2306.15595.\\nTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan,\\nLeyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. TVM: An automated\\nEnd-to-End optimizing compiler for deep learning. InUSENIX Symposium on Operating Systems Design\\nand Implementation (OSDI), Carlsbad, CA, 2018. URLhttps://www.usenix.org/conference/osdi18\\n/presentation/chen.\\nTianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, and Luming Liang. Lorashear: Efficient large\\nlanguage model structured pruning and knowledge recovery, 2023e, arXiv preprint arXiv:2310.18356.\\nURL http://arxiv.org/abs/2310.18356.\\nWuyang Chen, Yanqi Zhou, Nan Du, Yanping Huang, James Laudon, Zhifeng Chen, and Claire Cui. Life-\\nlong language pretraining with distribution-specialized experts. InProceedings of the 40th International\\nConference on Machine Learning, Honolulu, Hawaii, USA, 2023f. URLhttps://dl.acm.org/doi/10.55\\n55/3618408.3618621.\\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang\\nLuong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V Le. Symbolic discovery of optimization algorithms. In\\nThirty-seventh Conference on Neural Information Processing Systems, 2023g. URLhttps://openreview\\n.net/forum?id=ne6zeqLFCZ.\\n40'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 40, 'page_label': '41'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-\\ncontext tuning. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), Dublin, Ireland, 2022c. URLhttps://aclanthology.org/2022.acl-long.53.\\nYongrui Chen, Haiyun Jiang, Xinting Huang, Shuming Shi, and Guilin Qi. Tegit: Generating high-quality\\ninstruction-tuning data with text-grounded task design, 2023h,arXiv preprint arXiv:2309.05447. URL\\nhttp://arxiv.org/abs/2309.05447.\\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora:\\nEfficient fine-tuning of long-context large language models, 2023i,arXiv preprint arXiv:2309.12307.URL\\nhttp://arxiv.org/abs/2309.12307.\\nZeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, and Kyle Richardson. DISCO: Distilling\\ncounterfactuals with large language models. InProceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023j. URLhttps://aclant\\nhology.org/2023.acl-long.302.\\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress\\ncontexts. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\\nSingapore, 2023. URLhttps://aclanthology.org/2023.emnlp-main.232.\\nZewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj,\\nXia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation collapse of sparse mixture\\nof experts. In Advances in Neural Information Processing Systems, 2022. URL https://openreview.n\\net/forum?id=mWaYC6CZf5.\\nYew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. Contrastive chain-of-\\nthought prompting, 2023,arXiv preprint arXiv:2311.09277.URL http://arxiv.org/abs/2311.09277.\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse trans-\\nformers, 2019,arXiv preprint arXiv:1904.10509.URL http://arxiv.org/abs/1904.10509.\\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,\\nPeter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling\\nfor proteins via linearly scalable long-context transformers, 2020,arXiv preprint arXiv:2006.03555.URL\\nhttp://arxiv.org/abs/2006.03555.\\nKrzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger,\\nLucy J Colwell, and Adrian Weller. Rethinking attention with performers, 2021. URLhttps://openre\\nview.net/forum?id=Ua6zuk0WRH.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar\\nPrabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\\nIsard, GuyGur-Ari, PengchengYin, TojuDuke, AnselmLevskaya, SanjayGhemawat, SunipaDev, Henryk\\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022,\\narXiv preprint arXiv:2204.02311.URL http://arxiv.org/abs/2204.02311.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha\\n41'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 41, 'page_label': '42'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nValter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun\\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and\\nJason Wei. Scaling instruction-finetuned language models, 2022,arXiv preprint arXiv:2210.11416.URL\\nhttp://arxiv.org/abs/2210.11416.\\nJae-Won Chung, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and Mosharaf Chowdhury. Perseus:\\nRemoving energy bloat from large model training, 2023,arXiv preprint arXiv:2312.06902.URL http:\\n//arxiv.org/abs/2312.06902.\\nDamai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, and Furu Wei. StableMoE: Stable\\nrouting strategy for mixture of experts. InProceedings of the 60th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), Dublin, Ireland, 2022. URLhttps://aclantholo\\ngy.org/2022.acl-long.489.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-\\nXL: Attentive language models beyond a fixed-length context. InProceedings of the 57th Annual Meeting\\nof the Association for Computational Linguistics, Florence, Italy, 2019. URLhttps://aclanthology.o\\nrg/P19-1285.\\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. InThe Twelfth\\nInternational Conference on Learning Representations, 2024. URL https://openreview.net/forum?i\\nd=mZn2Xyh9Ec.\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\\nefficient exact attention with io-awareness.Advances in Neural Information Processing Systems, 35, 2022.\\nURL https://openreview.net/forum?id=H4DqfPSibmx.\\nTri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference.\\nhttps://pytorch.org/blog/flash-decoding/, 2023.\\nSoham De and Samuel L. Smith. Batch normalization biases residual blocks towards the identity function\\nin deep networks. InProceedings of the 34th International Conference on Neural Information Processing\\nSystems, 2020. URLhttps://dl.acm.org/doi/abs/10.5555/3495724.3497400.\\nTim Dettmers. Bitsandbytes.https://github.com/TimDettmers/bitsandbytes, 2023.\\nTimDettmers, MikeLewis, YounesBelkada, andLukeZettlemoyer. GPT3.int8(): 8-bitmatrixmultiplication\\nfor transformers at scale. In Advances in Neural Information Processing Systems, 2022. URL https:\\n//openreview.net/forum?id=dXiGWqBoxaD.\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of\\nquantized LLMs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\\nhttps://openreview.net/forum?id=OUIFPHEgJU.\\nTim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos,\\nAlexander Borzunov, Torsten Hoefler, and Dan Alistarh. SpQR: A sparse-quantized representation for\\nnear-lossless LLM weight compression. InThe Twelfth International Conference on Learning Representa-\\ntions, 2024. URLhttps://openreview.net/forum?id=Q1u25ahSuy.\\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and\\nFuru Wei. Longnet: Scaling transformers to 1,000,000,000 tokens, 2023a,arXiv preprint arXiv:2307.02486.\\nURL http://arxiv.org/abs/2307.02486.\\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\\ngeneration, 2023b,arXiv preprint arXiv:2311.04254.URL http://arxiv.org/abs/2311.04254.\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li,\\nand Zhifang Sui. A survey on in-context learning, 2023,arXiv preprint arXiv:2301.00234.URL http:\\n//arxiv.org/abs/2301.00234.\\n42'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 42, 'page_label': '43'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\\nYanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou,\\nTao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju\\nDuke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. GLaM: Efficient\\nscaling of language models with mixture-of-experts. InProceedings of the 39th International Conference\\non Machine Learning, 2022. URLhttps://proceedings.mlr.press/v162/du22c.html.\\nFeyza Duman Keles, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. On the computational com-\\nplexity of self-attention. In Proceedings of The 34th International Conference on Algorithmic Learning\\nTheory, 2023. URLhttps://proceedings.mlr.press/v201/duman-keles23a.html.\\nLance Eliot. Generative pre-trained transformers (gpt-3) pertain to ai in the law, 2021. URL http:\\n//dx.doi.org/10.2139/ssrn.3974887.\\nFacebook AI Research (FAIR). fairseq: Fp16 optimizer - line 468.https://github.com/facebookresearc\\nh/fairseq/blob/main/fairseq/optim/fp16_optimizer.py, 2023.\\nFairScale authors. Fairscale: A general purpose modular pytorch library for high performance and large\\nscale training. https://github.com/facebookresearch/fairscale, 2021.\\nJiaruiFang, ZilinZhu, ShengguiLi, HuiSu, YangYu, JieZhou, andYangYou. Paralleltrainingofpre-trained\\nmodels via chunk-based dynamic memory management.IEEE Transactions on Parallel and Distributed\\nSystems, 34(1):304–315, 2023. doi: 10.1109/TPDS.2022.3219819. URLhttps://ieeexplore.ieee.org/\\ndocument/9940581.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: scaling to trillion parameter models\\nwith simple and efficient sparsity.Journal of Machine Learning Research, 23, 2022. URLhttps://dl.a\\ncm.org/doi/abs/10.5555/3586589.3586709.\\nSiyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng,\\nCody Hao Yu, Yong Yu, and Tianqi Chen. Tensorir: An abstraction for automatic tensorized program\\noptimization. InACM International Conference on Architectural Support for Programming Languages and\\nOperating Systems, 2023. URLhttps://doi.org/10.1145/3575693.3576933.\\nElias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quan-\\ntization and pruning. In Advances in Neural Information Processing Systems, New Orleans, Louisiana,\\n2022. URL https://openreview.net/forum?id=ksVGCOlOEba.\\nElias Frantar and Dan Alistarh. SparseGPT: Massive language models can be accurately pruned in one-\\nshot. In Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/frantar23a.html.\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for gen-\\nerative pre-trained transformers. InThe Eleventh International Conference on Learning Representations,\\n2023. URL https://openreview.net/forum?id=tcbBPnfwxS.\\nDaniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry\\nhungry hippos: Towards language modeling with state space models. In The Eleventh International\\nConference on Learning Representations, 2023a. URLhttps://openreview.net/forum?id=COZDy0WYGg.\\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models\\ntowards multi-step reasoning. InProceedings of the 40th International Conference on Machine Learning,\\nHonolulu, Hawaii, 2023b. URLhttps://proceedings.mlr.press/v202/fu23d.html.\\nYichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking the sequential dependency of llm inference\\nusing lookahead decoding, 2023c. URLhttps://lmsys.org/blog/2023-11-21-lookahead-decoding/ .\\n43'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 43, 'page_label': '44'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTrevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with\\nmixture-of-experts. Proceedings of Machine Learning and Systems, 5, 2023. URLhttps://proceeding\\ns.mlsys.org/paper_files/paper/2023/hash/5a54f79333768effe7e8927bcccffe40-Abstract-mlsys\\n2023.html.\\nTao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context\\ncompression in a large language model. InThe Twelfth International Conference on Learning Represen-\\ntations, 2024. URLhttps://openreview.net/forum?id=uREj4ZuGJE.\\nMichael Glass, Alfio Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, G P Shrivatsa Bhargav, Dinesh\\nGarg, and Avi Sil. Span selection pre-training for question answering. InProceedings of the 58th Annual\\nMeeting of the Association for Computational Linguistics, 2020. URLhttps://aclanthology.org/202\\n0.acl-main.247.\\nLinyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of BERT\\nby progressively stacking. In Proceedings of the 36th International Conference on Machine Learning,\\nvolume 97, Long Beach, California, 2019. URLhttps://proceedings.mlr.press/v97/gong19a.html.\\nGoogle. Pax: A jax-based machine learning framework for large scale models.https://github.com/googl\\ne/paxml, 2023a. URLhttps://github.com/google/paxml. GitHub repository.\\nGoogle. Sax. https://github.com/google/saxml, 2023b.\\nJianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey.\\nInternational Journal of Computer Vision, 129, 2021. URLhttps://doi.org/10.1007/s11263-021-0\\n1453-z.\\nShane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy\\nshaping: Integrating human feedback with reinforcement learning. In Advances in Neural Information\\nProcessing Systems, volume 26, 2013. URLhttps://proceedings.neurips.cc/paper%5Ffiles/paper\\n/2013/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf.\\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023,arXiv\\npreprint arXiv:2312.00752.URL http://arxiv.org/abs/2312.00752.\\nAlbert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state\\nspaces. In International Conference on Learning Representations, 2022a. URL https://openreview.n\\net/forum?id=uYLFoz1vlAC.\\nXiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han. On the transformer growth\\nfor progressive BERT training. In Proceedings of the 2021 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies, Online, 2021. URL\\nhttps://aclanthology.org/2021.naacl-main.406.\\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. PPT: Pre-trained prompt tuning for few-shot learning.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), Dublin, Ireland, 2022b. URLhttps://aclanthology.org/2022.acl-long.576.\\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation of large language\\nmodels. In The Twelfth International Conference on Learning Representations, 2024. URL https:\\n//openreview.net/forum?id=5h0qf7IBZZ.\\nCong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo,\\nand Yuhao Zhu. Olive: Accelerating large language models via hardware-friendly outlier-victim pair\\nquantization. In Proceedings of the 50th Annual International Symposium on Computer Architecture,\\n2023. URL https://doi.org/10.1145/3579371.3589038.\\nAhan Gupta, Yueming Yuan, Yanqi Zhou, and Charith Mendis. Flurka: Fast fused low-rank & kernel\\nattention, 2023,arXiv preprint arXiv:2306.15799.URL http://arxiv.org/abs/2306.15799.\\n44'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 44, 'page_label': '45'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nAnkit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state\\nspaces. In Advances in Neural Information Processing Systems, 2022. URLhttps://openreview.net/f\\norum?id=RjS0j6tsSrf.\\nInsu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David Woodruff, and Amir Zandieh. Hyperat-\\ntention: Long-context attention in near-linear time. InThe Twelfth International Conference on Learning\\nRepresentations, 2024. URLhttps://openreview.net/forum?id=Eh0Od2BJIM.\\nJiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. Fastmoe: A fast mixture-of-\\nexpert training system, 2021,arXiv preprint arXiv:2103.13262.URL http://arxiv.org/abs/2103.132\\n62.\\nJiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, and Qin Li. Fastermoe:\\nmodeling and optimizing training of large-scale dynamic pre-trained models. InProceedings of the 27th\\nACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 2022a. URL https:\\n//dl.acm.org/doi/10.1145/3503221.3508418.\\nKai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. A Survey of\\nLarge Language Models for Healthcare: from Data, Technology, and Applications to Accountability and\\nEthics, 2023,arXiv preprint arXiv:2310.05694.URL http://arxiv.org/abs/2310.05694.\\nShwai He, Liang Ding, Daize Dong, Miao Zhang, and Dacheng Tao. Sparseadapter: An easy approach for\\nimproving the parameter-efficiency of adapters. InFindings of EMNLP, 2022b. URLhttps://aclantho\\nlogy.org/2022.findings-emnlp.160.\\nNamgyuHo, LauraSchmid, andSe-YoungYun. Large languagemodelsarereasoningteachers. In Proceedings\\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\nToronto, Canada, 2023. URLhttps://aclanthology.org/2023.acl-long.830.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\\nKatherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen\\nSimonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of\\ncompute-optimal large language model training. InAdvances in Neural Information Processing Systems,\\n2022. URL https://openreview.net/forum?id=iBBcRUlOAPR.\\nConnor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam Rajbhandari,\\nReza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, and Yuxiong He. Deepspeed-\\nfastgen: High-throughput text generation for llms via mii and deepspeed-inference, 2024,arXiv preprint\\narXiv:2401.08671. URL http://arxiv.org/abs/2401.08671.\\nKe Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and\\nYu Wang. Flashdecoding++: Faster large language model inference on gpus, 2023, arXiv preprint\\narXiv:2311.01282. URL http://arxiv.org/abs/2311.01282.\\nOr Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction induction: From few examples\\nto natural language task descriptions. InProceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023. URLhttps://aclantholo\\ngy.org/2023.acl-long.108.\\nColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt\\nKeutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache\\nquantization, 2024,arXiv preprint arXiv:2401.18079.URL http://arxiv.org/abs/2401.18079.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-\\nmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Pro-\\nceedings of the 36th International Conference on Machine Learning, volume 97, 2019. URL https:\\n//proceedings.mlr.press/v97/houlsby19a.html.\\n45'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 45, 'page_label': '46'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nSissie Hsiao, Yury Pinsky, and Sundar Pichai. Bard: Google’s generative language model.https://blog.g\\noogle/products/search/bard-updates/, 2023.\\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Kr-\\nishna, Chen-YuLee, andTomasPfister. Distillingstep-by-step! outperforminglargerlanguagemodelswith\\nless training data and smaller model sizes. InFindings of the Association for Computational Linguistics:\\nACL 2023, Toronto, Canada, 2023. URLhttps://aclanthology.org/2023.findings-acl.507.\\nYen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language model com-\\npression with weighted low-rank factorization. InInternational Conference on Learning Representations,\\n2022. URL https://openreview.net/forum?id=uPv9Y3gmAI5.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on\\nLearning Representations, 2022. URLhttps://openreview.net/forum?id=nZeVKeeFYf9.\\nShengdingHu, NingDing, WeilinZhao, XingtaiLv, ZhenZhang, ZhiyuanLiu, andMaosongSun. OpenDelta:\\nA plug-and-play library for parameter-efficient adaptation of pre-trained models. InProceedings of the 61st\\nAnnual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations),\\nToronto, Canada, 2023a. URLhttps://aclanthology.org/2023.acl-demo.26.\\nZhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and\\nRoy Lee. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large language models.\\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore,\\n2023b. URL https://aclanthology.org/2023.emnlp-main.319.\\nChengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient cross-\\ntask generalization via dynamic lora composition, 2023,arXiv preprint arXiv:2307.13269. URL http:\\n//arxiv.org/abs/2307.13269.\\nJie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. In\\nFindings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, 2023. URL\\nhttps://aclanthology.org/2023.findings-acl.67.\\nXiaoShiHuang, FelipePerez, JimmyBa, andMaksimsVolkovs. Improvingtransformeroptimizationthrough\\nbetter initialization. In Proceedings of the 37th International Conference on Machine Learning, volume\\n119, 2020. URLhttps://proceedings.mlr.press/v119/huang20f.html.\\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong\\nLee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant\\nneural networks using pipeline parallelism. InProceedings of the 33rd International Conference on Neural\\nInformation Processing Systems, Red Hook, NY, USA, 2019.\\nYukun Huang, Yanda Chen, Zhou Yu, and Kathleen McKeown. In-context learning distillation: Transferring\\nfew-shot learning ability of pre-trained language models, 2022,arXiv preprint arXiv:2212.10670. URL\\nhttp://arxiv.org/abs/2212.10670.\\nHuggingFace. text-generation-inference. https://github.com/huggingface/nanotron , 2023a. Accessed:\\n2024-05-10.\\nHuggingFace. text-generation-inference. https://github.com/huggingface/text-generation-inferen\\nce, 2023b. Accessed: 2024-05-10.\\nDeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent\\ntransformers. In Advances in Neural Information Processing Systems, 2022. URLhttps://openreview\\n.net/forum?id=uloenYmLCAo.\\nChangho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin\\nJose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, and Yongqiang Xiong. Tutel: Adaptive\\nmixture-of-experts at scale.Proceedings of Machine Learning and Systems, 5, 2023.\\n46'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 46, 'page_label': '47'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nRégis Pierrard Ilyas Moutawwakil. Llm-perf leaderboard.https://huggingface.co/spaces/optimum/ll\\nm-perf-leaderboard, 2023.\\nMaor Ivgi, Uri Shaham, and Jonathan Berant. Efficient long-text understanding with short-text models.\\nTransactions of the Association for Computational Linguistics, 11, 2023. URLhttps://aclanthology.o\\nrg/2023.tacl-1.17.\\nHamish Ivison, Noah A. Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. Data-efficient finetuning using\\ncross-task nearest neighbors. In Findings of the Association for Computational Linguistics, Toronto,\\nCanada, 2023. URLhttps://aclanthology.org/2023.findings-acl.576.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and\\nWilliam El Sayed. Mistral 7b, 2023a,arXiv preprint arXiv:2310.06825.URL http://arxiv.org/abs/23\\n10.06825.\\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.\\nLongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In\\nICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024. URL\\nhttps://openreview.net/forum?id=9YvfRrpmyw.\\nYuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang. Lion: Adversarial distillation of proprietary\\nlarge language models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language\\nProcessing, Singapore, 2023b. URLhttps://aclanthology.org/2023.emnlp-main.189.\\nYunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. $s^3$: Increasing GPU utilization during\\ngenerativeinferenceforhigherthroughput. In Thirty-seventh Conference on Neural Information Processing\\nSystems, 2023. URLhttps://openreview.net/forum?id=zUYfbdNl1m.\\nTyler Johnson, Pulkit Agrawal, Haijie Gu, and Carlos Guestrin. AdaScale SGD: A user-friendly algorithm\\nfor distributed training. In Hal Daumé III and Aarti Singh (eds.),Proceedings of the 37th International\\nConference on Machine Learning, volume119of Proceedings of Machine Learning Research, pp.4911–4920.\\nPMLR, 13–18 Jul 2020. URLhttps://proceedings.mlr.press/v119/johnson20a.html.\\nHoyoun Jung and Kyung-Joong Kim. Discrete prompt compression with reinforcement learning, 2023,arXiv\\npreprint arXiv:2308.08758.URL http://arxiv.org/abs/2308.08758.\\nJean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy.\\nChallenges and applications of large language models, 2023,arXiv preprint arXiv:2307.10169.URL http:\\n//arxiv.org/abs/2307.10169.\\nDhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth\\nAvancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang,\\nJongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha\\nSmelyanskiy, Bharat Kaul, and Pradeep Dubey. A study of bfloat16 for deep learning training, 2019,\\narXiv preprint arXiv:1905.12322.URL http://arxiv.org/abs/1905.12322.\\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hyper-\\ncomplex adapter layers. InAdvances in Neural Information Processing Systems, volume 34, New Orleans,\\nLouisiana, 2021. URLhttps://proceedings.neurips.cc/paper%5Ffiles/paper/2021/file/081be9f\\ndff07f3bc808f935906ef70c0-Paper.pdf.\\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs: Fast\\nautoregressive transformers with linear attention. InProceedings of the 37th International Conference on\\nMachine Learning, volume 119, 2020. URLhttps://proceedings.mlr.press/v119/katharopoulos20\\na.html.\\n47'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 47, 'page_label': '48'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nJeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo\\nLee. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization.\\nIn Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openre\\nview.net/forum?id=2jUKhUrBxP.\\nMinsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, and Jungwook\\nChoi. Token-scaled logit distillation for ternary weight generative language models. InThirty-seventh\\nConference on Neural Information Processing Systems, 2023b. URLhttps://openreview.net/forum?i\\nd=FUnEkOkodU.\\nSehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami,\\nand Kurt Keutzer. Speculative decoding with big little decoder. InThirty-seventh Conference on Neural\\nInformation Processing Systems, 2023c. URLhttps://openreview.net/forum?id=EfMyf9MC3t.\\nSehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney,\\nand Kurt Keutzer. Squeezellm: Dense-and-sparse quantization, 2024,arXiv preprint arXiv:2306.07629.\\nURL http://arxiv.org/abs/2306.07629.\\nYoung Jin Kim, Rawn Henry, Raffy Fahim, and Hany Hassan Awadalla. Finequant: Unlocking efficiency\\nwith fine-grained weight-only quantization for llms, 2023d,arXiv preprint arXiv:2308.09723.URL http:\\n//arxiv.org/abs/2308.09723.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017,arXiv preprint\\narXiv:1412.6980. URL http://arxiv.org/abs/1412.6980.\\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. InInternational\\nConference on Learning Representations, 2020. URLhttps://openreview.net/forum?id=rkgNKkHtvB.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language\\nmodels are zero-shot reasoners. In Advances in Neural Information Processing Systems, 2022. URL\\nhttps://openreview.net/forum?id=e2TBb5y0yFf.\\nVijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad\\nShoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. In\\nProceedings of Machine Learning and Systems, volume 5, 2023. URLhttps://proceedings.mlsys.or\\ng/paper_files/paper/2023/hash/e851ca7b43815718fbbac8afb2246bf8-Abstract-mlsys2023.html.\\nSiddharth Krishna Kumar. On weight initialization in deep neural networks, 2017, arXiv preprint\\narXiv:1704.08863. URL http://arxiv.org/abs/1704.08863.\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonza-\\nlez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with\\npagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, 2023. URL\\nhttps://doi.org/10.1145/3600006.3613165.\\nChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Lessons learned from ac-\\ntivation outliers for weight quantization in large language models, 2023,arXiv preprint arXiv:2306.02272.\\nURL http://arxiv.org/abs/2306.02272.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\\nKrikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation\\nand automatic sharding. In International Conference on Learning Representations, 2021. URL https:\\n//openreview.net/forum?id=qrwe7XHTmYb.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.\\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. URL\\nhttps://aclanthology.org/2021.emnlp-main.243.\\n48'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 48, 'page_label': '49'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative\\ndecoding. In Proceedings of the 40th International Conference on Machine Learning, 2023. URL\\nhttps://dl.acm.org/doi/10.5555/3618408.3619203.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. BASE Layers: Simplifying\\ntraining of large, sparse models. InProceedings of the 38th International Conference on Machine Learning,\\nvolume 139, 2021. URLhttps://proceedings.mlr.press/v139/lewis21a.html.\\nConglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and Yuxiong He. 1-bit lamb:\\nCommunication efficient large-scale large-batch training with lamb’s convergence speed. In IEEE In-\\nternational Conference on High Performance Computing, Data, and Analytics (HiPC) , 2022. URL\\nhttps://ieeexplore.ieee.org/abstract/document/10106313.\\nJiamin Li, Yimin Jiang, Yibo Zhu, Cong Wang, and Hong Xu. Accelerating distributed MoE training and\\ninference with lina. InUSENIX Annual Technical Conference (USENIX ATC), Boston, MA, 2023a. URL\\nhttps://www.usenix.org/conference/atc23/presentation/li-jiamin.\\nLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Symbolic chain-\\nof-thought distillation: Small models can also “think” step-by-step. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada,\\n2023b. URL https://aclanthology.org/2023.acl-long.150.\\nShanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai,\\nYiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions\\nimproves long context transformers. InThe Twelfth International Conference on Learning Representations,\\n2024a. URL https://openreview.net/forum?id=rR03qFesqk.\\nShenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, and\\nYang You. Colossal-ai: A unified deep learning system for large-scale parallel training. InProceedings of\\nthe 52nd International Conference on Parallel Processing, 2023c. URLhttps://doi.org/10.1145/3605\\n573.3605613.\\nShenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long\\nsequence training from system perspective. InProceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023d. URLhttps://aclant\\nhology.org/2023.acl-long.134.\\nShiyang Li, Jianshu Chen, yelong shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin\\nPeng, Yi Mao, Wenhu Chen, and Xifeng Yan. Explanations from large language models make small\\nreasoners better. InWorkshop on Sustainable AI, 2024b. URLhttps://openreview.net/forum?id=rH\\n8ZUcfL9r.\\nShiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong\\nYang, and Yu Wang. Evaluating quantized large language models, 2024c,arXiv preprint arXiv:2402.18158.\\nURL http://arxiv.org/abs/2402.18158.\\nXiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen\\nQin, Zheng Zhang, Aixin Sun, and Yequan Wang. Flm-101b: An open llm and how to train it with $100k\\nbudget, 2023e,arXiv preprint arXiv:2309.03852.URL http://arxiv.org/abs/2309.03852.\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. InProceedings\\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), 2021. URL https://acla\\nnthology.org/2021.acl-long.353.\\nXiaonan Li and Xipeng Qiu. Finding support examples for in-context learning. InFindings of the Association\\nfor Computational Linguistics: EMNLP 2023, Singapore, 2023. URLhttps://aclanthology.org/202\\n3.findings-emnlp.411.\\n49'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 49, 'page_label': '50'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng\\nQiu. Unified demonstration retriever for in-context learning. InProceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023f. URL\\nhttps://aclanthology.org/2023.acl-long.256.\\nYixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao. LoSparse:\\nStructured compression of large language models based on low-rank and sparse approximation. InPro-\\nceedings of the 40th International Conference on Machine Learning, volume 202, pp. 20336–20350, 23–29\\nJul 2023g. URLhttps://proceedings.mlr.press/v202/li23ap.html.\\nYixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, and Tuo Zhao. Loftq:\\nLoRA-fine-tuning-aware quantization for large language models. InThe Twelfth International Conference\\non Learning Representations, 2024d. URLhttps://openreview.net/forum?id=LzPWWPAdY4.\\nChen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Less is more: Task-\\naware layer-wise distillation for language model compression. In Proceedings of the 40th International\\nConference on Machine Learning, volume 202, 2023. URLhttps://proceedings.mlr.press/v202/lia\\nng23j.html.\\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang Gan, and Song Han.\\nAwq: Activation-aware weight quantization for llm compression and acceleration, 2023,arXiv preprint\\narXiv:2306.00978. URL http://arxiv.org/abs/2306.00978.\\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A\\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. InAdvances\\nin Neural Information Processing Systems, volume 35, 2022a. URLhttps://proceedings.neurips.cc\\n/paper_files/paper/2022/file/0cde695b83bd186c1fd456302888454c-Paper-Conference.pdf.\\nHong Liu, Zhiyuan Li, David Leo Wright Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic\\nsecond-order optimizer for language model pre-training. In The Twelfth International Conference on\\nLearning Representations, 2024a. URLhttps://openreview.net/forum?id=3xHDeA8Noi.\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes\\ngood in-context examples for GPT-3? InProceedings of Deep Learning Inside Out: The 3rd Workshop on\\nKnowledge Extraction and Integration for Deep Learning Architectures, 2022b. URL https://aclantho\\nlogy.org/2022.deelio-1.10.\\nJiachen Liu, Zhiyu Wu, Jae-Won Chung, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury. Andes:\\nDefining and enhancing quality-of-experience in llm-based text streaming services, 2024b,arXiv preprint\\narXiv:2404.16283. URL http://arxiv.org/abs/2404.16283.\\nJing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, and Bohan Zhuang. QLLM: Accurate and\\nefficient low-bitwidth quantization for large language models. InThe Twelfth International Conference on\\nLearning Representations, 2024c. URLhttps://openreview.net/forum?id=FIplmUWdm3.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\\nprompt, and predict: A systematic survey of prompting methods in natural language processing.ACM\\nComputing Surveys, 55, 2023a. URLhttps://doi.org/10.1145/3560815.\\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt\\ntuning can be comparable to fine-tuning across scales and tasks. InProceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume 2: Short Papers), Dublin, Ireland, 2022c. URL\\nhttps://aclanthology.org/2022.acl-short.8.\\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands,\\ntoo, 2023b,arXiv preprint arXiv:2103.10385.URL http://arxiv.org/abs/2103.10385.\\n50'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 50, 'page_label': '51'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nYuliang Liu, Shenggui Li, Jiarui Fang, Yanjun Shao, Boyuan Yao, and Yang You. Colossal-auto: Uni-\\nfied automation of parallelization and activation checkpoint for large-scale models, 2023c,arXiv preprint\\narXiv:2302.02599. URL http://arxiv.org/abs/2302.02599.\\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,\\nRaghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for\\nlarge language models, 2023d,arXiv preprint arXiv:2305.17888.URL http://arxiv.org/abs/2305.178\\n88.\\nZichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis,\\nand Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for LLM\\nKV cache compression at test time. In Thirty-seventh Conference on Neural Information Processing\\nSystems, 2023e. URLhttps://openreview.net/forum?id=JZfg6wGi6g.\\nZichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,\\nYuandong Tian, Christopher Re, and Beidi Chen. Deja vu: Contextual sparsity for efficient LLMs at\\ninference time. In Proceedings of the 40th International Conference on Machine Learning, volume 202,\\n2023f. URL https://proceedings.mlr.press/v202/liu23am.html.\\nZirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Tang, Zhimeng Jiang,\\nKaixiong Zhou, Vipin Chaudhary, Shuai Xu, and Xia Hu. Winner-take-all column row sampling for\\nmemory efficient adaptation of language model. In Thirty-seventh Conference on Neural Information\\nProcessing Systems, 2023g. URLhttps://openreview.net/forum?id=SquMNyrk1O.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on\\nLearning Representations, 2019. URLhttps://openreview.net/forum?id=Bkg6RiCqY7.\\nYaoLu, MaxBartolo, AlastairMoore, SebastianRiedel, andPontusStenetorp. Fantasticallyorderedprompts\\nand where to find them: Overcoming few-shot prompt order sensitivity. InProceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, 2022.\\nURL https://aclanthology.org/2022.acl-long.556.\\nYucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, and Yuxiong He. Maximizing communication\\nefficiency for large-scale training via 0/1 adam. InThe Eleventh International Conference on Learning\\nRepresentations, 2023. URLhttps://openreview.net/forum?id=-CefY2EOupj.\\nMan Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and\\nVincent Y Zhao. Dr.ICL: Demonstration-retrieved in-context learning. InR0-FoMo:Robustness of Few-\\nshot and Zero-shot Learning in Large Foundation Models, 2023. URL https://openreview.net/forum\\n?id=NDNb6L5xjI.\\nKai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. Full parameter fine-\\ntuning for large language models with limited resources, 2023,arXiv preprint arXiv:2306.09782. URL\\nhttp://arxiv.org/abs/2306.09782.\\nXinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-pruner: On the structural pruning of large language\\nmodels. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:\\n//openreview.net/forum?id=J8Ajf9WfXP.\\nSadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev\\nArora. Fine-tuning language models with just forward passes. InThirty-seventh Conference on Neural\\nInformation Processing Systems, 2023. URLhttps://openreview.net/forum?id=Vota6rFhBQ.\\nPedro Henrique Martins, Zita Marinho, and Andre Martins.∞-former: Infinite memory transformer. In\\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), Dublin, Ireland, 2022. URLhttps://aclanthology.org/2022.acl-long.375.\\n51'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 51, 'page_label': '52'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nHarsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via\\ngated state spaces. In The Eleventh International Conference on Learning Representations, 2023. URL\\nhttps://openreview.net/forum?id=5MkYIYCbva.\\nMeta. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https:\\n//ai.meta.com/blog/meta-llama-3/.\\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee\\nWong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna\\nAbhyankar, and Zhihao Jia. Specinfer: Accelerating generative large language model serving with\\ntree-based speculative inference and verification, 2024, arXiv preprint arXiv:2305.09781. URL http:\\n//arxiv.org/abs/2305.09781.\\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris\\nGinsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training.\\nIn International Conference on Learning Representations, 2018. URL https://openreview.net/forum\\n?id=r1gs9JgRZ.\\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context.\\nInProceedings of the 2022 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Seattle, United States, 2022a. URLhttps://aclanthology\\n.org/2022.naacl-main.201.\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\\nmoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings\\nof the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United Arab\\nEmirates, 2022b. URLhttps://aclanthology.org/2022.emnlp-main.759.\\nMLC-LLM, MLC-LLM, 2023,https://github.com/mlc-ai/mlc-llm.\\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for\\ntransformers, 2023,arXiv preprint arXiv:2305.16300.URL http://arxiv.org/abs/2305.16300.\\nGiovanni Monea, Armand Joulin, and Edouard Grave. Pass: Parallel speculative sampling, 2023,arXiv\\npreprint arXiv:2311.13581.URL http://arxiv.org/abs/2311.13581.\\nPhilipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih\\nElibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: A distributed framework for\\nemerging AI applications. In13th USENIX Symposium on Operating Systems Design and Implementation\\n(OSDI), Carlsbad, CA, 2018. URLhttps://www.usenix.org/conference/osdi18/presentation/mori\\ntz.\\nMosaicML. Composer. https://github.com/mosaicml/composer, 2023a. GitHub repository.\\nMosaicML. Llm foundry.https://github.com/mosaicml/llm-foundry, 2023b. GitHub repository.\\nJesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. InThirty-\\nseventh Conference on Neural Information Processing Systems, 2023. URLhttps://openreview.net/f\\norum?id=2DtxPCL3T5.\\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Kor-\\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee,\\nand Matei Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. In\\nProceedings of the International Conference for High Performance Computing, Networking, Storage and\\nAnalysis, 2021. URLhttps://doi.org/10.1145/3458817.3476209.\\nXuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeleton-of-thought:\\nPrompting LLMs for efficient parallel generation. InThe Twelfth International Conference on Learning\\nRepresentations, 2024. URLhttps://openreview.net/forum?id=mqVgBbNCm9.\\n52'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 52, 'page_label': '53'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nNVIDIA. Fastertransformer: High performance transformer kernels.https://github.com/NVIDIA/Faster\\nTransformer, 2023a. GitHub repository.\\nNVIDIA. Tensorrt-llm. https://github.com/NVIDIA/TensorRT-LLM, 2023b. Accessed: 2024-05-10.\\nOpenAI. Gpt base model.https://platform.openai.com/docs/models/gpt-base, 2023.\\nShankar Padmanabhan, Yasumasa Onoe, Michael JQ Zhang, Greg Durrett, and Eunsol Choi. Propagating\\nknowledge updates to LMs through distillation. In Thirty-seventh Conference on Neural Information\\nProcessing Systems, 2023. URLhttps://openreview.net/forum?id=DFaGf3O7jf.\\nMatteo Pagliardini, Daniele Paliotta, Martin Jaggi, and François Fleuret. Fast attention over long sequences\\nwith dynamic sparse flash attention. In Thirty-seventh Conference on Neural Information Processing\\nSystems, 2023. URLhttps://openreview.net/forum?id=UINHuKeWUa.\\nYu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng Shang, Xin Jiang, and Qun Liu. Reusing pretrained\\nmodels by multi-linear operators for efficient training. InThirty-seventh Conference on Neural Information\\nProcessing Systems, 2023. URLhttps://openreview.net/forum?id=RgNXKIrWyU.\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4,\\n2023a, arXiv preprint arXiv:2304.03277.URL http://arxiv.org/abs/2304.03277.\\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao,\\nXin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He,\\nHaowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju\\nLin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind,\\nStanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing\\nRNNs for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP\\n2023, Singapore, 2023b. URLhttps://aclanthology.org/2023.findings-emnlp.936.\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension\\nof large language models. In The Twelfth International Conference on Learning Representations, 2024.\\nURL https://openreview.net/forum?id=wHBfxhZu1u.\\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random\\nfeature attention. InInternational Conference on Learning Representations, 2021. URLhttps://openre\\nview.net/forum?id=QtTKTdVrFBB.\\nAaron Pham, Chaoyu Yang, Sean Sheng, Shenyang Zhao, Sauyon Lee, Bo Jiang, Fog Dong, Xipeng Guan,\\nand Frost Ming, OpenLLM: Operating LLMs in production, 2023,https://github.com/bentoml/OpenL\\nLM.\\nJason Phang, Yi Mao, Pengcheng He, and Weizhu Chen. Hypertuning: toward adapting large language mod-\\nels without back-propagation. InProceedings of the 40th International Conference on Machine Learning,\\n2023. URL https://dl.acm.org/doi/10.5555/3618408.3619566.\\nJonathan Pilault, Mahan Fathi, Orhan Firat, Christopher Pal, Pierre-Luc Bacon, and Ross Goroshin. Block-\\nstate transformers. InThirty-seventh Conference on Neural Information Processing Systems, New Orleans,\\nLouisiana, 2023. URLhttps://openreview.net/forum?id=XRTxIBs2eu.\\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio,\\nStefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. In\\nProceedings of the 40th International Conference on Machine Learning, volume 202, 2023. URLhttps:\\n//proceedings.mlr.press/v202/poli23a.html.\\nEdoardo Maria Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy. Combining parameter-efficient\\nmodules for task-level generalisation. InProceedings of the 17th Conference of the European Chapter of\\nthe Association for Computational Linguistics, Dubrovnik, Croatia, 2023. URLhttps://aclanthology\\n.org/2023.eacl-main.49.\\n53'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 53, 'page_label': '54'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan\\nXiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.Proceedings of Machine\\nLearning and Systems, 5, 2023.\\nRamya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, and Ashish Panwar. vatten-\\ntion: Dynamic memory management for serving llms without pagedattention, 2024, arXiv preprint\\narXiv:2405.04437. URL http://arxiv.org/abs/2405.04437.\\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables\\ninput length extrapolation. InInternational Conference on Learning Representations, 2022. URLhttps:\\n//openreview.net/forum?id=R8sQPpGCv0.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and\\nnarrowing the compositionality gap in language models. InFindings of the Association for Computational\\nLinguistics: EMNLP, Singapore, 2023. URLhttps://aclanthology.org/2023.findings-emnlp.378.\\nChengwei Qin, Aston Zhang, Anirudh Dagar, and Wenming Ye. In-context learning with iterative demon-\\nstration selection, 2023a,arXiv preprint arXiv:2310.09881.URL http://arxiv.org/abs/2310.09881.\\nGuanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, and Benjamin Van Durme. Nugget 2d: Dynamic\\ncontextual compression for scaling decoder-only language models, 2023b,arXiv preprint arXiv:2310.02409.\\nURL http://arxiv.org/abs/2310.02409.\\nYujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng\\nLi, Maosong Sun, and Jie Zhou. Knowledge inheritance for pre-trained language models. InProceedings\\nof the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Seattle, United States, 2022. URLhttps://aclanthology.org/2022.na\\nacl-main.288.\\nZhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-\\n2: A free lunch for handling unlimited sequence lengths in large language models, 2024,arXiv preprint\\narXiv:2401.04658. URL http://arxiv.org/abs/2401.04658.\\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for\\nlong document understanding. In Findings of the Association for Computational Linguistics: EMNLP,\\n2020. URL https://aclanthology.org/2020.findings-emnlp.232.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\\nare unsupervised multitask learners. OpenAI blog, 2019. URLhttps://cdn.openai.com/better-langu\\nage-models/language_models_are_unsupervised_multitask_learners.pdf.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob\\nMenick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh,\\nPo-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato,\\nJohn Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar,\\nElena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre,\\nLena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic\\nDonato, AngelikiLazaridou, ArthurMensch, Jean-BaptisteLespiau, MariaTsimpoukelli, NikolaiGrigorev,\\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien\\nde Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego\\nde Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura\\nWeidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Ge-\\noffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2022,arXiv\\npreprint arXiv:2112.11446.URL http://arxiv.org/abs/2112.11446.\\n54'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 54, 'page_label': '55'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\\nJournal of Machine Learning Research, 21, 2020. URLhttps://dl.acm.org/doi/abs/10.5555/34557\\n16.3455856.\\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward\\ntraining trillion parameter models. InProceedings of the International Conference for High Performance\\nComputing, Networking, Storage and Analysis, 2020. URLhttps://dl.acm.org/doi/10.5555/3433701\\n.3433727.\\nSamyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. Zero-infinity: Breaking\\nthe gpu memory wall for extreme scale deep learning. InProceedings of the International Conference for\\nHigh Performance Computing, Networking, Storage and Analysis, 2021. URLhttps://doi.org/10.114\\n5/3458817.3476205.\\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad\\nAwan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture-of-experts inference and train-\\ning to power next-generation AI scale. InProceedings of the 39th International Conference on Machine\\nLearning, volume 162, 2022. URLhttps://proceedings.mlr.press/v162/rajbhandari22a.html.\\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations\\nenable training deep learning models with over 100 billion parameters. InProceedings of the 26th ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining, 2020. URL https://doi.\\norg/10.1145/3394486.3406703.\\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon\\nShashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows for large language models.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), Toronto, Canada, 2023. URLhttps://aclanthology.org/2023.acl-long.352.\\nRay Project, RayLLM - LLMs on Ray, GitHub repository, 2023,https://github.com/ray-project/ray\\n-llm, Accessed on: 2023-10-02.\\nJie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang,\\nDong Li, and Yuxiong He. ZeRO-Offload: Democratizing Billion-Scale model training. InUSENIX Annual\\nTechnical Conference (USENIX ATC, 2021. URL https://www.usenix.org/conference/atc21/pres\\nentation/ren-jie.\\nLiliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. Sparse\\nmodular activation for efficient sequence modeling. InThirty-seventh Conference on Neural Information\\nProcessing Systems, 2023a. URLhttps://openreview.net/forum?id=TfbzX6I14i.\\nXiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda\\nZhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin\\nJiang, Teng Su, Qun Liu, and Jun Yao. Pangu-Σ: Towards trillion parameter language model with sparse\\nheterogeneous computing, 2023b,arXiv preprint arXiv:2303.10845.URL http://arxiv.org/abs/2303\\n.10845.\\nAdithya Renduchintala, Tugrul Konuk, and Oleksii Kuchaiev. Tied-lora: Enhacing parameter efficiency of\\nlorawithweighttying, 2023, arXiv preprint arXiv:2311.09578.URLhttp://arxiv.org/abs/2311.09578.\\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention\\nwith routing transformers.Transactions of the Association for Computational Linguistics, 9, 2021. URL\\nhttps://aclanthology.org/2021.tacl-1.4.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In\\nProceedings of the 2022 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Seattle, United States, 2022. URLhttps://aclanthology\\n.org/2022.naacl-main.191.\\n55'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 55, 'page_label': '56'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nAndrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin,\\nand Emanuele Rodola. Accelerating transformer inference for translation via parallel decoding. InPro-\\nceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), Toronto, Canada, 2023. URLhttps://aclanthology.org/2023.acl-long.689.\\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.\\nRAPTOR: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International\\nConference on Learning Representations, 2024. URLhttps://openreview.net/forum?id=GN921JHCRw.\\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné,\\nAlexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella\\nBiderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muen-\\nnighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-\\nMajor, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lau-\\nrençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor\\nSoroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\\nChris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir\\nRadev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni,\\nGérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu,\\nIdris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian\\nZhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo\\nChen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan\\nDey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin\\nCoavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb,\\nNishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert,\\nPauloVillegas, PeterHenderson, PierreColombo, PriscillaAmuok, QuentinLhoest, RhezaHarliman, Rishi\\nBommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik\\nBose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav\\nSilberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin\\nDanchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak\\nTalat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J.\\nMielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti\\nDatta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan\\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal\\nNayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim,\\nTali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin\\nYong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jae-\\nsung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff\\nRasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas\\nPatry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée,\\nRémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim\\nDettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramo-\\nnian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Takta-\\nsheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo,\\nJekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Ma-\\nrine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der\\nWal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina,\\nThomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov,\\nYada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pes-\\ntana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash\\nAghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade,\\nBharat Saxena, Carlos Muñoz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David,\\nDouwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline\\nOnoniwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\\njadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri,\\n56'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 56, 'page_label': '57'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nMargot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri,\\nMykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An,\\nRasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Vigu-\\nier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo\\nPalasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz,\\nBo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán,\\nDaniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyased-\\ndin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas\\nGolde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato,\\nMadeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina,\\nMario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihalj-\\ncic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio\\nBroad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert\\nMartin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh,\\nShubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil\\nBharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Ba-\\njaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras,\\nYounes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model,\\n2023, arXiv preprint arXiv:2211.05100.URL http://arxiv.org/abs/2211.05100.\\nStephanie Schoch, Ritwick Mishra, and Yangfeng Ji. Data selection for fine-tuning large language mod-\\nels using transferred shapley values. In Proceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 4: Student Research Workshop), Toronto, Canada, 2023. URL\\nhttps://aclanthology.org/2023.acl-srw.37.\\nHang Shao, Bei Liu, and Yanmin Qian. One-shot sensitivity-aware mixed sparsity pruning for large language\\nmodels, 2024,arXiv preprint arXiv:2310.09499.URL http://arxiv.org/abs/2310.09499.\\nJunru Shao, Xiyou Zhou, Siyuan Feng, Bohan Hou, Ruihang Lai, Hongyi Jin, Wuwei Lin, Masahiro Masuda,\\nCody Hao Yu, and Tianqi Chen. Tensor program optimization with probabilistic programs. InAdvances\\nin Neural Information Processing Systems, volume 35, 2022. URLhttps://proceedings.neurips.cc/p\\naper%5Ffiles/paper/2022/file/e894eafae43e68b4c8dfdacf742bcbf3-Paper-Conference.pdf.\\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In\\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 2 (Short Papers), New Orleans, Louisiana, 2018.\\nURL https://aclanthology.org/N18-2074.\\nNoam Shazeer. Fast transformer decoding: One write-head is all you need, 2019, arXiv preprint\\narXiv:1911.02150. URL http://arxiv.org/abs/1911.02150.\\nSheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for\\ntransformer language models. InProceedings of the 39th International Conference on Machine Learning,\\nvolume 162, 2022. URLhttps://proceedings.mlr.press/v162/shen22f.html.\\nSheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph,\\nWilliam Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Y\\nZhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. Mixture-of-experts meets instruction\\ntuning: A winning combination for large language models. InThe Twelfth International Conference on\\nLearning Representations, 2024. URLhttps://openreview.net/forum?id=6mLjDwYte5.\\nYing Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie,\\nBeidi Chen, Clark W. Barrett, Joseph Gonzalez, Percy Liang, Christopher Ré, Ioan Cristian Stoica,\\nand Ce Zhang. High-throughput generative inference of large language models with a single gpu. In\\nInternational Conference on Machine Learning, 2023. URL https://dl.acm.org/doi/10.5555/36184\\n08.3619696.\\n57'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 57, 'page_label': '58'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting\\nKnowledge from Language Models with Automatically Generated Prompts. InProceedings of the 2020\\nConference on Empirical Methods in Natural Language Processing (EMNLP), Online, 2020. URLhttps:\\n//aclanthology.org/2020.emnlp-main.346.\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\\nMegatron-lm: Training multi-billion parameter language models using model parallelism, 2020, arXiv\\npreprint arXiv:1909.08053.URL http://arxiv.org/abs/1909.08053.\\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling reasoning capabilities into smaller\\nlanguage models. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto,\\nCanada, 2023. URLhttps://aclanthology.org/2023.findings-acl.441.\\nAntoine Simoulin, Namyong Park, Xiaoyi Liu, and Grey Yang. Memory-efficient selective fine-tuning. In\\nWorkshop on Efficient Systems for Foundation Models, 2023. URL https://openreview.net/forum?i\\nd=zaNbLceVwm.\\nSiddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, and Abhinav\\nBhatele. A hybrid tensor-expert-data parallelism approach to optimize mixture-of-experts training. In\\nICS 2023, 2023.\\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,\\nZhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yaz-\\ndani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh\\nTiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-\\nscale generative language model, 2022,arXiv preprint arXiv:2201.11990.URL http://arxiv.org/abs/\\n2201.11990.\\nSaleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith\\nPeris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar,\\nFabian Triefenbach, Apurv Verma, Gokhan Tur, and Prem Natarajan. Alexatm 20b: Few-shot learning\\nusing a large-scale multilingual seq2seq model, 2022,arXiv preprint arXiv:2208.01448.URL http://ar\\nxiv.org/abs/2208.01448.\\nJ.C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation.\\nIEEE Transactions on Automatic Control, 37, 1992. URLhttps://ieeexplore.ieee.org/document/1\\n19632.\\nBenjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding, 2023,arXiv\\npreprint arXiv:2308.04623.URL http://arxiv.org/abs/2308.04623.\\nHongjin SU, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\\nLuke Zettlemoyer, Noah A. Smith, and Tao Yu. Selective annotation makes language models better\\nfew-shot learners. In The Eleventh International Conference on Learning Representations, 2023. URL\\nhttps://openreview.net/forum?id=qY1hlv7gwg.\\nHui Su, Xiao Zhou, Houjin Yu, Xiaoyu Shen, Yuwen Chen, Zilin Zhu, Yang Yu, and Jie Zhou. Welm: A\\nwell-read pre-trained language model for chinese, 2023a,arXiv preprint arXiv:2209.10372.URL http:\\n//arxiv.org/abs/2209.10372.\\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\\ntransformer with rotary position embedding, 2023b,arXiv preprint arXiv:2104.09864.URL http://arxi\\nv.org/abs/2104.09864.\\nMingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large\\nlanguage models. In The Twelfth International Conference on Learning Representations, 2024. URL\\nhttps://openreview.net/forum?id=PxoFut3dWW.\\n58'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 58, 'page_label': '59'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTianxiang Sun, Zhengfu He, Qin Zhu, Xipeng Qiu, and Xuanjing Huang. Multitask pre-training of modular\\nprompt for Chinese few-shot learning. InProceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023a. URLhttps://aclantho\\nlogy.org/2023.acl-long.625.\\nYutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu\\nWei. Retentive network: A successor to transformer for large language models, 2023b,arXiv preprint\\narXiv:2307.08621. URL http://arxiv.org/abs/2307.08621.\\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia\\nSong, and Furu Wei. A length-extrapolatable transformer. InProceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023c. URL\\nhttps://aclanthology.org/2023.acl-long.816.\\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for rein-\\nforcement learning with function approximation. InAdvances in Neural Information Processing Systems,\\nvolume 12, 1999. URLhttps://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b8\\n5b0bed98e80ade0a5c43b0f-Paper.pdf.\\nWeng Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Jiahua Liu, Tao Li, Yuxiao Dong, and Jie Tang. Parameter-\\nefficient prompt tuning makes generalized and calibrated neural text retrievers. InFindings of the Asso-\\nciation for Computational Linguistics: EMNLP 2023, Singapore, 2023. URLhttps://aclanthology.o\\nrg/2023.findings-emnlp.874.\\nHanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian,\\nJi Liu, Ce Zhang, and Yuxiong He. 1-bit adam: Communication efficient large-scale training with adam’s\\nconvergence speed. InProceedings of the 38th International Conference on Machine Learning, volume 139,\\n2021. URL https://proceedings.mlr.press/v139/tang21a.html.\\nChaofanTao, LuHou, WeiZhang, LifengShang, XinJiang, QunLiu, PingLuo, andNgaiWong. Compression\\nof generative pre-trained language models via quantization. InProceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, 2022. URL\\nhttps://aclanthology.org/2022.acl-long.331.\\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. InProceed-\\nings of the 37th International Conference on Machine Learning, 2020. URLhttps://dl.acm.org/doi/a\\nbs/10.5555/3524938.3525813.\\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM\\nComputing Surveys, 55, 2022. URLhttps://doi.org/10.1145/3530811.\\nGemini Team and Google. Gemini: A family of highly capable multimodal models.https://storage.go\\nogleapis.com/deepmind-media/gemini/gemini_1_report.pdf, 2023.\\nThe MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms.\\nhttps://www.mosaicml.com/blog/mpt-7b, 2023.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin\\nGhafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,\\nYuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching\\nChang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-\\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben\\nZevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina,\\nErin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya\\nKuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,\\nMarian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022, arXiv\\npreprint arXiv:2201.08239.URL http://arxiv.org/abs/2201.08239.\\n59'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 59, 'page_label': '60'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nInar Timiryasov and Jean-Loup Tastet. Baby llama: knowledge distillation from an ensemble of teachers\\ntrained on a small dataset with no performance penalty. InProceedings of the BabyLM Challenge at the\\n27th Conference on Computational Natural Language Learning, Singapore, 2023. URLhttps://aclant\\nhology.org/2023.conll-babylm.24.\\nDenis Timonin, Bo Yang Hsueh, and Vinh Nguyen. Accelerated inference for large transformer models using\\nnvidia triton inference server, 2022. URLhttps://developer.nvidia.com/blog/accelerated-infer\\nence-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-triton-inf\\nerence-server.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models.ArXiv,\\nabs/2302.13971, 2023a. URLhttps://api.semanticscholar.org/CorpusID:257219404.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Fer-\\nrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh\\nKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,\\nXavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy\\nReizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subra-\\nmanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,\\nRobert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat\\nmodels, 2023b,arXiv preprint arXiv:2307.09288.URL http://arxiv.org/abs/2307.09288.\\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi\\nHuang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero,\\nAlexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023,arXiv preprint\\narXiv:2310.16944. URL http://arxiv.org/abs/2310.16944.\\nSzymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr\\nMiłoś. Focused transformer: Contrastive training for context scaling. InThirty-seventh Conference on\\nNeural Information Processing Systems, 2023. URLhttps://openreview.net/forum?id=s1FjXzJ0jy.\\nMojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. DyLoRA: Parameter-efficient\\ntuning of pre-trained models using dynamic search-free low-rank adaptation. InProceedings of the 17th\\nConference of the European Chapter of the Association for Computational Linguistics, Dubrovnik, Croatia,\\n2023. URL https://aclanthology.org/2023.eacl-main.239.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need. InAdvances in Neural Information Processing Systems,\\nvolume 30, 2017. URLhttps://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee2435\\n47dee91fbd053c1c4a845aa-Paper.pdf.\\nApoorv Vyas, Angelos Katharopoulos, and François Fleuret. Fast transformers with clustered attention. In\\nAdvances in Neural Information Processing Systems, volume 33, 2020. URLhttps://proceedings.neur\\nips.cc/paper_files/paper/2020/file/f6a8dd1c954c8506aadc764cc32b895e-Paper.pdf.\\nZhongwei Wan, Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang, Guangyong Chen, Xin Jiang, and Qun\\nLiu. G-MAP: General memory-augmented pre-trained language model for domain tasks. InProceedings\\nof the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United Arab\\nEmirates, 2022. URLhttps://aclanthology.org/2022.emnlp-main.441.\\n60'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 60, 'page_label': '61'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nZhongwei Wan, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, César Quilodrán-Casas, and\\nRossella Arcucci. Med-unic: Unifying cross-lingual medical vision-language pre-training by diminishing\\nbias. In Advances in Neural Information Processing Systems, volume 36, 2023. URLhttps://proceedi\\nngs.neurips.cc/paper_files/paper/2023/file/af38fb8e90d586f209235c94119ba193-Paper-Confe\\nrence.pdf.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,\\nand Samuel R. Bowman. Superglue: a stickier benchmark for general-purpose language understanding\\nsystems. In Proceedings of the 33rd International Conference on Neural Information Processing Systems,\\n2019. URL https://dl.acm.org/doi/10.5555/3454287.3454581.\\nBoxiang Wang, Qifan Xu, Zhengda Bian, and Yang You. Tesseract: Parallelize the tensor parallelism\\nefficiently. InProceedings of the 51st International Conference on Parallel Processing, 2023a. URLhttps:\\n//doi.org/10.1145/3545008.3545087.\\nGuoxin Wang, Yijuan Lu, Lei Cui, Tengchao Lv, Dinei Florencio, and Cha Zhang. A simple yet effective\\nlearnable positional encoding method for improving document transformer model. In Findings of the\\nAssociation for Computational Linguistics: AACL-IJCNLP 2022, 2022a. URLhttps://aclanthology.o\\nrg/2022.findings-aacl.42.\\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping\\nWang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models, 2023b,arXiv\\npreprint arXiv:2310.11453.URL http://arxiv.org/abs/2310.11453.\\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling\\ntransformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024a.\\nURL https://ieeexplore.ieee.org/document/10496231.\\nLiang Wang, Nan Yang, and Furu Wei. Learning to retrieve in-context examples for large language models.\\nIn Proceedings of the 18th Conference of the European Chapter of the Association for Computational\\nLinguistics (Volume 1: Long Papers), St. Julian’s, Malta, 2024b. URLhttps://aclanthology.org/202\\n4.eacl-long.105.\\nNingning Wang, Guobing Gan, Peng Zhang, Shuai Zhang, Junqiu Wei, Qun Liu, and Xin Jiang. Clus-\\nterFormer: Neural clustering attention for efficient and effective transformer. InProceedings of the 60th\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ire-\\nland, 2022b. URLhttps://aclanthology.org/2022.acl-long.170.\\nPeifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. SCOTT: Self-consistent\\nchain-of-thought distillation. InProceedings of the 61st Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023c. URLhttps://aclanthology.org\\n/2023.acl-long.304.\\nPeihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio\\nFeris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for\\nefficient transformer training. In The Eleventh International Conference on Learning Representations,\\n2023d. URL https://openreview.net/forum?id=cDYRS5iZ16f.\\nShuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Junyuan Shang,\\nYanbin Zhao, Chao Pang, Jiaxiang Liu, Xuyi Chen, Yuxiang Lu, Weixin Liu, Xi Wang, Yangfan Bai,\\nQiuliang Chen, Li Zhao, Shiyong Li, Peng Sun, Dianhai Yu, Yanjun Ma, Hao Tian, Hua Wu, Tian\\nWu, Wei Zeng, Ge Li, Wen Gao, and Haifeng Wang. Ernie 3.0 titan: Exploring larger-scale knowledge\\nenhanced pre-training for language understanding and generation, 2021,arXiv preprint arXiv:2112.12731.\\nURL http://arxiv.org/abs/2112.12731.\\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear\\ncomplexity, 2020,arXiv preprint arXiv:2006.04768.URL http://arxiv.org/abs/2006.04768.\\n61'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 61, 'page_label': '62'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nWeizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting\\nlanguage models with long-term memory. InThirty-seventh Conference on Neural Information Processing\\nSystems, 2023e. URLhttps://openreview.net/forum?id=BryMFPQ4L6.\\nXin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. Svd-llm: Truncation-aware singular value decom-\\nposition for large language model compression, 2024c, arXiv preprint arXiv:2403.07378. URL http:\\n//arxiv.org/abs/2403.07378.\\nXinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models\\nare latent variable models: Explaining and finding good demonstrations for in-context learning. InThirty-\\nseventh Conference on Neural Information Processing Systems, 2023f. URLhttps://openreview.net/f\\norum?id=BGvkwZEGt7.\\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah,\\nand Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient model tuning. InProceedings\\nof the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United Arab\\nEmirates, 2022c. URLhttps://aclanthology.org/2022.emnlp-main.388.\\nYiming Wang, Yu Lin, Xiaodong Zeng, and Guannan Zhang. Multilora: Democratizing lora for better\\nmulti-task learning, 2023g,arXiv preprint arXiv:2311.11501.URL http://arxiv.org/abs/2311.11501.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh\\nHajishirzi. Self-instruct: Aligning language models with self-generated instructions. InProceedings of the\\n61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto,\\nCanada, 2023h. URLhttps://aclanthology.org/2023.acl-long.754.\\nYufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin\\nJiang, and Qun Liu. Aligning large language models with human: A survey, 2023i, arXiv preprint\\narXiv:2307.12966. URL http://arxiv.org/abs/2307.12966.\\nZhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. Multitask prompt\\ntuning enables parameter-efficient transfer learning. InThe Eleventh International Conference on Learning\\nRepresentations, 2023j. URLhttps://openreview.net/forum?id=Nk2pDtuhTq.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy\\nLiang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on\\nMachine Learning Research, 2022a. URLhttps://openreview.net/forum?id=yzkSU5zdwD.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\\nand Denny Zhou. Chain of thought prompting elicits reasoning in large language models. InAdvances in\\nNeural Information Processing Systems, 2022b. URLhttps://openreview.net/forum?id=_VjQlMeSB_J.\\nXiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu.\\nOutlier suppression+: Accurate quantization of large language models by equivalent and effective shifting\\nand scaling. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\\nSingapore, 2023. URLhttps://aclanthology.org/2023.emnlp-main.102.\\nGenta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, and Pascale Fung. Lightweight and\\nefficient end-to-end speech recognition using low-rank transformer. InIEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP), 2020. URLhttps://ieeexplore.ieee.org/docume\\nnt/9053878.\\nQingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu. Memformer: A\\nmemory-augmented transformer for sequence modeling. InFindings of the Association for Computational\\nLinguistics: AACL-IJCNLP 2022, 2022a. URLhttps://aclanthology.org/2022.findings-aacl.29.\\nXiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme compression for pre-trained\\ntransformers made simple and efficient. InNeurIPS 2022, 2022b.\\n62'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 62, 'page_label': '63'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nXiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4 quanti-\\nzation for language models: latency speedup, composability, and failure cases. InProceedings of the 40th\\nInternational Conference on Machine Learning, 2023a. URLhttps://dl.acm.org/doi/10.5555/36184\\n08.3619970.\\nXiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap forward in llms post-training w4a8 quan-\\ntization using floating-point formats, 2023b,arXiv preprint arXiv:2307.09782.URL http://arxiv.org/\\nabs/2307.09782.\\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers.\\nIn International Conference on Learning Representations, 2022c. URLhttps://openreview.net/forum\\n?id=TrjbxzRcnf-.\\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning: An\\ninformation compression perspective for in-context example selection and ordering. InProceedings of the\\n61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto,\\nCanada, 2023c. URLhttps://aclanthology.org/2023.acl-long.79.\\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared LLaMA: Accelerating language model\\npre-training via structured pruning. InWorkshop on Advancing Neural Network Training: Computational\\nEfficiency, Scalability, and Resource Optimization, 2023. URLhttps://openreview.net/forum?id=6s\\n77hjBNfS.\\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate\\nand efficient post-training quantization for large language models. InProceedings of the 40th International\\nConference on Machine Learning, volume 202, 2023. URLhttps://proceedings.mlr.press/v202/xia\\no23c.html.\\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language\\nmodels with attention sinks. InThe Twelfth International Conference on Learning Representations, 2024.\\nURL https://openreview.net/forum?id=NG7sS51zVF.\\nSang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V\\nLe, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model\\npretraining. InThirty-seventh Conference on Neural Information Processing Systems, 2023a. URLhttps:\\n//openreview.net/forum?id=lXuByUeHhd.\\nSang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via\\nimportance resampling. InThirty-seventh Conference on Neural Information Processing Systems, 2023b.\\nURL https://openreview.net/forum?id=uPSQv0leAu.\\nMingxue Xu, Yao Lei Xu, and Danilo P. Mandic. Tensorgpt: Efficient compression of the embedding\\nlayer in llms based on the tensor-train decomposition, 2023a, arXiv preprint arXiv:2307.00526. URL\\nhttp://arxiv.org/abs/2307.00526.\\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\\nBakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language\\nmodels. In The Twelfth International Conference on Learning Representations, 2024a. URL https:\\n//openreview.net/forum?id=xw5nxFWMlo.\\nQifan Xu and Yang You. An efficient 2d method for training super-large deep learning models. In2023\\nIEEE International Parallel and Distributed Processing Symposium (IPDPS), pp. 222–232, 2023. doi:\\n10.1109/IPDPS54959.2023.00031.\\nYuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, XIAOPENG\\nZHANG, and Qi Tian. QA-loRA: Quantization-aware low-rank adaptation of large language models. In\\nThe Twelfth International Conference on Learning Representations, 2024b. URLhttps://openreview.n\\net/forum?id=WvFoJccpo8.\\n63'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 63, 'page_label': '64'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nZhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and Anshumali Shri-\\nvastava. Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable\\nprompt, 2023b,arXiv preprint arXiv:2305.11186.URL http://arxiv.org/abs/2305.11186.\\nFuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe:\\nOpen mixture-of-experts language models, 2023. URLhttps://github.com/XueFuzhao/OpenMoE.\\nCheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, and Jingqiao Zhang. Progressively\\nstacking 2.0: A multi-stage layerwise training method for bert training speedup, 2020,arXiv preprint\\narXiv:2011.13635. URL http://arxiv.org/abs/2011.13635.\\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large\\nlanguage models as optimizers, 2023a,arXiv preprint arXiv:2309.03409.URL http://arxiv.org/abs/\\n2309.03409.\\nGreg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, David Farhi, Jakub Pachocki, Xiaodong Liu,\\nWeizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyper-\\nparameter transfer. In NeurIPS 2021, March 2022. URLhttps://www.microsoft.com/en-us/resear\\nch/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/ .\\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong,\\nBing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond.ACM\\nTransactions on Knowledge Discovery from Data, 2024. URLhttps://doi.org/10.1145/3649506.\\nKeming Yang, Zichen Liu, and Philip Cheng, MOSEC: Model Serving made Efficient in the Cloud, 2021,\\nhttps://github.com/mosecorg/mosec.\\nNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu\\nWei. Inference with reference: Lossless acceleration of large language models, 2023b, arXiv preprint\\narXiv:2304.04487. URL http://arxiv.org/abs/2304.04487.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R\\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. InThirty-seventh\\nConference on Neural Information Processing Systems, 2023a. URLhttps://openreview.net/forum?i\\nd=5Xc1ecxO1h.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. Re-\\nact: Synergizing reasoning and acting in language models. InThe Eleventh International Conference on\\nLearning Representations, 2023b. URLhttps://openreview.net/forum?id=WE_vluYUL-X.\\nXingcheng Yao, Yanan Zheng, Xiaocong Yang, and Zhilin Yang. NLP from scratch without large-scale\\npretraining: A simple and efficient framework. In Proceedings of the 39th International Conference on\\nMachine Learning, volume 162, 2022a. URLhttps://proceedings.mlr.press/v162/yao22c.html.\\nYiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. Masked structural growth for 2x faster language\\nmodel pre-training. In The Twelfth International Conference on Learning Representations, 2024. URL\\nhttps://openreview.net/forum?id=rL7xsg1aRn.\\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant:\\nEfficient and affordable post-training quantization for large-scale transformers. In Advances in Neural\\nInformation Processing Systems, volume 35, 2022b. URLhttps://proceedings.neurips.cc/paper_f\\niles/paper/2022/file/adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf.\\nZhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad\\nAwan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly\\nSmith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, and Yuxiong He.\\nDeepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales, 2023c,arXiv\\npreprint arXiv:2308.01320.URL http://arxiv.org/abs/2308.01320.\\n64'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 64, 'page_label': '65'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nZhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Exploring post-\\ntraining quantization in llms from comprehensive study to low rank compensation, 2023d,arXiv preprint\\narXiv:2303.08302. URL http://arxiv.org/abs/2303.08302.\\nRongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. Edgemoe: Fast on-\\ndevice inference of moe-based large language models, 2023,arXiv preprint arXiv:2308.14352.URL http:\\n//arxiv.org/abs/2308.14352.\\nJie You, Jae-Won Chung, and Mosharaf Chowdhury. Zeus: Understanding and optimizing GPU energy\\nconsumption of DNN training. In 20th USENIX Symposium on Networked Systems Design and Imple-\\nmentation (NSDI), Boston, MA, 2023. URLhttps://www.usenix.org/conference/nsdi23/presentat\\nion/you.\\nGyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed\\nserving system for Transformer-Based generative models. In 16th USENIX Symposium on Operating\\nSystems Design and Implementation (OSDI), Carlsbad, CA, 2022. URLhttps://www.usenix.org/con\\nference/osdi22/presentation/yu.\\nLILI YU, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis.\\nMEGABYTE: Predicting million-byte sequences with multiscale transformers. In Thirty-seventh Con-\\nference on Neural Information Processing Systems, 2023. URLhttps://openreview.net/forum?id=JT\\nmO2V9Xpz.\\nZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang\\nWu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language\\nmodels, 2023a,arXiv preprint arXiv:2304.01089.URL http://arxiv.org/abs/2304.01089.\\nZhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activation-\\naware singular value decomposition for compressing large language models, 2023b, arXiv preprint\\narXiv:2312.05821. URL http://arxiv.org/abs/2312.05821.\\nManzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\\nPham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: transformers for longer\\nsequences. InProceedings of the 34th International Conference on Neural Information Processing Systems,\\n2020. URL https://dl.acm.org/doi/abs/10.5555/3495724.3497174.\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\\nZheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu,\\nPeng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pre-trained model. In The\\nEleventh International Conference on Learning Representations, 2023. URLhttps://openreview.net/f\\norum?id=-Aw0rrrPUF.\\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\\nWang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang, Jun Wang, Jianfeng Yu, Qi Guo,\\nYue Yu, Yan Zhang, Jin Wang, Hengtao Tao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing Jiang, Han\\nZhang, Lingfeng Deng, Yehong Zhang, Zhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo, Shanzhi\\nGu, Gaojun Fan, Yaowei Wang, Xuefeng Jin, Qun Liu, and Yonghong Tian. Pangu- α: Large-scale\\nautoregressive pretrained chinese language models with auto-parallel computation, 2021,arXiv preprint\\narXiv:2104.12369. URL http://arxiv.org/abs/2104.12369.\\nMingshu Zhai, Jiaao He, Zixuan Ma, Zan Zong, Runqing Zhang, and Jidong Zhai. SmartMoE: Efficiently\\ntraining Sparsely-Activated models through combining offline and online parallelization. In2023 USENIX\\nAnnual Technical Conference (USENIX ATC), Boston, MA, 2023. URLhttps://www.usenix.org/con\\nference/atc23/presentation/zhai.\\nChen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. Towards the law of capacity gap in distilling language\\nmodels, 2023a,arXiv preprint arXiv:2311.07052.URL http://arxiv.org/abs/2311.07052.\\n65'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 65, 'page_label': '66'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nHang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. Pool-\\ningformer: Long document modeling with pooling attention. In Proceedings of the 38th International\\nConference on Machine Learning, volume 139, 2021. URLhttps://proceedings.mlr.press/v139/zha\\nng21h.html.\\nHongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Residual learning without normalization via better\\ninitialization. In International Conference on Learning Representations, 2019. URL https://openrevi\\new.net/forum?id=H1gsz30cKX.\\nLongteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient low-rank\\nadaptation for large language models fine-tuning, 2023b,arXiv preprint arXiv:2308.03303.URL http:\\n//arxiv.org/abs/2308.03303.\\nMingyangZhang, HaoChen, ChunhuaShen, ZhenYang, LinlinOu, XinyiYu, andBohanZhuang. Loraprune:\\nPruning meets low-rank parameter-efficient fine-tuning, 2023c, arXiv preprint arXiv:2305.18403. URL\\nhttp://arxiv.org/abs/2305.18403.\\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.\\nAdaptive budget allocation for parameter-efficient fine-tuning. InThe Eleventh International Conference\\non Learning Representations, 2023d. URLhttps://openreview.net/forum?id=lq62uWRJjiY.\\nRenrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, and Peng Gao.\\nLLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In The\\nTwelfth International Conference on Learning Representations, 2024. URLhttps://openreview.net/f\\norum?id=d4UiXAHN2W.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\\nMona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained\\ntransformer language models, 2022a,arXiv preprint arXiv:2205.01068.URL http://arxiv.org/abs/22\\n05.01068.\\nTianyi Zhang, Mina Lee, Xiang Lisa Li, Ende Shen, and Tatsunori Hashimoto. TempLM: Distilling language\\nmodels into template-based generators. InFindings of the Association for Computational Linguistics: ACL\\n2023, Toronto, Canada, 2023e. URLhttps://aclanthology.org/2023.findings-acl.124.\\nYiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. InProceedings\\nof the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United Arab\\nEmirates, 2022b. URLhttps://aclanthology.org/2022.emnlp-main.622.\\nYue Zhang, Hongliang Fei, Dingcheng Li, and Ping Li. PromptGen: Automatically generate prompts using\\ngenerative models. InFindings of the Association for Computational Linguistics: NAACL 2022, Seattle,\\nUnited States, 2022c. URLhttps://aclanthology.org/2022.findings-naacl.3.\\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong\\nTian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for\\nefficient generative inference of large language models. InWorkshop on Efficient Systems for Foundation\\nModels, 2023f. URLhttps://openreview.net/forum?id=ctPizehA9D.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large\\nlanguage models. In The Eleventh International Conference on Learning Representations, 2023g. URL\\nhttps://openreview.net/forum?id=5NTt8GFjUHkr.\\nJiawei Zhao, Florian Tobias Schaefer, and Anima Anandkumar. ZerO Initialization: Initializing neural\\nnetworks with only zeros and ones. Transactions on Machine Learning Research, 2022. URL https:\\n//openreview.net/forum?id=1AxQpKmiTc.\\n66'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 66, 'page_label': '67'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A Survey\\nof Large Language Models, 2023a,arXiv preprint arXiv:2303.18223.URL http://arxiv.org/abs/2303\\n.18223.\\nWeilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, and Maosong Sun. CPET: Effective\\nparameter-efficient tuning for compressed large language models, 2023b,arXiv preprint arXiv:2307.07705.\\nURL http://arxiv.org/abs/2307.07705.\\nYanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid\\nShojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen,\\nGeeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. PyTorch FSDP: Experiences on Scaling Fully\\nSharded Data Parallel.Proceedings of the VLDB Endowment, 16, 2023c. URLhttps://doi.org/10.147\\n78/3611540.3611569.\\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang,\\nYang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generation with\\nmultilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining, 2023. URLhttps://doi.org/10.1145/3580305.3599790.\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping\\nYu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less\\nis more for alignment. InThirty-seventh Conference on Neural Information Processing Systems, 2023a.\\nURL https://openreview.net/forum?id=KBMOKmX2he.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\\nCui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning\\nin large language models. InThe Eleventh International Conference on Learning Representations, 2023b.\\nURL https://openreview.net/forum?id=WZH7099tgfM.\\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, zhifeng Chen,\\nQuoc V Le, and James Laudon. Mixture-of-experts with expert choice routing. InAdvances in Neural\\nInformation Processing Systems, volume 35, 2022. URLhttps://proceedings.neurips.cc/paper_fil\\nes/paper/2022/file/2f00ecd787b432c1d36f3de9800728eb-Paper-Conference.pdf.\\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\\nBa. Large language models are human-level prompt engineers. InThe Eleventh International Conference\\non Learning Representations, 2023c. URLhttps://openreview.net/forum?id=92gvk82DE-.\\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. PoSE: Efficient con-\\ntext window extension of LLMs via positional skip-wise training. InThe Twelfth International Conference\\non Learning Representations, 2024. URLhttps://openreview.net/forum?id=3Z1gxuAQrA.\\nBohan Zhuang, Jing Liu, Zizheng Pan, Haoyu He, Yuetian Weng, and Chunhua Shen. A survey on efficient\\ntraining of transformers, 2023,arXiv preprint arXiv:2302.01107.URL http://arxiv.org/abs/2302.0\\n1107.\\nZirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and\\nXia Hu. Kivi : Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization.Arxiv,\\n2023. doi: 10.13140/RG.2.2.28167.37282. URLhttps://rgdoi.net/10.13140/RG.2.2.28167.37282.\\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Jianfeng Gao, and Tuo\\nZhao. Taming sparsely activated transformer with stochastic experts. In International Conference on\\nLearning Representations, 2022. URLhttps://openreview.net/forum?id=B72HXs80q4.\\n67')]\n",
      "type:<class 'list'> len:67\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "\n",
    "print(docs) \n",
    "print(f\"type:{type(docs)} len:{len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fbee6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Published in Transactions on Machine Learning Research (May/2024)\n",
      "Efficient Large Language Models: A Survey\n",
      "Zhongwei Wan∗ wan.512@osu.edu\n",
      "Xin Wang∗ wang.15980@osu.edu\n",
      "Che Liu† che.liu21@imperial.ac.uk\n",
      "Samiul Alam∗ alam.140@osu.edu\n",
      "Yu Zheng‡ zhengy30@msu.edu\n",
      "Jiachen Liu§ amberljc@umich.edu\n",
      "Zhongnan Qu¶‡‡ znqu@amazon.com\n",
      "Shen Yan∥ shenyan@google.com\n",
      "Yi Zhu†† yi@boson.ai\n",
      "Quanlu Zhang∗∗ quzha@microsoft.com\n",
      "Mosharaf Chowdhury§ mosharaf@umich.edu\n",
      "Mi Zhang∗ mizhang.1@osu.edu\n",
      "∗The Ohio State University †Imperial College London ‡Michigan State University §University of\n",
      "Michigan ¶Amazon AWS AI ∥Google Research ∗∗Microsoft Research Asia ††Boson AI\n",
      "Reviewed on OpenReview:https://openreview.net/forum?id=bsCCJHbO8A\n",
      "Abstract\n",
      "Large Language Models (LLMs) have demonstrated remarkable capabilities in important\n",
      "tasks such as natural language understanding and language generation, and thus have the\n",
      "potential to make a substantial impact on our society. Such capabilities, however, come with\n",
      "the considerable resources they demand, highlighting the strong need to develop effective\n",
      "techniques for addressing their efficiency challenges. In this survey, we provide a systematic\n",
      "and comprehensive review of efficient LLMs research. We organize the literature in a taxon-\n",
      "omy consisting of three main categories, covering distinct yet interconnected efficient LLMs\n",
      "topics from model-centric, data-centric, and framework-centric perspective, respectively. We\n",
      "have also created a GitHub repository where we organize the papers featured in this survey\n",
      "at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain\n",
      "the repository and incorporate new research as it emerges. We hope our survey can serve as\n",
      "a valuable resource to help researchers and practitioners gain a systematic understanding of\n",
      "efficient LLMs research and inspire them to contribute to this important and exciting field.\n",
      "‡‡The work is done outside Amazon.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fbd11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02de45d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\n",
    "    path=data_path,\n",
    "    glob='*.pdf',\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b553125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 0, 'page_label': '1'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient Large Language Models: A Survey\\nZhongwei Wan∗ wan.512@osu.edu\\nXin Wang∗ wang.15980@osu.edu\\nChe Liu† che.liu21@imperial.ac.uk\\nSamiul Alam∗ alam.140@osu.edu\\nYu Zheng‡ zhengy30@msu.edu\\nJiachen Liu§ amberljc@umich.edu\\nZhongnan Qu¶‡‡ znqu@amazon.com\\nShen Yan∥ shenyan@google.com\\nYi Zhu†† yi@boson.ai\\nQuanlu Zhang∗∗ quzha@microsoft.com\\nMosharaf Chowdhury§ mosharaf@umich.edu\\nMi Zhang∗ mizhang.1@osu.edu\\n∗The Ohio State University †Imperial College London ‡Michigan State University §University of\\nMichigan ¶Amazon AWS AI ∥Google Research ∗∗Microsoft Research Asia ††Boson AI\\nReviewed on OpenReview:https://openreview.net/forum?id=bsCCJHbO8A\\nAbstract\\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in important\\ntasks such as natural language understanding and language generation, and thus have the\\npotential to make a substantial impact on our society. Such capabilities, however, come with\\nthe considerable resources they demand, highlighting the strong need to develop effective\\ntechniques for addressing their efficiency challenges. In this survey, we provide a systematic\\nand comprehensive review of efficient LLMs research. We organize the literature in a taxon-\\nomy consisting of three main categories, covering distinct yet interconnected efficient LLMs\\ntopics from model-centric, data-centric, and framework-centric perspective, respectively. We\\nhave also created a GitHub repository where we organize the papers featured in this survey\\nat https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain\\nthe repository and incorporate new research as it emerges. We hope our survey can serve as\\na valuable resource to help researchers and practitioners gain a systematic understanding of\\nefficient LLMs research and inspire them to contribute to this important and exciting field.\\n‡‡The work is done outside Amazon.\\n1'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 1, 'page_label': '2'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\n1 Introduction\\nLarge Language Models (LLMs) are a type of advanced AI models designed to understand and generate\\nhuman languages. Recently, we have witnessed a surge in LLMs including those developed by Open AI\\n(GPT-4 (Achiam et al., 2023) and GPT-3 (Brown et al., 2020)), Meta (LLaMA-3 (Meta, 2024), LLaMA-\\n2 (Touvron et al., 2023b), LLaMA-1 (Touvron et al., 2023a)), and Google (Gemini (Team & Google, 2023),\\nPaLM-2 (Anil et al., 2023), PaLM (Chowdhery et al., 2022), GLaM (Du et al., 2022)) as well as many other\\nmodels such as BLOOM (Scao et al., 2023), PanGu-∑ (Ren et al., 2023b), and GLM (Zeng et al., 2023).\\nThese models have demonstrated remarkable performance across a variety of tasks such as natural language\\nunderstanding (NLU), language generation, complex reasoning (Yang et al., 2024), and domain-specific tasks\\nrelated to biomedicine (He et al., 2023; Wan et al., 2023; 2022), law (Eliot, 2021) and code generation (Wei\\net al., 2022b; Chen et al., 2021b). Such performance breakthroughs can be attributed to their massive scales\\nin model sizes and volumes of training data, as they contain billions or even trillions of parameters while\\nbeing trained on a gigantic amount of data from diverse sources.\\nAlthough LLMs are leading the next wave of AI revolution, their remarkable capabilities come at substantial\\nresource demands (Achiam et al., 2023; Du et al., 2022; Chowdhery et al., 2022; Ren et al., 2023b). Figure 1\\nillustrates the relationship between model performance and model training time in terms of GPU hours for\\nLLaMA series, where the size of each circle is proportional to the number of model parameters. As shown,\\nalthough larger models are able to achieve better performance, the amounts of GPU hours used for training\\nthem grow exponentially as model sizes scale up. In addition to training, inference also contributes quite\\nsignificantly to the operational cost of LLMs. Figure 2 depicts the relationship between model performance\\nand inference throughput. Similarly, scaling up the model size enables better performance but comes at\\nthe cost of lower inference throughput (higher inference latency), presenting challenges for these models in\\nexpanding their reach to a broader customer base and diverse applications in a cost-effective way.\\nThe high resource demands of LLMs highlight the strong need to develop techniques to enhance the efficiency\\nof LLMs. As shown in Figure 2, compared to LLaMA-1-33B, Mistral-7B (Jiang et al., 2023a), which uses\\ngrouped-query attention and sliding window attention to speed up inference, achieves comparable perfor-\\nmance and much higher throughput. This superiority highlights the feasibility and significance of designing\\nefficiency techniques for LLMs.\\nThe overarching goal of this survey is to provide a holistic view of the technological advances in efficient\\nLLMs. AsillustratedinFigure3, weorganizetheliteratureinataxonomyconsistingofthreemaincategories,\\ncovering efficient LLMs topics frommodel-centric, data-centric, and framework-centric perspective,\\nrespectively. These three categories cover distinct yet interconnected research topics, collectively providing\\na systematic and comprehensive review of efficient LLMs research. Specifically,\\n• Model-Centric Methods: Model-centric methods focus on bothalgorithm-level and system-\\nlevel efficient techniques where the model itself is the focal point. With billions or even trillions\\nof parameters, LLMs exhibit distinct characteristics (Wei et al., 2022a) compared to smaller-scale\\nmodels, necessitating the development of new techniques to enhance their efficiency. In §2, we survey\\nefficient techniques that cover research directions related to model compression, efficient pre-training,\\nefficient fine-tuning, efficient inference, and efficient architecture design.\\n• Data-Centric Methods: In the realm of LLMs, the importance of data is as crucial as that of\\nthe model itself. Data-centric methods focus on the role of the quality and structure of data in\\nenhancing the efficiency of LLMs. In §3, we survey efficient techniques that cover research directions\\nrelated to data selection and prompt engineering.\\n• LLM Frameworks: The advent of LLMs necessitates the development of specialized frameworks\\nto efficiently handle their training, fine-tuning, inference, and serving. While mainstream AI frame-\\nworks such as TensorFlow and PyTorch provide the foundations, they lack built-in support for spe-\\ncific optimizations and features crucial for LLMs. In §4, we survey existing frameworks specifically\\ndesigned for efficient LLMs, covering their unique features, underlying libraries, and specializations.\\n2'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 2, 'page_label': '3'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\n60 62 64 66 68 70 72 74\\nPerformance (Commonsense Reasoning Score)\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n1.50\\n1.75\\nTraining Time (Million GPU hours)LLaMA-1-7B LLaMA-1-13B\\nLLaMA-1-33B\\nLLaMA-1-65B\\nLLaMA-2-7B\\nLLaMA-2-13B\\nLLaMA-2-34B\\nLLaMA-2-70B5B 10B 25B 50B 75B\\nNumber of model parameters\\nFigure 1: Illustration of model performance and model training time in GPU hours of LLaMA models at dif-\\nferent scales. The reported performance is the average score of several commonsense reasoning benchmarks.\\nThe training time is based on Nvidia A100 80GB GPU. The size of each circle corresponds to the number\\nof model parameters. The original data can be found in Touvron et al. (2023a;b).\\n30 35 40 45 50 55\\nHuggingFace Open LLM Leaderboard Score (%)\\n20\\n30\\n40\\n50\\n60\\n70\\n80Throughput (tokens/s)\\nLLaMA-1-33BOPT-30B\\nGPT-NeoX-20B\\nCodeGen-NL-16B\\nLLaMA-2-13B\\nLLaMA-1-13BOPT-13B\\nXGLM-7.5B\\nMistral-7B\\nCodeGen-NL-6B\\nLLaMA-2-7BLLaMA-1-7B\\nOPT-6.7B\\nCerebras-\\nGPT-6.7B\\nMPT-7B\\nPythia-6.9B\\nXGLM-4.5B\\nOPT-2.7B\\nCerebras-\\nGPT-2.7B\\nCerebras-\\nGPT-1.3B\\n5GB 10GB 50GB 80GB\\nMemory\\nFigure 2: Performance scorevs. inference throughput for various LLMs. The throughputs are measured on\\nNvidia A100 80GB GPU with 16-bit floating point quantization. The size of each circle corresponds to the\\nmemory footprint (in Gigabytes) of each model when running with a batch size of 1, prompt size of 256, and\\ngenerating 1000 tokens. The original data can be found in Ilyas Moutawwakil (2023).\\nIn addition to the survey, we have established aGitHub repositorywhere we compile the papers featured\\nin this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain it\\nand incorporate new research as it emerges.\\n3'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 3, 'page_label': '4'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient LLMs Methods\\nModel-Centric(§2)\\nModel Compression(§2.1)\\nQuantization Post-Training QuantizationWeight-Only Quantization\\nWeight-Activation Co-QuantizationQuantization-Aware Training\\nParameter PruningStructured Pruning\\nUnstructured PruningLow-Rank Approximation\\nKnowledge DistillationWhite-Box KD\\nBlack-Box KD\\nEfficient Pre-Training(§2.2)\\nMixed Precision Training\\nScaling Models\\nInitialization Techniques\\nTraining Optimizers\\nSystem-Level Pre-TrainingEfficiency Optimization\\nEfficient Fine-Tuning(§2.3)\\nParameter-EfficientFine-Tuning\\nLow-Rank Adaptation\\nAdapter-based Tuning\\nPrefix Tuning\\nPrompt TuningMemory-Efficient Fine-Tuning\\nEfficient Inference(§2.4)\\nAlgorithm-LevelInference Acceleration\\nSpeculative Decoding\\nKV-Cache Optimization\\nSystem-Level InferenceAcceleration\\nEfficient Architecture(§2.5)\\nEfficient Attention\\nSharing-based Attention\\nKernelization or Low-Rank\\nFixed Pattern Strategies\\nLearnable Pattern Strategies\\nHardware-Assisted Attention\\nMixture of Experts (MoE)\\nMoE-based LLMs\\nAlgorithm-Level MoE Optimization\\nSystem-Level MoE Optimization\\nLong Context LLMs\\nExtrapolation and Interpolation\\nRecurrent Structure\\nSegmentation and Sliding Window\\nMemory-Retrieval Augmentation\\nTransformer-AlternativeArchitectures\\nState Space Models\\nOther Sequential Models\\nData-Centric(§3)\\nData Selection(§3.1)\\nData Selection forEfficient Pre-Training\\nData Selection forEfficient Fine-Tuning\\nPrompt Engineering(§3.2)\\nFew-Shot Prompting\\nDemonstration OrganizationDemonstration Selection\\nDemonstration Ordering\\nTemplate FormattingInstruction Generation\\nMulti-Step ReasoningPrompt Compression\\nPrompt Generation\\nFrameworks(§4) DeepSpeed, Megatron, Colossal-AI, Nanotron, MegaBlocks, FairScale, Pax, Composer,OpenLLM, LLM Foundry, vLLM, TensorRT-LLM, TGI, RayLLM, MLC LLM, Sax, Mosec\\nFigure 3: Taxonomy of efficient large language models (LLMs) literature.\\nAlthough there are a few surveys on LLMs (Zhao et al., 2023a; Chang et al., 2024; Wang et al., 2023i;\\nKaddour et al., 2023), this survey provides a focused review and discussion on the literature related to the\\nefficiency aspect of LLMs. There are also surveys on efficient Transformers (Tay et al., 2022) and their\\ntraining methods (Zhuang et al., 2023). In contrast, this survey specifically focuses on efficiency techniques\\ndesigned for models of more than billions of parameters. We hope this survey together with the GitHub\\nrepository can help researchers and practitioners navigate through the literature and serve as a catalyst for\\ninspiring further research on efficient LLMs.\\n4'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 4, 'page_label': '5'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nModel Compression\\nQuantization\\nPost-Training Quantization\\nWeight-Only Quantization\\nLLM.int8() (Dettmers et al., 2022), GPTQ (Frantar et al., 2023),\\nOBQ (Frantar & Alistarh, 2022), QuIP (Chee et al., 2023),\\nAWQ (Lin et al., 2023), OWQ (Lee et al., 2023),\\nSpQR (Dettmers et al., 2024), FineQuant (Kim et al., 2023d)\\nWeight-Activation Co-Quantization\\nZeroQuant (Yao et al., 2022b), ZeroQuant-FP (Wu et al., 2023b),\\nZeroQuant-V2 (Yao et al., 2023d), SmoothQuant (Xiao et al., 2023),\\nOliVe (Guo et al., 2023), RPTQ (Yuan et al., 2023a),\\nAhmadian et al. (2023), Outlier Suppression+ (Wei et al., 2023),\\nQLLM (Liu et al., 2024c)\\nQuantization-Aware TrainingQuantGPT (Tao et al., 2022), LLM-QAT (Liu et al., 2023d), BitNet (Wang et al., 2023b)\\nParameter Pruning\\nStructured PruningLLM-Pruner (Ma et al., 2023), Sheared LLaMA (Xia et al., 2023), LoRAPrune (Zhang et al., 2023c),\\nLoRAShear (Chen et al., 2023e), Deja Vu (Liu et al., 2023f)\\nUnstructured PruningSparseGPT (Frantar & Alistarh, 2023), Wanda (Sun et al., 2024), Shao et al. (2024)\\nLow-Rank ApproximationTensorGPT (Xu et al., 2023a), LoSparse (Li et al., 2023g), FWSVD (Hsu et al., 2022), ASVD (Yuan et al., 2023b), SVD-LLM (Wang et al., 2024c)\\nKnowledge Distillation\\nWhite-Box KDBaby LLaMA (Timiryasov & Tastet, 2023), MiniLLM (Gu et al., 2024), KPTD (Padmanabhan et al., 2023),\\nTED (Liang et al., 2023), TSLD (Kim et al., 2023b), MiniMA (Zhang et al., 2023a), GKD (Agarwal et al., 2024)\\nBlack-Box KD\\nMetaICL (Min et al., 2022a), Multitask-ICT (Huang et al., 2022), Li et al. (2024b), Lion (Jiang et al., 2023b),\\nDISCO (Chen et al., 2023j), Fu et al. (2023b), Distilling Step-by-Step (Hsieh et al., 2023),\\nFine-tune-CoT (Ho et al., 2023), SOCRATIC CoT (Shridhar et al., 2023), SCOTT (Wang et al., 2023c),\\nSCoTD (Li et al., 2023b), Peng et al. (2023a), Zephyr (Tunstall et al., 2023)\\nFigure 4: Summary of model compression techniques for LLMs.\\n2 Model-Centric Methods\\n2.1 Model Compression\\nModel compression enhances efficiency by reducing the sizes and the amount of arithmetic operations of\\nLLMs. Unlike conventional model compression techniques, most LLM compression approaches are designed\\nunder the post-training setting to avoid resource-intensive retraining. As summarized in Figure 4, model\\ncompression techniques for LLMs can be grouped into four categories: quantization, parameter pruning,\\nlow-rank approximation, and knowledge distillation. These four categories are orthogonal to each other, and\\ncompress LLMs from different perspectives.\\n2.1.1 Quantization\\nQuantization compresses LLMs by converting model weights and/or activations of high-precision data types\\nXH such as 32-bit floating point into low-precision data typesXL such as 8-bit integer (Dettmers et al.,\\n2023) as:\\nXL = Round\\n(\\nabsmax\\n(\\nXL)\\nabsmax (XH) XH\\n)\\n= Round\\n(\\nK·XH)\\n, (1)\\nwhere Round denotes mapping a floating point number into an approximate integer;absmax denotes the\\nabsolute maximum of the input elements; andKdenotes the quantization constant. Quantization techniques\\nfor LLMs can be classified into post-training quantization (PTQ) and quantization-aware training (QAT).\\nCompared to other LLM compression methods such as parameter pruning and low-rank approximation,\\nquantization methods have been shown to achieve superior compression-accuracy trade-offs (Li et al., 2024c).\\nPost-Training Quantization (PTQ).PTQ quantizes LLMs after the model has been trained. To com-\\npensate for the accuracy drop, PTQ uses a small calibration dataset to update the quantized weights and/or\\nactivations. PTQ in general can be grouped into two categories: weight-only quantization, and weight-\\nactivation co-quantization.\\n• Weight-Only Quantization focuses on quantizing model weights only. For example, Dettmers\\net al. (2022) introduce the first multi-billion-scale 8-bit integers (or INT8) weight quantization\\nmethod named LLM.int8() that significantly reduces memory usage during inference while being able\\n5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 5, 'page_label': '6'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTeacher Model\\n(Transparent)\\xa0 Student Model\\xa0\\nWhite-\\nBox KD\\nBlack-\\nBox KD\\nTraining Data Training Data\\nHigh-Precision Weight\\nwith Different Values\\nLow-Precision Weight\\nwith Different Values\\nZero Weight\\xa0 /\\nActivation Value\\nLow-Precision Activation\\nwith Different Values\\nupdate \\nTraining\\nData\\nQAT\\nPTQ (optional) \\n(a) Quantization\\nStructured Unstructured\\n(b) Parameter Pruning\\nX \\nV T \\nDecompose \\nU \\n(c) Low-Rank Approximation\\n (d) Knowledge Distillation\\n+ \\n+ \\nHigh-Precision Activation\\nwith Different Values\\nCalibration\\nData\\nupdate \\nupdate \\nCalibration\\nData\\nupdate \\n+ \\nTeacher Model\\n(Hidden)\\xa0\\nFigure 5: Illustrations of model compression techniques for LLMs.\\nto maintain the performance of the full-precision model. Frantar et al. (2023) push one step further\\nand propose GPTQ, a post-training weight quantization method that compresses LLM weights to\\n3 or 4 bits instead of 8 bits. GPTQ employs layer-wise quantization with Optimal Brain Quanti-\\nzation (OBQ) (Frantar & Alistarh, 2022) to update weights with inverse Hessian information. This\\ntechnique enables quantizing GPT models with 175 billion parameters in roughly four GPU hours\\nwith minimal accuracy drop compared to the original model. Furthermore, driven by the insights\\nthat quantization can be more effective when model weights and proxy Hessian matrices are incoher-\\nent, Chee et al. (2023) propose QuIP, a post-training quantization method that applies incoherence\\nprocessing to quantize LLMs to 2 bits per weight. As another line of research under weight-only\\nquantization, Lin et al. (2023) observe that there exists a small subset of model weights, character-\\nized by larger activation magnitudes, known as salient weights, play a crucial role in determining\\nthe quantization loss. Based on this observation, they propose an approach named activation-aware\\nweight quantization (AWQ) to quantize LLMs while preserving the salient weights in high pre-\\ncision, demonstrating superior performance over GPTQ. Similarly, Lee et al. (2023) observe that\\nactivation outliers amplify weight quantization loss. They propose outlier-aware weight quantiza-\\ntion (OWQ) to identify those vulnerable weights with activation outliers and allocate high-precision\\nto them. Dettmers et al. (2024) introduce Sparse-Quantized Representation (SpQR) to separate\\noutlier weights that are prone to large quantization errors. These outlier weights are preserved at\\nhigher precision, while the remaining weights are compressed to 3-4 bits. Additionally, they intro-\\nduce a decoding scheme tailored for the SpQR format that enhances the efficiency of inference on\\na token-by-token basis. Lastly, Kim et al. (2023d) aim to address the issue of outliers that distort\\nthe distribution of quantized weights, and propose FineQuant that employs an empirically crafted,\\nheuristic-based approach to allocate varying levels of granularity to different weight matrices.\\n6'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 6, 'page_label': '7'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\n• Weight-Activation Co-Quantizationdiffers from weight-only quantization in the sense that it\\nquantizes both model weights and activations. For instance, Yao et al. (2022b) propose ZeroQuant,\\nwhich combines group-wise quantization for model weights and token-wise quantization for activa-\\ntions. However, ZeroQuant falls short in maintaining accuracy for models with more than 175 billion\\nparameters. To address this issue, Yao et al. (2023d) and Wu et al. (2023b) propose ZeroQuant-FP\\nand ZeroQuant-V2 respectively, both of which utilize low-rank matrices to recover the accuracy\\ndrop. A key challenge of weight-activation co-quantization is that due to the existence of outliers,\\nactivations are more difficult to quantize than model weights (Bondarenko et al., 2021). To address\\nthis challenge, Xiao et al. (2023) propose SmoothQuant which introduces a per-channel scaling trans-\\nformation that migrates the quantization difficulty from activations to weights to achieve lossless\\nquantization of weights and activations to 8 bits for LLMs up to 530 billion parameters. Guo\\net al. (2023) pinpoint that outliers are critical in weight-activation co-quantization but their nearby\\nnormal values are not. Given that, they propose OliVe, which prunes normal values adjacent to\\nthe outliers so that the outliers can be encoded with higher precision. Yuan et al. (2023a) iden-\\ntify the challenge of quantizing activations when different channels have disparate ranges. They\\npropose RPTQ, which groups channels in activations that have similar value ranges and applies\\nuniform quantization parameters to the values in each group. Ahmadian et al. (2023) demonstrate\\nthat it is possible to suppress large activation outliers at scales as large as 52B. Given the right\\noptimization choices during pre-training, they can quantize models ranging in size from 410M to\\n52B with minimal accuracy degradation. Wei et al. (2023) observe that the activation outliers in\\nLLMs are asymmetric and tend to cluster in particular channels. Based on this observation, they\\npropose Outlier Suppression+, which introduces operations that shift and scale channels individually\\nto neutralize asymmetric outliers. Lastly, Liu et al. (2024c) propose QLLM, an adaptive channel\\nreassembly method that tackles activation outliers and utilizes calibration data to offset the in-\\nformation loss incurred from quantization. Experimental result shows that QLLM achieves better\\ncompression performance than SmoothQuant and Outlier Suppression+ on LLaMA model family.\\nQuantization-Aware Training (QAT).Different from PTQ, QAT quantizes LLMs during the training\\nprocess, allowing LLMs to learn quantization-friendly representations. Since QAT requires training using\\nthe complete training dataset, it is much more expensive and time consuming than PTQ. Tao et al. (2022)\\npropose QuantGPT, which combines contrastive distillation from a full-precision teacher model and logit\\ndistillation to a quantized student model during autoregressive pretraining. QuantGPT achieves 14.4× and\\n13.4× compression rates on GPT-2 and BART with comparable performance with the full-precision models.\\nLLM-QAT (Liu et al., 2023d) uses data generated by LLMs itself to distill knowledge with the objective of\\nquantizing a student model. Specifically, LLM-QAT retains the original output distribution and is capable of\\nquantizing a model irrespective of its initial training data. Besides quantizing weights and activations, LLM-\\nQAT also quantizes the key-value cache, a crucial step for enhancing throughput and accommodating long\\nsequence dependencies in LLMs. Experimental results show that LLM-QAT achieves better performance\\nover training-free methods especially in low-bit settings. Lastly, BitNet (Wang et al., 2023b) pioneers QAT\\nfor 1-bit LLMs. It proposes to use low-precision binary weights and quantized activations while keeping\\noptimizer states and gradients high-precision during training. Experimental results show that compared to\\nFP16 Transformer baselines, BitNet is able to achieve competitive performance while substantially reducing\\nmemory footprint and energy consumption.\\n2.1.2 Parameter Pruning\\nParameter pruning compresses LLMs by removing redundant or less important model weights. Parameter\\npruning methods for LLMs can be categorized into structured pruning and unstructured pruning.\\nStructured Pruning.Structured pruning focuses on pruning structured patterns such as groups of consec-\\nutive parameters or hierarchical structures such as rows, columns, or sub-blocks of the LLM weight matrices.\\nFor instance, LLM-Pruner (Ma et al., 2023) introduces a task-agnostic structured pruning strategy that se-\\nlectively eliminates non-essential interconnected structures using gradient information. LLM-Pruner utilizes\\na small amount of data to obtain the weight, parameter, and group importance of the coupled structure\\nfor LLaMA (Touvron et al., 2023a), and uses LoRA (Hu et al., 2022) to recover accuracy after pruning,\\n7'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 7, 'page_label': '8'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nshowing competitive zero-shot performance. Sheared LLaMA (Xia et al., 2023), on the other hand, proposes\\ntwo techniques to improve the performance of LLM-Pruner. The first technique, named targeted struc-\\ntured pruning, prunes a larger model to a designated target shape by eliminating layers, heads, intermediate\\nand hidden dimensions in an end-to-end manner. The second technique, named dynamic batch loading,\\ndynamically configures the composition of sampled data in each training batch based on losses in various\\ndomains. Through these two techniques, Sheared LLaMA is able to prune LLaMA2-7B down to 1.3B pa-\\nrameters, achieving superior compression ratio compared to LLM-Pruner. LoRAPrune (Zhang et al., 2023c)\\nintroduces a LoRA-based pruning criterion using LoRA’s weights and gradients for importance estimation.\\nBy employing an iterative structure pruning process to eliminate excess channels and heads, LoRAPrune\\nachieves better efficiency over LLM-Pruner at 50% compression rate. Lastly, given the input, Deja Vu (Liu\\net al., 2023f) predicts a small set of attention heads and MLP parameters, referred to as contextual sparsity,\\nyields approximately the same output as the dense model. By exploiting such contextual sparsity, Deja Vu\\nis able to achieve much lower latency compared to FasterTransformer without accuracy drop.\\nUnstructured Pruning.Unstructured pruning, on the other hand, focuses on pruning model weights indi-\\nvidually. Compared to structured pruning, unstructured pruning has much more pruning flexibility and thus\\nenjoys a lower accuracy drop. However, unstructured pruning incurs irregular sparsification, which in general\\nmakes the resulting pruned models difficult to be deployed on hardware except specific types of hardware\\nsuch as Nvidia Ampere GPUs (Busato & Pool, 2020). For instance, Frantar & Alistarh (2023) introduce\\nSparseGPT, an one-shot LLM unstructured pruning approach that does not require retraining. SparseGPT\\nformulates pruning as a sparse regression problem and solves it by utilizing an approximate solver based\\non the inversion of the Hessian matrix. In doing so, SparseGPT reaches about 60% unstructured sparsity\\non models such as OPT-135B while experiencing only a slight performance drop. Sun et al. (2024) propose\\nWanda, which prunes weights based on the product values of weight magnitudes and their respective input\\nactivations. Compared to SparseGPT, Wanda neither relies on second-order information nor necessitates\\nweight update, and is able to achieve competitive performance. Shao et al. (2024) improve the performance\\nof SparseGPT in another way. Specifically, instead of performing the unstructured pruning with a unified\\nratio for every layer, they propose to utilize Hessian sensitivity-aware mixed sparsity pruning to achieve a\\nminimum of 50% sparsity in LLMs without retraining. This method adaptively assigns sparsity based on\\nsensitivity to minimize the error induced by pruning while preserving the overall level of sparsity.\\n2.1.3 Low-Rank Approximation\\nLow-rank approximation compresses LLMs by approximating the LLM weight matrixWm×n with smaller\\nlow-rank matricesU and V such thatW ≈UV⊤, where U ∈Rm×r, V ∈Rn×r, and r is typically much\\nsmaller than m,n. In doing so, low-rank approximation reduces the number of parameters and enhances\\nefficiency. For example, Xu et al. (2023a) introduce TensorGPT which compresses the embedding layers of\\nLLMs using Tensor-Train Decomposition (TTD). It transforms and breaks down each token embedding and\\ncreates an efficient embedding format named Matrix Product State (MPS) that can be efficiently computed\\nin a distributed manner. LoSparse (Li et al., 2023g) improves the performance of TensorGPT by compressing\\nthe coherent and expressive components within neurons through low-rank approximation while eliminating\\nthe incoherent and non-expressive elements through pruning. As another line of research, FWSVD (Hsu\\net al., 2022) compresses the weight matrix of an LLM instead of the token embedding matrix via low-\\nrank approximation. Specifically, instead of using vanilla singular value decomposition (SVD), FWSVD\\nproposes a weighted SVD approach which uses Fisher information to weigh the importance of the weights for\\ncompression. While FWSVD demonstrates competitive compression results under low compression ratios, it\\nrequires calculating the gradients based on the training dataset of the target task to estimate the importance\\nscores, which is task-specific and demands significant computation resources. In contrast, ASVD (Yuan\\net al., 2023b) proposes a training-free SVD-based approach. It scales the weight matrix based on the\\nactivation distribution that enhances the decomposition accuracy and efficiency for model compression.\\nHowever, neither FWSVD nor ASVD directly correlate singular values with compression loss. Consequently,\\ntruncating the smaller singular values might result in increased compression loss. SVD-LLM (Wang et al.,\\n2024c) addresses this drawback by incorporating a truncation-aware data whitening strategy that establishes\\na direct mapping between singular values and compression loss. Experimental results demonstrate the\\nsuperiority of SVD-LLM over FWSVD and ASVD in terms of compression performance and speed.\\n8'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 8, 'page_label': '9'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\n2.1.4 Knowledge Distillation\\nKnowledge Distillation (KD) compresses LLMs by transferring knowledge from a large teacher LLM to a\\nsmaller student LLM. Though effective, compared to other LLM compression methods, knowledge distillation\\nmethods incur a resource-demanding distillation process. In general, KD for LLMs can be categorized into\\nwhite-box KD methods and black-box KD methods.\\nWhite-Box Knowledge Distillation. White-box KD refers to KD techniques where the parameters\\nor logits of the teacher LLM are used in the distillation process (Gou et al., 2021). For example, as a\\npioneering effort in this direction, Baby LLaMA (Timiryasov & Tastet, 2023) trains an ensemble of GPT-2\\nand a collection of smaller LLaMA models using the BabyLM dataset of 10M words. This ensemble is\\nthen distilled into a compact LLaMA model with 58 million parameters, which outperforms both its original\\nteachermodelsaswellasacomparablemodelthatwastrainedwithouttheuseofdistillation. Guetal.(2024)\\nobserve that conventional KD objectives, such as Kullback-Leibler divergence (KLD), may not be suited for\\nopen text generation tasks due to their complex output spaces compared to classification tasks. To address\\nthis issue, they propose MiniLLM which minimizes reverse KLD using the gradient of the objective function\\nthrough policy gradient techniques (Sutton et al., 1999). Experimental results show that MiniLLM achieves\\nbetter accuracy than conventional KD or directly fine-tuning student models. KPTD (Padmanabhan et al.,\\n2023) demonstrates that white-box KD can transfer and disseminate knowledge from entity definitions into\\nthe parameters of a pre-trained language model. Specifically, KPTD creates a transfer set by prompting\\nthe language model to generate text based on the definition of the entity. The model parameters are then\\nupdated to align the distribution of the student model with that of the teacher model. TED (Liang et al.,\\n2023) introduces a technique for layer-specific task distillation. It uses specially designed filters to align the\\ninternal states of both student and teacher models in each layer. These filters extract relevant knowledge\\nfrom the internal states that is beneficial for the specific task. TED shows considerable and steady gains in\\nperformance on both continual pre-training and fine-tuning. TSLD (Kim et al., 2023b) leverages token-level\\ndistillation to enhance QAT. It addresses the limitations of layer-to-layer KD in token prediction recovery\\nby reforming intermediate representation and has successfully applied QAT to LLMs. MiniMA (Zhang\\net al., 2023a) proposes a viewport towards the capacity gap in distilling LLMs, converting it into a principle\\nthrough analysis and introducing a 3B Language Model that sets a new benchmark for compute-performance\\npareto frontier. Experimental results show that MiniMA achieves the best accuracy compared to other 3B\\ndistilled LLMs. Lastly, Generalized knowledge distillation (GKD) (Agarwal et al., 2024) addresses the issue\\nof distribution mismatch by drawing output sequences from the student model during training. GKD can\\nbe applied in combination with other distillation methods to improve their compression performance.\\nBlack-Box Knowledge Distillation.Different from white-box KD, in black-box KD, only the outputs\\ngenerated from the teacher LLM are used in the distillation process. Inspired by ICT (Chen et al., 2022c) and\\nMetaICL (Min et al., 2022a), where the language model is meta-trained under a wide range of tasks using\\nin-context learning objectives and then fine-tuned for unseen tasks through in-context learning, Multitask-\\nICT (Huang et al., 2022) introduces a concept known as in-context learning distillation to transfer the\\nfew-shot learning capabilities from the teacher model to the student model. Experimental results show\\nthat under Multitask-ICT, in-context learning objectives achieve the best performance when combined with\\nlanguage modeling objectives. Similarly, Li et al. (2024b) introduce a hybrid prompting technique that\\nemploys multi-task learning along with explanations generated by GPT-3 text-davinci-002 version (OpenAI,\\n2023). This method is used to distill explanations into smaller models, achieving consistent and significant\\nimprovements over strong single-task fine-tuning benchmarks in different scenarios. Experiments on multiple\\nreasoning tasks show that this method even perform better than finetuning or prompting a 60x larger\\nGPT-3 (175B) model by up to 9.5% in accuracy. Lion (Jiang et al., 2023b) introduces an adversarial\\ndistillation architecture aimed at enhancing the efficiency of knowledge transfer by incrementally improving\\nthe skill level of the student model. Specifically, it prompts LLMs to recognize challenging instructions\\nand creates new complex instructions for the student model, thereby establishing a three-phase adversarial\\ncycle involving imitation, discrimination, and generation. Experimental results show that Lion-13B not only\\nachieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional instruction-\\ntuned models. DISCO (Chen et al., 2023j) prompts a general LLM to produce phrasal perturbations. These\\ngenerated perturbations are then filtered by a specialized teacher model to distill high-quality counterfactual\\n9'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 9, 'page_label': '10'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTable 1: Pre-training costs of representative LLMs.\\nModel Parameter SizeData ScaleGPUs CostTraining Time\\nGPT-3 (Brown et al., 2020) 175B 300B tokens - -\\nGPT-NeoX-20B (Black et al., 2022)20B 825GB corpus96 A100-40G -\\nOPT (Zhang et al., 2022a) 175B 180B tokens992 A100-80G -\\nBLOOM (Scao et al., 2023) 176B 366B tokens384 A100-80G 105 days\\nGLM (Zeng et al., 2023) 130B 400B tokens786 A100-40G 60 days\\nLLaMA (Touvron et al., 2023a)65B 1.4T tokens2048 A100-80G 21 days\\nLLaMA-2 (Touvron et al., 2023b)70B 2T tokens A100-80G 71,680 GPU days\\nGopher (Rae et al., 2022) 280B 300B tokens 1024 A100 13.4 days\\nLaMDA (Thoppilan et al., 2022)137B 768B tokens 1024 TPU-v3 57.7 days\\nGLaM (Du et al., 2022) 1200B 280B tokens 1024 TPU-v4 574 hours\\nPanGu-α(Zeng et al., 2021) 13B 1.1TB corpus2048 Ascend 910 -\\nPanGu-∑(Ren et al., 2023b) 1085B 329B tokens512 Ascend 910 100 days\\nPaLM (Chowdhery et al., 2022)540B 780B tokens 6144 TPU-v4 -\\nPaLM-2 (Anil et al., 2023) - 3.6T tokens TPUv4 -\\nWeLM (Su et al., 2023a) 10B 300B tokens128 A100-40G 24 days\\nFlan-PaLM (Chung et al., 2022)540B - 512 TPU-v4 37 hours\\nAlexaTM (Soltan et al., 2022)20B 1.3 tokens 128 A100 120 days\\nCodegeex (Zheng et al., 2023)13B 850 tokens1536 Ascend 910 60 days\\nMPT-7B (Team, 2023) 7B 1T tokens - -\\ndata into smaller student models, allowing the smaller models to learn causal representations more reliably.\\nAs another line of research, some studies have shown that chain-of-thought (CoT) prompting can elicit\\nlanguage models to solve complex reasoning tasks step by step, with the aim to transfer such ability from\\nlarge models into smaller ones through black-box KD. For example, to enhance the CoT math reasoning\\ncapabilities of smaller models, Fu et al. (2023b) propose a method for instruct-tuning a student model\\n(FlanT5) by distilling the reasoning pathways found in the GSM8K dataset from a teacher model (GPT-\\n3.5 code-davinci-002 (Chen et al., 2021b)). Fine-tuning and distilling smaller models require substantial\\namounts of training data to match the performance of the large model. To address this issue, Hsieh et al.\\n(2023) propose Distilling Step-by-Step, a technique that uses CoT prompting to extract LLM rationales\\nfor extra guidance in training smaller models within a multi-task setting. Experimental results show that\\nDistilling Step-by-Step achieves better performance with much fewer labeled or unlabeled training examples\\ncompared to both fine-tuning and standard distillation. Fine-tune-CoT (Ho et al., 2023) utilizes existing\\nzero-shot CoT prompting techniques (Kojima et al., 2022) to create rationales from LLMs. These rationales\\nare then used to fine-tune smaller student models. It also introduces diverse reasoning, a method that\\nemploys stochastic sampling to generate a variety of reasoning solutions from teacher models, which serves\\nto enrich the training data for the student models. SOCRATIC CoT (Shridhar et al., 2023) breaks down\\nthe original problem into a series of smaller sub-problems and utilizes this decomposition to direct the\\nintermediate steps of reasoning. It is used to train a pair of smaller, distilled models: one specializes in\\ndissecting the problem and the other focuses on solving these sub-problems. SOCRATIC COT is shown to\\nbe an effective alternative to CoT, enabling a much smaller model (GPT-2 large) to outperform a 10x larger\\nmodel (GPT-3 6B). SCOTT (Wang et al., 2023c) uses rationales generated by LLMs to train a student\\nmodel under a counterfactual reasoning framework. It ensures that the student model does not overlook the\\nprovided rationales, thereby preventing it from making inconsistent predictions. Experimental results show\\nthat SCOTT can generate CoT rationales that are more faithful than original CoT prompting. Li et al.\\n(2023b) present a method called symbolic CoT distillation (SCoTD) that draws CoT rationales from a LLM\\nusing unlabeled data instances. A smaller model is then trained to predict both the sampled rationales and\\nthe associated labels. Lastly, Peng et al. (2023a) utilize GPT-4 as a teacher model to generate English and\\nChinese instruction-based datasets to refine student LLMs. They show that the 52K data points generated\\nby GPT-4 are able to improve zero-shot performance compared to instruction-following data generated from\\nprevious state-of-the-art models.\\n10'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 10, 'page_label': '11'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient Pre-Training\\nMixed Precision TrainingAMP (Micikevicius et al., 2018; Facebook AI Research (FAIR), 2023; Rae et al., 2022),\\nBrain Floating Point (BFLOAT16) (Kalamkar et al., 2019; Burgess et al., 2019)\\nScaling Models\\nProgressive Stacking (Gong et al., 2019), MSLT (Yang et al., 2020), CompoundGrow (Gu et al., 2021), bert2BERT (Chen et al., 2022b),\\nKnowledge Inheritance (Qin et al., 2022), Staged Training (Shen et al., 2022), LiGO (Wang et al., 2023d), Mango (Pan et al., 2023),\\nYao et al. (2024), Growth Strategy (Li et al., 2023e)\\nInitialization TechniquesKumar (2017), Fixup (Zhang et al., 2019), ZerO (Zhao et al., 2022), SkipInit (De & Smith, 2020),\\nReZero (Bachlechner et al., 2021), T-Fixup (Huang et al., 2020), DeepNet (Wang et al., 2024a)\\nTraining OptimizersLion (Chen et al., 2023g), Sophia (Liu et al., 2024a)\\nSystem-Level Pre-Training\\nEfficiency Optimization\\nZeRO (Rajbhandari et al., 2020), FSDP (Zhao et al., 2023c), ZeRO-Offload (Ren et al., 2021), ZeRO-Infinity (Rajbhandari et al., 2021),\\nZeus (You et al., 2023), Perseus (Chung et al., 2023)\\nFigure 6: Summary of efficient pre-training techniques for LLMs.\\nUpdate Backward\\nTraining Data New Layer\\nOld Layer\\nProgressive Update\\nForward Forward\\nHigh-Precision Weight / Gradient\\n0 0 0\\n1 1 1\\nTraining\\nTraining Data\\nInitial Model Weight:\\nGradient:\\nEfficient \\nOptimizer \\n(a)\\xa0Mixed Precision Training (b)\\xa0Scaling Models \\n(c)\\xa0Initialization\\xa0Techniques (d)\\xa0Training Optimizers \\nLow-Precision Weight / Activation / Gradient\\nActivation:\\nConvert\\nFigure 7: Illustrations of efficient pre-training techniques for LLMs.\\n2.2 Efficient Pre-Training\\nAs shown in Table 1, pre-training LLMs incurs significant costs. Efficient pre-training techniques focus on\\nreducing the costs of the LLM pre-training process in terms of compute resources, training time, memory and\\nenergy consumption. As summarized in Figure 6, enhancing the efficiency of pre-training can be achieved\\nthrough different and complementary techniques, including mixed precision acceleration, scaling models,\\ninitialization techniques, training optimizers, and system-level pre-training efficiency optimization.\\nMixed Precision Training. Mixed precision training enhances pre-training efficiency by using low-\\nprecision models for forward and backward propagation and then converting the calculated low-precision\\ngradients to high-precision ones for updating the original high-precision weights. For example, Micikevicius\\net al. (2018) propose Automatic Mixed Precision (AMP) to keep a master copy of weights in full-precision\\n(FP32) for updates, whereas weights, activations, and gradients are stored in FP16 for arithmetic operations.\\nNotably, the improved version of AMP (Facebook AI Research (FAIR), 2023) has eliminated the copy of\\nFP32 weights, but the optimizer (AdamW) still uses FP32 internally. Meanwhile, Rae et al. (2022) demon-\\nstrate that FP16 in AMP results in accuracy loss due to the restricted numerical range. To address this\\nissue, Brain Floating Point (BFLOAT16), which has a greater dynamic range — i.e., number of exponent\\nbits — than FP16, was proposed (Kalamkar et al., 2019; Burgess et al., 2019) to achieve better training\\nperformance.\\n11'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 11, 'page_label': '12'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nScaling Models.Techniques based on scaling models accelerate pre-training convergence and reduce train-\\ning costs by leveraging the weights of a smaller model to upscale to a larger one. For example, Gong et al.\\n(2019) introduce a technique named progressive stacking to transfer knowledge from a simpler model to a\\nmore complex one to enhance model training efficiency. Meanwhile, Yang et al. (2020) observe that as the\\ndepth of the model increases through progressive stacking, the training speed however decreases. To address\\nthis issue, they propose multi-stage layer training (MSLT), which only updates the output and newly intro-\\nduced top encoder layers while keeping the previously trained layers unchanged. Once all the layers have\\nbeen trained, MSLT fine-tunes the entire model by updating each layer with 20% of the total steps, making\\nit more time-efficient than the traditional progressive stacking approach. Similarly, Gu et al. (2021) intro-\\nduce CompoundGrow, which begins with training a small model and then incrementally expands it using\\na mix of model growth techniques, including increasing input length, model breadth and depth, leading to\\nan acceleration in the pre-training process in wall-clock time compared to progressive stacking. Chen et al.\\n(2022b) propose bert2BERT, which applies function-preserving initialization (FPI) and advanced knowledge\\ninitialization (AKI) to transfer the knowledge of a smaller pre-trained model to a large model to improve the\\npre-training efficiency of the large model. Specifically, FPI enforces the initialized larger model to closely\\nmirror the behavior of the smaller model, laying a strong basis for later optimization; and AKI promotes\\nfaster convergence by replicating weights from higher layers. Experimental results show that bert2BERT is\\nable to save a significant amount of training cost over MSLT. Qin et al. (2022) propose Knowledge Inheri-\\ntance which employs knowledge distillation as an auxiliary supervision during pre-training. This facilitates\\ntraining a larger model from a smaller teacher model, thereby enhancing both the pre-training speed and\\nthe generalization ability. Shen et al. (2022) introduce Staged Training that begins with a small model and\\nprogressively increases its depth and breadth through a growth operator. By starting each stage with the\\nresults from the previous one, it effectively reuses computation, leading to a more efficient training process\\ncompared to previous techniques like CompoundGrow and progressive stacking. Wang et al. (2023d) propose\\nLinear Growth Operator (LiGO) that linearly maps the parameters of a smaller model to initiate a larger one.\\nBy using a composition of width-and depth-growth operators further enhanced with Kronecker factorization\\nto capture architectural knowledge, LiGO outperforms bert2BERT which saves about 30% computational\\ncosts. Pan et al. (2023) introduce a technique named Mango which establishes a linear relationship between\\neach weight of the target model and all weights of the pretrained model to boost acceleration capabilities.\\nIt also employs multi-linear operators to decrease computational and spatial complexity during pre-training,\\nachieving 59.9% acceleration ratio compared to Chen et al. (2022b) and LiGO. Drawing from these scaling\\ntechniques and the progressive pre-training (Yao et al., 2024), recent LLMs like FLM-101B (Li et al., 2023e)\\nintroduce a growth strategy to cut LLM training costs by expanding model structures offline and resuming\\nfrom the previous stage’s smaller model checkpoint.\\nInitialization Techniques.Initialization plays a key role in enhancing the efficiency of LLM pre-training\\nbecause a good initialization can accelerate the convergence of the model. Most LLMs employ initialization\\ntechniques that were adopted in training smaller-scale models. For example, initialization method introduced\\nby Kumar (2017) balances input and output variances. Fixup (Zhang et al., 2019) and ZerO (Zhao et al.,\\n2022) set the backbone to zero, preserving signal identity. SkipInit (De & Smith, 2020) substitutes batch\\nnormalization with a zero-value multiplier. ReZero (Bachlechner et al., 2021) adds zero-valued parameters\\nto maintain identity which leads to faster convergence. T-Fixup (Huang et al., 2020) follows Fixup to adopt\\nrescaling schemes for the initialization of the residual blocks of Transformer models. DeepNet (Wang et al.,\\n2024a) adjusts the residual connection in deep Transformers using Post-LN-init, ensuring stable inputs to\\nlayer normalization and mitigating gradient vanishing for stable optimization.\\nTraining Optimizers. Popular LLMs such as GPT-3 (Brown et al., 2020), OPT (Zhang et al., 2022a),\\nBLOOM (Scao et al., 2023), and Chinchilla (Hoffmann et al., 2022) are predominately pre-trained using\\nAdam (Kingma & Ba, 2017) or AdamW (Loshchilov & Hutter, 2019) as optimizers. However, both Adam\\nand AdamW are memory hungry and computationally expensive. Some studies (Chen et al., 2023g; Liu et al.,\\n2024a) propose new optimizers to accelerate LLM pre-training. Specifically, Chen et al. (2023g) propose to\\nleverage search techniques to traverse a large and sparse program space to discover optimizers for model\\ntraining. The discovered optimizer, named Lion (EvoLved Sign Momentum), is more memory-efficient than\\nAdam as it only keeps track of the momentum. Liu et al. (2024a), on the other hand, propose Sophia\\nas a lightweight second-order optimizer that outpaces Adam with doubling the pre-training speed. Sophia\\n12'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 12, 'page_label': '13'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient Fine-Tuning\\nParameter-Efficient Fine-Tuning\\nLow-Rank Adaptation\\nLoRA (Hu et al., 2022), LoRA-FA (Zhang et al., 2023b),\\nLoraHub (Huang et al., 2023), LongLoRA (Chen et al., 2023i),\\nMulti-Head Routing (Caccia et al., 2023), AdaLoRA (Zhang et al., 2023d),\\nDyLoRA (Valipour et al., 2023), CEPT (Zhao et al., 2023b),\\nTied-LoRA (Renduchintala et al., 2023)\\nAdapter-based Tuning\\nLLM-Adapters (Hu et al., 2023b), Compacter (Karimi Mahabadi et al., 2021),\\n(IA)3(Liu et al., 2022a), Meta-Adapters (Bansal et al., 2022),\\nAdaMix (Wang et al., 2022c), OpenDelta (Hu et al., 2023a),\\nSparseAdapter (He et al., 2022b)\\nPrefix TuningPrefix-Tuning (Li & Liang, 2021), LLaMA-Adapter (Zhang et al., 2024)\\nHyperTuning (Phang et al., 2023)\\nPrompt Tuning\\nPrompt Tuning (Lester et al., 2021), P-Tuning (Liu et al., 2023b),\\nP-Tuning v2 (Liu et al., 2022c), Tam et al. (2023), MP2(Sun et al., 2023a),\\nPPT (Gu et al., 2022b), Multitask Prompt Tuning (Wang et al., 2023j),\\nXu et al. (2023b)\\nMemory-Efficient Fine-Tuning\\nQLoRA (Dettmers et al., 2023), QA-LoRA (Xu et al., 2024b), LoftQ (Li et al., 2024d), PEQA (Kim et al., 2023a),\\nSelective Fine-Tuning (Simoulin et al., 2023), LOMO (Lv et al., 2023), MeZO (Malladi et al., 2023),\\nLiu et al. (2023g)\\nFigure 8: Summary of efficient fine-tuning methods for LLMs.\\ncalculates the moving average of gradients and the estimated Hessian, dividing the former by the latter and\\napplying element-wise clipping. It effectively moderates update sizes, addresses non-convexity and rapid\\nhessian changes, enhancing both memory utilization and efficiency.\\nSystem-Level Pre-Training Efficiency Optimization.Due to high demand on memory and compute\\nresources, LLMs are usually pre-trained across multiple compute nodes in a distributed manner. Therefore,\\nmost system-level optimization techniques are designed in the setting of large-scale distributed training. For\\ninstance, Zero Redundancy Data Parallelism (ZeRO) (Rajbhandari et al., 2020) provides three stages of\\noptimization to partition various training states across different devices. Specifically, ZeRO-1 only partitions\\nthe optimizer states, whereas ZeRO-2 partitions both the optimizer states and the gradients. ZeRO-3\\nfurther partitions the model parameters across devices compared with ZeRO-1 and ZeRO-2. Although\\nruntime memory is further reduced through ZeRO-3, there is about 50% increase in communication volume.\\nTherefore, it is recommended to use ZeRO-3 within a node to minimize the communication time while\\nusing ZeRO-1 and ZeRO-2 across nodes. Fully Sharded Data Parallel (FSDP) (Zhao et al., 2023c) shares a\\nsimilar idea for optimization, and designs a hybrid sharding strategy to allow users to define which nodes or\\nprocesses to partition the gradients, model parameters, and optimizer states across different nodes. In the\\ncase when the weight memory exceeds the aggregated memory that can be provided by all of the compute\\nnodes, ZeRO-Offload (Ren et al., 2021) enables offloading any stage of ZeRO to CPU memory, whereas\\nZeRO-Infinity (Rajbhandari et al., 2021) provides a mechanism to offload to NVMe drives in addition to\\nCPU memory. However, it is quite difficult to maintain performance using these two alternatives, as the data\\nmovement between CPU and GPU is slow. Lastly, training LLMs on numerous GPUs consumes a massive\\namount of energy, Zeus (You et al., 2023) and Perseus (Chung et al., 2023) are proposed to optimize energy\\nconsumption by finding the best GPU-level configurations based on the unique LLM characteristics. The\\nevaluation shows that Perseus reduces energy consumption of large model training by up to 30%.\\n2.3 Efficient Fine-Tuning\\nEfficient fine-tuning techniques focus on reducing the costs of the LLM fine-tuning process. As summarized\\nin Figure 8, efficient fine-tuning techniques can be grouped into parameter-efficient fine-tuning (PEFT) and\\nmemory-efficient fine-tuning (MEFT).\\n13'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 13, 'page_label': '14'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nfixed\\ntuned (added)\\nLLMs \\nreduce\\nreduce\\nbackward\\nLarge ActivationSmall\\nActivation\\nLarge GradientSmall\\nGradient\\nupdate\\nforward\\nFine-T uning \\nData \\nFine-T uning \\nData \\nfixed\\n+ X \\ntuned\\nLLMs \\nFine-T uning \\nData \\nfixed\\n(d) Prompt Tuning\\n(b) Adapter-based Tuning\\nEmbedding\\n+ \\ntuned\\nFine-T uning \\nData \\nfixed\\ntuned\\n(a) Low-Rank Adaptation\\n(c) Preﬁx Tuning\\n(e) Memory-Efﬁcient Fine-Tuning\\nFigure 9: Illustrations of parameter-efficient fine-tuning (a)-(d) and memory-efficient fine-tuning (e).\\n2.3.1 Parameter-Efficient Fine-Tuning\\nParameter-efficient fine-tuning (PEFT) adapts an LLM to downstream tasks by freezing the whole LLM\\nbackbone and only updating a small set of newly added extra parameters. In general, PEFT methods can be\\ngrouped into four categories: low-rank adaptation, adapter-based tuning, prefix tuning, and prompt tuning.\\nLow-Rank Adaptation.Low-rank adaptation (LoRA) (Hu et al., 2022) is a widely used PEFT approach\\nfor LLMs. The hypothesis is that the change in weights during model adaptation has a low “intrinsic rank”.\\nHence, LoRA introduces two trainable low-rank matricesA ∈Rm×r and B ∈Rr×n and adjusts the weight\\nmatrix byW ←W+∆W = W+A·B. As such, only the small matricesA and B are updated during fine-\\ntuning, while the original large weight matrix remains frozen, making the fine-tuning process more efficient.\\nTo enhance the efficiency of LoRA, LoRA-FA (Zhang et al., 2023b) keeps the projection-down weights of\\nA fixed while only updating the projection-up weights of B in each LoRA adapter so that the weight\\nmodifications during fine-tuning are confined to a low-rank space, thereby eliminating the need to store the\\nfull-rank input activations. It achieves comparable accuracy related to full parameter fine-tuning and LoRA.\\nBuilding on top of LoRA, LoraHub (Huang et al., 2023) explores the composability of LoRA for the purpose\\nof generalizing across different tasks. It combines LoRA modules that have been trained on various tasks\\nwith the goal of attaining good performance on tasks that have not been seen before. LongLoRA (Chen et al.,\\n2023i), on the other hand, extends LoRA to the long-context fine-tuning scenario. It introduces shift short\\nattention (S2-Attn), which effectively facilitates context expansion, showing that LoRA is effective for long\\ncontext when utilizing trainable embedding and normalization. Multi-Head Routing (MHR) (Caccia et al.,\\n2023) extends LoRA to Mixture-of-Experts (MoE) architectures. It outperforms Polytropon (Ponti et al.,\\n2023) when operating with a similar parameter allocation. Notably, it achieves competitive performance\\nwhile focusing on fine-tuning the routing function alone, without making adjustments to the adapters,\\ndemonstrating remarkable parameter efficiency. Zhang et al. (2023d) observe that many PEFT techniques\\nneglect the differing significance of various weight parameters. To address this, they propose AdaLoRA\\nwhich employs singular value decomposition to parameterize incremental updates and adaptively distributes\\nthe parameter budget based on the importance score of each weight matrix. The rank in LoRA is static and\\ncannot be adaptively adjusted during fine-tuning. Valipour et al. (2023) propose DyLoRA to introduce a\\ndynamic low-rank adaptation method that trains LoRA blocks across multiple ranks rather than just one by\\norganizing the representations learned by the adapter module based on their ranks. Different from the above-\\nmentioned methods that apply LoRA-based methods to full-size LLMs, CEPT (Zhao et al., 2023b) introduces\\naframeworkthatutilizescompressedLLMs. Specifically, itassesseshowprevalentLLMcompressionmethods\\naffect PEFT performance and subsequently implements strategies for knowledge retention and recovery to\\ncounteract the loss of knowledge induced by compression. Lastly, Tied-LoRA (Renduchintala et al., 2023)\\nuses weight tying and selective training to further increase parameter efficiency of LoRA.\\n14'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 14, 'page_label': '15'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nAdapter-based Tuning.Adapters are bottleneck-like trainable modules integrated into LLMs, which first\\ndown-project the input feature vector followed by a non-linear layer and then up-project back to the original\\nsize (Houlsby et al., 2019). Adapter-based tuning includes both series adapters and parallel adapters. In\\nseries adapters, each LLM layer has two adapter modules added after its attention and feed-forward modules;\\nwhereas parallel adapters position two adapter modules alongside the attention and feed-forward modules\\nwithin each layer of the LLM. In particular, Hu et al. (2023b) propose LLM-Adapters, which integrates series\\nor parallel adapters into LLMs for fine-tuning on different tasks. Karimi Mahabadi et al. (2021) propose\\nCompacter, which unifies adapters, low-rank techniques, and the latest hyper-complex multiplication layers\\nto achieve a balanced trade-off between the amount of trainable parameters and task performance compared\\nto original Adapter method (Houlsby et al., 2019). Furthermore, (IA)3 (Liu et al., 2022a) introduces a\\ntechnique that scales activations using learned vectors. It outperforms Adapter (Houlsby et al., 2019)\\nand Compacter on few-shot setting with better accuracy and computational efficiency. Following meta-\\nlearning principles, Meta-Adapters (Bansal et al., 2022) designs a resource-efficient fine-tuning technique\\nfor the few-shot scenario where it incorporates adapter layers that have been meta-learned into a pre-\\ntrained model, transforming the fixed pre-trained model into an efficient few-shot learning framework. Meta-\\nAdapters outperforms Adapter (Houlsby et al., 2019) at few-shot fine-tuning with less parameters to fine-\\ntune. AdaMix (Wang et al., 2022c) takes inspiration from sparsely-activated mixture-of-experts (MoE)\\nmodels (Zuo et al., 2022) and proposes a mixture of adaptation modules to learn multiple views of the given\\ntask. Compared to Adapter, it demonstrates better results on both natural language understanding and\\ngeneration tasks with less learnable parameters. Lastly, OpenDelta (Hu et al., 2023a) is an open-source\\nsoftware library that offers a versatile and plug-and-play framework for implementing a range of adapter-\\nbased techniques, and is designed to be compatible with various LLMs architectures.\\nPrefix Tuning.Prefix-Tuning (Li & Liang, 2021) adds a series of trainable vectors, known as prefix tokens,\\nto each layer in an LLM. These prefix tokens are tailored to specific tasks and can be treated as virtual\\nword embeddings. Building on top of Prefix-Tuning, LLaMA-Adapter (Zhang et al., 2024) incorporates a\\nset of trainable adaptation embeddings and attaches them to the word embeddings in the upper layers of the\\nLLMs. A zero-initialized attention scheme with zero gating is also introduced. It dynamically incorporates\\nnew guiding signals into LLaMA-1 while retaining its pre-trained knowledge. Different from conventional\\nprefix tuning, HyperTuning (Phang et al., 2023) employs a hyper-model to produce task-specific parameters\\nsuch as soft prefixes for a downstream model, showing improved performance through initialization from\\nhypermodel-generated parameters for subsequent fine-tuning.\\nPrompt Tuning. Different from prefix tuning, prompt tuning incorporates trainable prompt tokens only\\nat the input layer. These tokens can be inserted either as a prefix or anywhere within the input tokens.\\nPrompt Tuning (Lester et al., 2021) keeps the entire pre-trained model fixed while adding an extraktrainable\\ntokens at the beginning of the input text for each downstream task. It outperforms few-shot prompts and\\nnarrows the performance gap compared to full-model fine-tuning. P-Tuning (Liu et al., 2023b) utilizes a\\nsmall number of parameters as prompts, which are processed by a prompt encoder before being used as\\ninput for pre-trained LLMs. Instead of searching for discrete prompts, P-Tuning fine-tunes these prompts\\nthrough gradient descent and improves performance on a wide range of natural language understanding tasks\\ncompared to Prompt Tuning. Liu et al. (2022c) observe that earlier versions of prefix tuning struggle with\\ncomplex sequence labeling tasks. To address this, they propose P-Tuning v2, which borrows the ideas from\\nprefix tuning by introducing continuous prompts at each layer of the pre-trained model. This modification\\nhas proven effective in boosting performance across various parameter sizes for tasks related to natural\\nlanguage understanding. Tam et al. (2023) introduce efficient prompt tuning for text retrieval, updating just\\n0.1% of parameters and outperforming traditional full-parameter update methods in diverse domains. Sun\\net al. (2023a) claim that prompt tuning tends to struggle in few-shot learning scenarios, and thus propose\\nMP2 that pre-trains a collection of modular prompts using multitask learning. These prompts are then\\nselectively triggered and assembled by a trainable routing mechanism for specific tasks. As a result, MP2\\ncan quickly adapt to downstream tasks by learning how to merge and reuse pretrained modular prompts.\\nDifferent from MP2, PPT (Gu et al., 2022b) attributes the performance degradation of prompt tuning in\\nfew-shot learning to the poor initialization of soft prompt, and thus proposes to add the soft prompt into\\nthe pre-training stage for a better initialization. Lastly, Multitask Prompt Tuning (Wang et al., 2023j)\\nextends Prompt Tuning and harnesses the knowledge of the various tasks through the use of prompt vectors\\n15'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 15, 'page_label': '16'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nin a multitask learning settings. Specifically, it initially learns a single, transferable prompt by extracting\\nknowledge from various task-specific source prompts, and then applies multiplicative low-rank updates to\\nthis prompt to effectively tailor it for each downstream task. By doing this, Multitask Prompt Tuning is\\nable to attain performance levels that are competitive compared to full-model fine-tuning methods.\\n2.3.2 Memory-Efficient Fine-Tuning\\nDifferent from PEFT methods which focus on parameter efficiency, MEFT methods focus on memory savings\\nduring the LLMs fine-tuning process. For instance, Dettmers et al. (2023) propose QLoRA which first\\nquantizes the model into a 4-bit NormalFloat data type, and then fine-tunes this quantized model with\\nadded low-rank adapter (LoRA) weights (Hu et al., 2022). In doing so, QLoRA reduces memory usage\\nduring fine-tuning without performance degradation compared to standard full-model fine-tuning. QA-\\nLoRA (Xu et al., 2024b) improves QLoRA by introducing group-wise operators that improve quantization\\nflexibility (each group is quantized separately) while reducing adaptation parameters (each group utilizes\\nshared adaptation parameters). Similarly, LoftQ (Li et al., 2024d) combines model quantization with singular\\nvalue decomposition (SVD) to approximate the original high-precision pre-trained weights. As a result, it\\noffers a favorable initialization point for subsequent LoRA fine-tuning, leading to enhancements over QLoRA\\non both natural language understanding and generation tasks. PEQA (Kim et al., 2023a) introduces a two-\\nstage approach to quantization-aware fine-tuning. In the first stage, the parameter matrix for each fully\\nconnected layer is quantized into a matrix of low-bit integers along with a scalar vector. In the second\\nstage, the low-bit matrix remains unchanged, while fine-tuning is focused solely on the scalar vector for each\\nspecific downstream task. Employing this two-stage approach, PEQA not only minimizes memory usage\\nduring fine-tuning but also speeds up inference time by maintaining weights in a low-bit quantized form,\\nshowing better perplexity than GPTQ (Frantar et al., 2023) with LoRA. Different from above-mentioned\\nMEFT methods that combine LoRA with quantization to reduce fine-tuning memory footprints, as another\\nline of research, some studies propose MEFT methods based on gradient optimization. Specifically, Simoulin\\net al. (2023) propose Selective Fine-Tuning which minimizes memory usage by specifically preserving a subset\\nof intermediate activations from the forward pass for which the calculated gradients are nonzero. Notably,\\nthis approach delivers performance equivalent to full-model fine-tuning while using just up to one-third of the\\nGPU memory required otherwise. Lv et al. (2023) introduce LOMO, which minimizes memory consumption\\nduring fine-tuning by combining gradient calculation and parameter updating into a single step. As such,\\nLOMO eliminates all components of the optimizer state, lowering the memory requirements for gradient\\ntensors to O(1). Lastly, MeZO (Malladi et al., 2023) improves the zeroth-order method (Spall, 1992) for\\ngradient estimation using only two forward passes. This enables efficient fine-tuning of LLMs with memory\\nrequirements similar to inference and supports both full-parameter and PEFT methods like LoRA (Hu et al.,\\n2022) and prefix tuning (Li & Liang, 2021), enabling MeZO to train a 30-billion parameter model on a single\\nA100 80GB GPU.\\n2.4 Efficient Inference\\nEfficient inference techniques focus on reducing the costs of the LLMs inference process. As summarized in\\nFigure 10, efficient inference techniques can be grouped into techniques at algorithm level and system level.\\nAlgorithm-Level Inference Efficiency Optimization. Techniques that enhance LLM inference effi-\\nciency at the algorithm level include speculative decoding and KV-cache optimization.\\n• Speculative Decoding.Speculative decoding (i.e., speculative sampling) (Leviathan et al., 2023)\\nis a decoding strategy for autoregressive language models that speeds up the sampling process by\\ncomputing tokens using a smaller draft model in parallel to create speculative prefixes for the large\\ntarget model. Chen et al. (2023a) focus on the distributed serving setting for LLMs and propose\\nto run a faster autoregressive modelK times and then evaluate the preliminary output with the\\nlarge target model. A tailored rejection sampling strategy is employed to approve a selection of\\nthe draft tokens in a left-to-right order, thereby recapturing the distribution of the large target\\nmodel during the procedure. Staged Speculative (Spector & Re, 2023) transforms the speculative\\nbatch into a tree structure representing potential token sequences. This restructuring expedites the\\n16'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 16, 'page_label': '17'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient Inference\\nAlgorithm-Level Inference\\nEfficiency Optimization\\nSpeculative Decoding\\nSpeculative Decoding (Leviathan et al., 2023), Chen et al. (2023a),\\nStaged Speculative (Spector & Re, 2023), BiLD (Kim et al., 2023c),\\nSpecInfer (Miao et al., 2024), LLMA (Yang et al., 2023b),\\nMedusa (Cai et al., 2024), Santilli et al. (2023), PaSS (Monea et al., 2023)\\nKV-Cache Optimization\\nKIVI (Zirui Liu et al., 2023), KVQuant (Hooper et al., 2024),\\nHeavy-Hitter Oracle (H2O) (Zhang et al., 2023f),\\nScissorhands (Liu et al., 2023e), StreamingLLM (Xiao et al., 2024)\\nSystem-Level Inference\\nEfficiency Optimization\\nFlexGen (Sheng et al., 2023), Pope et al. (2023), S3(Jin et al., 2023), Orca (Yu et al., 2022), vLLM (Kwon et al., 2023),\\nDeepSpeed-Inference (Aminabadi et al., 2022), Flash-Decoding (Dao et al., 2023), FlashDecoding++ (Hong et al., 2023)\\nFigure 10: Summary of efficient inference techniques for LLMs.\\nLarge | language | model | has | witnessed | a | huge | advance\\nCheck & Regenerate\\nGenerate x x x \\nLLMs\\nSmall LMs\\nKV cache [T oken] \\nLLMs\\nnew query\\nold key, value\\nnew key,\\nvalue\\n1 \\n2 \\n3 \\n4 \\n(a)\\xa0Speculative Decoding (b)\\xa0KV-Cache Optimization\\nFigure 11: Illustrations of algorithm-level efficiency optimization techniques for LLM inference.\\ngeneration of larger and improved speculative batches. It also introduces an additional phase for\\nspeculative decoding of the initial model, thereby enhancing overall performance, showing 1.36x over\\nstandard speculative decoding. BiLD (Kim et al., 2023c) optimizes speculative decoding through\\ntwo innovative techniques: the fallback policy that permits the smaller draft model to waive control\\nto the larger target model when it lacks sufficient confidence; and the rollback policy that enables\\nthe target model to revisit and rectify any inaccurate predictions made by the smaller draft model.\\nSpecInfer (Miao et al., 2024) extends speculative decoding and speeds up inference by employing\\nspeculative inference techniques and token tree validation. Its core idea involves merging a range\\nof small speculative models that have been fine-tuned collectively to collaboratively forecast the\\noutput of the large target model, which is then used to validate all the predictions. Different\\nfrom speculative decoding that needs to introduce an additional efficient drafter model to generate\\na draft for checking, LLMA (Yang et al., 2023b) chooses a text segment from a closely related\\nreference and duplicates its tokens into the decoder. It then concurrently assesses the suitability of\\nthese tokens as the decoding output within a single decoding step. This approach results in a speed\\nincrease of more than two times while maintaining the same generated results as traditional greedy\\ndecoding. Similarly, instead of using a separate draft model to sequentially generate candidate\\noutput, Medusa (Cai et al., 2024) proposes to freeze the LLM backbone, fine-tune additional heads,\\nand use a tree-based attention mechanism to process predictions in parallel to speed up the decoding\\nprocess. Lastly, Santilli et al. (2023) propose parallel decoding including the Jacobi and Gauss-Seidel\\nfixed-point iteration methods for speculative decoding. Among these methods, Jacobi decoding was\\nextended into Lookahead decoding (Fu et al., 2023c) to further enhance the efficiency.\\n• KV-Cache Optimization.During inference, LLMs need to store the Key-Value (KV) pairs of the\\npast tokens into the cache for future token generation. The size of KV cache needed enlarges mas-\\nsively with the increase of generated token length, resulting in considerable memory consumption\\nand long inference latency. Therefore, reducing the size of KV cache is key to enhancing inference\\nefficiency. Existing KV-cache optimization techniques can in general be grouped into two categories.\\nThe first category is to compress the KV cache. For example, Zirui Liu et al. (2023) propose KIVI, a\\ntuning-free 2bit KV cache quantization algorithm which quantizes the key cache per-channel and the\\nvalue cache per-token, achieving 2.6x less peak memory usage during inference. Similarly, Hooper\\n17'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 17, 'page_label': '18'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\net al. (2024) conduct an empirical study on the impact of per-channel quantization and other types of\\nquantization such as quantization before rotary positional embedding. Based on their findings, they\\npropose KVQuant which combines these quantization methods to quantize the KV cache of LLaMA\\nto 3-bit. The second category of KV-Cache optimization techniques is to evict some KVs from the\\ncache. For instance, Zhang et al. (2023f) propose Heavy-Hitter Oracle (H2O), a KV cache eviction\\nstrategy that formulates the KV cache eviction as a dynamic submodular problem and dynami-\\ncally retains a balance between recent and performance-critical tokens, improving the throughput\\nfor LLMs inference. Similarly, Liu et al. (2023e) propose a hypothesis named the persistence of\\nimportance, suggesting that only tokens that were crucial at an earlier phase will have a significant\\nimpact on subsequent stages. Based on this hypothesis, they design Scissorhands which significantly\\nreduces the KV cache without compromising model quality. Lastly, StreamingLLM (Xiao et al.,\\n2024) incorporates window attention, where only the most recent KVs are cached into a fixed-size\\nsliding window, into their algorithm design. Through evicting the outdated KVs, StreamingLLM\\nensures constant memory usage and decoding speed after the cache is initially filled.\\nSystem-Level Inference Efficiency Optimization. The efficiency of LLM inference can also be op-\\ntimized at the system level under a specific hardware architecture. For example, FlexGen (Sheng et al.,\\n2023) is a high-throughput inference engine that enables the execution of LLMs on GPUs with limited\\nmemory. It uses a linear programming-based search approach to coordinate various hardware, combining\\nthe memory and computation from GPU, CPU, and disk. Furthermore, FlexGen quantizes the weights\\nand attention cache to 4 bits, which increases the inference speed of OPT-175B (Zhang et al., 2022a) on a\\nsingle 16GB GPU. Pope et al. (2023) develop a simple analytical framework to partition a model in order\\nto scale Transformer inference based on the application requirements. By combining it with scheduling and\\nmemory optimizations, they are able to achieve better efficiency on PaLM (Chowdhery et al., 2022) in com-\\nparison to FasterTransformer (NVIDIA, 2023a). Orca (Yu et al., 2022) employs iteration-level scheduling\\nto serve batched sequences with variable output sequence length. When a sequence in a batch is com-\\npleted, it is returned to the user so that a new sequence can be served immediately. As a result, Orca\\nimproves GPU utilization compared to static batching, showing 36.9× throughput improvement under the\\nsame level of latency compared to FasterTransformer. S3 (Jin et al., 2023) creates a system that is aware\\nof the output sequence beforehand. It can anticipate the length of the sequence and arrange generation\\nrequests accordingly, optimizing the utilization of device resources and increasing the rate of production,\\nshowing higher throughput than Orca with the same number of GPUs. However, both Orca and S3 lead to\\nmemory fragmentation due to their inaccurate memory provisioning for each request. vLLM (Kwon et al.,\\n2023) addresses the memory efficiency problem with PagedAttention, which enables the storage of contin-\\nuous keys and values in non-contiguous memory space. Specifically, PagedAttention divides the KV cache\\nof each sequence into blocks, each containing the keys and values for a fixed number of tokens. During\\nattention computation, PagedAttention kernel manages these blocks efficiently by maintaining a block table\\nto reduce memory fragmentation. Specifically, the contiguous logical blocks of a sequence are mapped to\\nnon-contiguous physical blocks via the table and the table automatically allocates a new physical block for\\nevery newly generated token. This reduces the amount of memory wasted when generating new tokens, thus\\nimproving its efficiency, showing that PagedAttention improves the throughput of popular LLMs by 2-4×\\nwith the same level of latency compared to FasterTransformer (NVIDIA, 2023a) and Orca (Yu et al., 2022).\\nOn the other hand, infinitely optimizing server-side aggregated metrics does not necessarily lead to good\\nuser experience or Quality of Experience (QoE) especially under high server load. Andes (Liu et al., 2024b)\\nfirst defines QoE for the LLM-based text streaming services and proposes a QoE-aware serving systems to\\noptimize QoE by prioritizing requests based on their resource demand and service acquired. DeepSpeed-\\nInference (Aminabadi et al., 2022) is a multi-GPU inference approach designed to enhance the efficiency\\nof both dense and sparse Transformer models when they are contained within the collective GPU memory.\\nFurthermore, it provides a mixed inference technique that utilizes CPU and NVMe memory, in addition\\nto GPU memory and computation, enabling high-throughput inference even for models that are too large\\nto fit in the combined GPU memory. This approach demonstrates lower latency than FasterTransformer\\nunder the same throughput. Flash-Decoding (Dao et al., 2023) boosts the speed of long-context inference\\nby breaking down keys/values into smaller pieces, computing attention on these pieces in parallel, and then\\ncombining them to generate the final output. It outperforms FasterTransformer and FlashAttention (Dao\\n18'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 18, 'page_label': '19'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nEfficient Architecture Design\\nEfficient Attention\\nSharing-based AttentionMQA (Shazeer, 2019), GQA (Ainslie et al., 2023)\\nKernelization or Low-RankSumformer (Alberti et al., 2023), FluRKA (Gupta et al., 2023), Scatterbrain (Chen et al., 2021a),LRT (Winata et al., 2020), Performer (Choromanski et al., 2021), RFA (Peng et al., 2021),Linear Transformer (Katharopoulos et al., 2020), Linformer (Wang et al., 2020)\\nFixed Pattern StrategiesPagliardini et al. (2023), Big Bird (Zaheer et al., 2020), Poolingformer (Zhang et al., 2021),Longformer (Beltagy et al., 2020), Blockwise Transformer (Qiu et al., 2020), Sparse Transformer (Child et al., 2019),Lightning Attention-2 (Qin et al., 2024)\\nLearnable Pattern StrategiesHyperAttention (Han et al., 2024), Reformer (Kitaev et al., 2020), Sparse Sinkhorn Attention (Tay et al., 2020),Clustered Attention (Vyas et al., 2020), ClusterFormer (Wang et al., 2022b), Routing Transformer (Roy et al., 2021)\\nHardware-Assisted AttentionFlashAttention (Dao et al., 2022), vAttention (Prabhu et al., 2024)\\nMixture of Experts (MoE)\\nMoE-based LLMsGShard (Lepikhin et al., 2021), Switch Transformer (Fedus et al., 2022), Artetxe et al. (2022),BASE Layer (Lewis et al., 2021), PanGu-∑(Ren et al., 2023b), Mixtral 8x7B (Jiang et al., 2023a)\\nAlgorithm-Level MoE OptimizationExpert Choice (Zhou et al., 2022), StableMoE (Dai et al., 2022), X-MoE (Chi et al., 2022),Lifelong-MoE (Chen et al., 2023f), Flan-MoE (Shen et al., 2024)\\nSystem-Level MoE OptimizationFastMoE (He et al., 2021), FasterMoE (He et al., 2022a), DeepSpeed-MoE (Rajbhandari et al., 2022),TA-MoE (Chen et al., 2022a), EdgeMoE (Yi et al., 2023), Tutel (Hwang et al., 2023),SmartMoE (Zhai et al., 2023), MegaBlocks (Gale et al., 2023)\\nLong Context LLMs\\nPositional Extrapolation and InterpolationALiBi (Press et al., 2022), xPOS (Sun et al., 2023c), CLEX (Chen et al., 2024a),RoPE-PI (Chen et al., 2023d), NTK Interpolation (bloc97, 2023),YaRN Interpolation (Peng et al., 2024), FIRE (Li et al., 2024a), PoSE (Zhu et al., 2024)\\nRecurrent StructureTransformer-XL (Dai et al., 2019), Memformer (Wu et al., 2022a),∞-former (Martins et al., 2022),RMT (Bulatov et al., 2022), Block-Recurrent Transformer (Hutchins et al., 2022), Retentive Network (Sun et al., 2023b)\\nSegmentation and Sliding WindowMistral (Jiang et al., 2023a), StreamingLLM (Xiao et al., 2024), PCW (Ratner et al., 2023),LongNet (Ding et al., 2023a), SLED (Ivgi et al., 2023), MemWalker (Chen et al., 2023c),RAPTOR (Sarthi et al., 2024)\\nMemory-Retrieval AugmentationMemorizing Transformer (Wu et al., 2022c), Landmark Attention (Mohtashami & Jaggi, 2023),LongMem (Wang et al., 2023e), Unlimiformer (Bertsch et al., 2023),Focused Transformer (Tworkowski et al., 2023), Xu et al. (2024a)\\nTransformer-Alternative Architecture\\nState Space ModelsStructured State Space (Gu et al., 2022a), Diagonal State Space (Gupta et al., 2022), H3 (Fu et al., 2023a),Gated State Space (Mehta et al., 2023), Block-State Transformer (Pilault et al., 2023),Mamba (Gu & Dao, 2023), SMA (Ren et al., 2023a)\\nOther Sequential ModelsRWKV (Peng et al., 2023b), Hyena (Poli et al., 2023), MEGABYTE (YU et al., 2023)\\nFigure 12: Summary of efficient architecture designs for LLMs.\\net al., 2022) in decoding speed for very large sequences. FlashDecoding++ (Hong et al., 2023) supports\\nmainstream language models and hardware backends through asynchronous softmax, double buffering for\\nflat GEMM optimization, and heuristic dataflow, resulting in up to 4.86x and 2.18x acceleration on Nvidia\\nand AMD GPUs respectively compared to HuggingFace implementations, showing higher speedup compared\\nto Flash-Decoding under the same throughput.\\n2.5 Efficient Architecture Design\\nEfficient architecture design for LLMs refers to the strategic optimization of model architecture and compu-\\ntational processes to enhance performance and scalability while minimizing resource consumption. Figure 12\\nprovides a summary of existing efforts on designing efficient architectures for LLMs.\\n2.5.1 Efficient Attention\\nThe quadratic time and space complexity of attention modules considerably slows down the pre-training,\\ninference, and fine-tuning of LLMs (Duman Keles et al., 2023). Many techniques have been proposed to\\nmake attention lightweight for more efficient execution. These techniques can be generally categorized as\\nsharing-based attention, kernelization or low-rank, fixed pattern strategies, learnable pattern strategies, and\\nhardware-assisted attention.\\nSharing-based Attention. Sharing-based attention accelerates attention computation during inference\\nthrough KV heads sharing. For example, LLaMA-2 (Touvron et al., 2023b) optimizes the autoregres-\\nsive decoding process by using multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention\\n(GQA) (Ainslie et al., 2023). In traditional multi-head attention (MHA), each head has distinct linear\\n19'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 19, 'page_label': '20'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nW 1 W 2 x \\nx x x\\n= (1,1) \\n(2,2) \\n(3,3) \\n(4,4) \\nValueIndexOriginal Weight\\nU 1 V 1 U 2 V 2 \\n≈\\n=\\nOriginal FeatureLearned Pattern\\nML\\n(b) Kernelization or\\nLow-Rank\\n(c) Fixed Pattern\\nStrategies\\n(d)\\xa0Learnable Pattern\\nStrategies\\xa0\\nKey \\nValue \\nQuery \\nMulti-Head Sharing-based \\n(a) Sharing-based\\nAttention\\xa0\\nFigure 13: Illustrations of attention optimizations.\\nFlashAttention\\nMemory Hierarchy with\\nBandwidth & Memory Size\\nAttention on GPT-2\\nFlashAttentionPyTorch\\nTime (ms)\\nMatmul\\nMask\\nSoftmax\\nDropout\\nMatmul\\nFused\\nKernel\\nQ: N x d V: N X d\\nKT: d x N\\nQKT: N x N\\nsm(QKT)V: N x d\\nOuter Loop\\nCopy Block to SRAM\\nCopy\\nOuter Loop\\nCopy\\nInner Loop\\nCompute Block\\non SRAM\\nOutput to HBM\\nInner Loop\\nInner Loop\\nOuter Loop\\nGPU\\nSRAM\\nGPU\\nHBM\\nMain Memory\\n(CPU DRAM)\\nSRAM: 19 TB/s (20 MB)\\nHBM: 1.5 TB/s (40 GB)\\nDRAM: 12.8 GB/s\\n                (>1 TB)\\n0\\n5\\n10\\n15 Figure14: DesignofFlashAttention(Dao\\net al., 2022).\\ntransformations for input matrix queries (Q), keys (K) and values (V), allowing for diverse representations\\nand attention mechanisms across different subspaces. In MQA, all heads share a single set of key and value\\nweights across all query heads. Thus, MQA speeds up the inference but could compromise output quality.\\nTo address this drawback, GQA interpolates MQA and MHA by employing one key and value heads for each\\ngroup of query heads to enhance inference quality.\\nKernelization or Low-Rank. Kernelization or low-rank techniques adopted by models such as Sum-\\nformer (Alberti et al., 2023), FluRKA (Gupta et al., 2023), Scatterbrain (Chen et al., 2021a), Low-Rank\\nTransformer (LRT) (Winata et al., 2020), Performer (Choromanski et al., 2021), Random Feature Attention\\n(RFA) (Peng et al., 2021), Linear Transformer (Katharopoulos et al., 2020), and Linformer (Wang et al.,\\n2020) enhance the efficiency by utilizing low-rank representations of the self-attention matrix or by adopting\\nattention kernelization techniques. Specifically, low-rank methods focus on compacting the dimensions of at-\\ntention keys and values. For example, Linformer (Wang et al., 2020) proposes to segment scaled dot-product\\nattention into smaller units via linear projection. Kernelization, a variant of low-rank techniques, focuses\\non approximating the attention matrix (Choromanski et al., 2020). For example, Performer (Choromanski\\net al., 2021) condenses softmax attention-kernels using positive orthogonal random features, outperforming\\nReformerandLinformeronlongproteinsequencebenchmark. Sumformer (Albertietal.,2023)approximates\\nthe equivariant sequence-to-sequence function, offering a universal solution for Linformer and Performer.\\nFixed Pattern Strategies.Fixed pattern strategies adopted by models such as (Pagliardini et al., 2023),\\nBig Bird (Zaheer et al., 2020), Poolingformer (Zhang et al., 2021), Longformer (Beltagy et al., 2020), Block-\\nwise Transformer (Qiu et al., 2020), and Sparse Transformer (Child et al., 2019) improve efficiency by\\nsparsifying the attention matrix. This is achieved by confining the attention scope to predetermined pat-\\nterns, such as local windows or fixed-stride block patterns. For instance, the attention mechanism adopted\\nby Longformer (Beltagy et al., 2020), designed as an alternative to conventional self-attention, merges local\\nwindowed attention with globally oriented attention tailored to specific tasks. Pagliardini et al. (2023) ex-\\npand FlashAttention (Dao et al., 2022) to support a broad spectrum of attention sparsity patterns, including\\nkey-query dropping and hashing-based attention techniques, achieving a multi-fold runtime speedup on top\\nof FlashAttention on long text benchmark.\\nLearnable Pattern Strategies. Different from fixed pattern strategies, learnable pattern strategies\\nadoptedbymodelssuchasHyperAttention(Hanetal.,2024), Reformer(Kitaevetal.,2020), SparseSinkhorn\\nAttention (Tay et al., 2020), Clustered Attention (Vyas et al., 2020), ClusterFormer (Wang et al., 2022b),\\nand Routing Transformer (Roy et al., 2021) improve efficiency by learning token relevance and subsequently\\ngrouping tokens into buckets or clusters. As an example, HyperAttention (Han et al., 2024) proposes a\\nparameterization for spectral approximation and employs two key metrics: the maximal column norm in the\\n20'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 20, 'page_label': '21'}, page_content=\"Published in Transactions on Machine Learning Research (May/2024)\\nLLMs \\nRouter Network 2\\nRouter Network 1\\nInput Output \\nExperts Experts Input \\nsentence 1\\nsentence 1\\nsentence 2\\nsentence 3\\nsentence 4\\nsentence 1\\nsentence 2\\nsentence 3\\nsentence 4\\nIt's short. I can do it.\\nIt's long. I can't do it.\\nIt's long. But I can do it.\\ngood generation\\nBad Generation\\ngood generation\\n(2) window & stream structure\\n(4) recurrent\\xa0 structure\\n(1) Extrapolation\\nand Interpolation\\n(3) Memory-\\nRetrieval\\nAugmentation\\n(a) Mixture of Experts (MoE) (b) Long Context LLMs\\nFigure 15: Illustrations of Mixture of Experts (MoE) and long context LLMs.\\nnormalized attention matrix and the row norm ratio in the unnormalized matrix after large entry removal.\\nIt also utilizes the learnable sort locality-sensitive hashing (sortLSH) technique and fast matrix multiplica-\\ntion via row norm sampling. The experiment results show that HyperAttention enhances both inference and\\ntraining speeds for LLMs with only minimal performance degradation, giving significant speed improvements\\ncompared to FlashAttention (Dao et al., 2022) on long contexts.\\nHardware-AssistedAttention. Hardware-assistedattentionfocusesondevelopinghardware-specifictech-\\nniques to enhance attention efficiency. For example, FlashAttention (Dao et al., 2022) reduces the number\\nof memory access between GPU high-bandwidth memory (HBM) and GPU on-chip SRAM when calculating\\nthe attention module in LLMs. Instead of transmitting the values and results between HBM and SRAM\\nmultiple times as is done in the standard attention mechanism, FlashAttention combines all the attention\\noperations into one kernel and tiles the weight matrices into smaller blocks to better fit the small SRAM as\\nshown in Figure 14. As a result, only one communication is required to process each attention block, which\\nsignificantly enhances the efficiency for processing the entire attention block. vAttention (Prabhu et al.,\\n2024) is proposed to store KV cache in contiguous virtual memory without committing physical memory\\nahead-of-time. It avoids the software complexity to store KV cache by leveraging CUDA support of low-level\\nvirtual memory APIs.\\n2.5.2 Mixture of Experts (MoE)\\nMixture of Experts (MoE) represents a sparse methodology utilized prominently in large-scale models like\\nLLMs. It operates on the principle of segmenting a designated task into several sub-tasks, and then de-\\nveloping numerous smaller, specialized models, dubbedexperts, with each honing in on a distinct sub-task.\\nSubsequently, these experts collaborate to deliver a consolidated output. For pre-traning or fine-tuning, MoE\\nrequires developers to manage a huge number of parameters efficiently, enhancing the model’s capacity and\\npotentially its performance while keeping the computational and memory requirements relatively manage-\\nable. For inference, MoE decreases the inference time by not engaging all experts simultaneously, but rather\\nactivating only a select few. Additionally, MoE is capable of minimizing communication between devices in\\nmodel-distributed scenarios by allocating each expert to an individual accelerator; communication is only\\nnecessary between the accelerators that host the router and the relevant expert model (Kaddour et al., 2023).\\nMoE-based LLMs.Several MoE-based LLMs have been proposed. For example, GShard (Lepikhin et al.,\\n2021) is a MoE-based LLM that offers a refined method to articulate a variety of parallel computation\\nframeworks with minor modifications to the existing model code. It also amplifies a multilingual neural\\nmachine translation Transformer model with Sparsely-Gated MoE beyond 600 billion parameters through\\nautomatic sharding. Switch Transformer (Fedus et al., 2022) brings forth a switch routing algorithm and\\ncraftsintuitivelyenhancedmodels, loweringcommunicationandcomputationalexpenditures. Itencompasses\\nup to one trillion parameters, dividing tasks among up to 2,048 experts, thereby illustrating the scalability\\n21\"), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 21, 'page_label': '22'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nand efficacy of the MoE framework. Artetxe et al. (2022) scale sparse language models to 1.1T parameters,\\ndiscerning superior performance up to this scale in language modeling, zero-shot and few-shot learning\\nin comparison to dense models. This suggests that sparse MoE models are a computationally efficient\\nsubstitute for traditionally employed dense architectures. Its largest MoE model outperforms its dense\\ncounterpart where the latter requires twice as much computation. BASE Layer (Lewis et al., 2021) defines\\ntoken-to-expert allocation as a linear assignment problem, allowing an optimal assignment where each expert\\nacquires an equal number of tokens, achieving lower validation perplexity during training relative to Switch\\nTransformer. PanGu-Σ (Ren et al., 2023b) is a MoE-based LLM with 1.085T parameters, transitioned from\\nthe dense Transformer model to a sparse one with Random Routed Experts (RRE), and effectively trains\\nthe model over 329B tokens utilizing Expert Computation and Storage Separation (ECSS). It outperforms\\ndense model like ERNIE 3.0 Titan Wang et al. (2021) on zero-shot test of Chinese downstream task. Lastly,\\nMixtral 8x7B (Jiang et al., 2023a) is a MoE with 46.7B total parameters. By leveraging the advantage of\\nMoE architecture, Mixtral 8x7B outperforms LLaMA-2 70B on most benchmarks such as MMLU, MBPP,\\nand GSM-8K with 6x faster inference by only using 12.9B parameters of the model per token for inference.\\nAlgorithm-Level MoE Optimization. The efficiency of MoE-based LLMs can be improved at the al-\\ngorithm level. Expert Choice (Zhou et al., 2022) allows experts to pick the top-k tokens instead of having\\ntokens choose the top-k experts, implying that each token can be directed to a variable number of experts\\nwhile each expert maintains a fixed bucket size. This method demonstrates higher performance in the GLUE\\nand SuperGLUE benchmarks, and outperforms T5 dense model in seven out of the 11 tasks. StableMoE\\n(Dai et al., 2022) identifies the issue of altering target experts for identical input during training and ad-\\ndresses this by creating two training phases. Initially, it cultivates a balanced routing strategy, which is\\nthen distilled into a decoupled lightweight router. In the following phase, this distilled router is used for a\\nfixed token-to-expert assignment, ensuring a stable routing strategy. StableMoE shows better results than\\nSwitch Transformer and BASE Layer with lower validation perplexity on language modeling. X-MoE (Chi\\net al., 2022) notes that earlier routing mechanisms foster token clustering around expert centroids, indicating\\na tendency toward representation collapse. It proposes to estimate the routing scores between tokens and\\nexperts on a low-dimensional hyper-sphere, showing improvements over Switch Transformer on multilingual\\nmulti-task benchmark. Lifelong-MoE (Chen et al., 2023f) observes that MoE increases the capacity of the\\nmodel to adapt to different corpus distributions in online data streams without extra computational cost,\\nsimply by incorporating additional expert layers and suitable expert regularization. This facilitates contin-\\nuous pre-training of a MoE-based LLM on sequential data distributions without losing previous knowledge.\\nIt outperforms other MoE models such as GShard on natural language generation and understanding tasks.\\nLastly, Shen et al. (2024) observe that compared to dense models, MoE gains more from instruction tuning.\\nBased on this observation, they propose Flan-MoE, which combines MoE and instruction tuning to en-\\nlarge language models without increasing demands in memory and compute resources while showing better\\nzero-shot and few-shot performance compared to FLAN-T5 dense model.\\nSystem-Level MoE Optimization.The efficiency of MoE-based LLMs can also be improved at the sys-\\ntem level. For example, FastMoE (He et al., 2021) is a distributed MoE training system built on PyTorch,\\ncompatible with common accelerators. This system offers a hierarchical interface that allows both flexi-\\nble model design and easy adaptation to various applications, such as Transformer-XL and Megatron-LM.\\nFasterMoE (He et al., 2022a) tries to address the challenges of dynamic load imbalance, inefficient syn-\\nchronous execution mode, and congested all-to-all communication during MoE training. It first introduces\\na performance model that predicts latency and analyzes end-to-end performance through a roofline-like\\nmethodology. Utilizing this model, it presents a dynamic shadowing technique for load balancing, a concur-\\nrent fine-grained schedule for operations, and a strategy to alleviate network congestion by adjusting expert\\nselection for model training. It outperforms FastMoE and achieves 1.37× - 17.87× speedup compared to\\nmethods including ZeRO, GShard, and BASE Layer. Lina (Li et al., 2023a) also improves training efficiency\\nand inference time by optimizing communication and load balancing in distributed MoE. Lina first prioritizes\\nall-to-all over allreduce using tensor partitioning and pipelining to improve its bandwidth in training, and\\nthen dynamically balances the workload with token-level expert selection pattern in inference. DeepSpeed-\\nMoE (Rajbhandari et al., 2022) has designed a Pyramid-Residual MoE (PR-MoE) architecture to enhance\\nboth the training and the inference efficiency of the MoE model parameter. PR-MoE is a dense-MoE hybrid\\nthat employs residual connections to optimally utilize experts, managing to reduce the parameter size by\\n22'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 22, 'page_label': '23'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nup to 3x without sacrificing quality or compute requirements. It serves massive MoE models with up to\\n4.5x faster and 9x cheaper inference compared to quality-equivalent dense models. DeepSpeed-MoE also\\nproposes a highly optimized MoE inference system which enables efficient scaling of inference workloads on\\nhundreds of GPUs, providing up to 7.3x reduction in inference latency and cost when compared with existing\\nMoE inference solutions. TA-MoE (Chen et al., 2022a) highlights that current MoE dispatch patterns do\\nnot fully leverage the underlying heterogeneous network environment and thus introduces a topology-aware\\nrouting strategy for large-scale MoE training that dynamically modifies the MoE dispatch pattern based\\non the network topology, making it outperform FastMoE, FasterMoE, and DeepSpeed-MoE. EdgeMoE (Yi\\net al., 2023) presents an on-device inference engine tailored for MoE-based LLMs. It optimizes memory and\\ncomputation for inference by distributing the model across different storage levels. Specifically, non-expert\\nmodel weights are stored directly on the edge device, while expert weights are kept externally and only\\nloaded into the device’s memory when necessary. Tutel (Hwang et al., 2023) proposes adaptive parallelism\\nand pipelining features to adapt to the dynamic workload. It employs a consistent layout for MoE param-\\neters and input data, supporting switchable parallelism and dynamic pipelining without any mathematical\\ninconsistencies or tensor migration costs, thus enabling free run-time optimization, achieving up to 5.75×\\nspeedup for a single MoE layer. SmartMoE (Zhai et al., 2023) focuses on the automatic parallelization for\\nMoE distributed training. In the offline stage, SmartMoE constructs a search space of hybrid parallelism\\nstrategies. In the online stage, it incorporates light-weight algorithms to identify the optimal parallel strat-\\negy. It achieves up to 1.88× speedup in end-to-end training over FasterMoE on a distributed training setting.\\nLastly, MegaBlocks (Gale et al., 2023) transforms MoE-oriented computation with block-sparse operations\\nand creates block-sparse GPU kernels to optimize MoE computation on hardware. This leads to training\\ntime up to 40% shorter compared to Tutel and 2.4x shorter than dense models trained with Megatron-LM.\\n2.5.3 Long Context LLMs\\nIn many real-world applications, such as multi-turn conversations and meeting summarization, existing LLMs\\nare often required to comprehend or generate context sequences that are much longer than what they have\\nbeen pre-trained with and may result in a degradation in accuracy due to the poor memorization for the long\\ncontext. One direct way to address this issue is to fine-tune LLMs with similar long-sequence data, which,\\nhowever, is time consuming and computation-intensive. To fill this gap, new methods have been developed\\nto enable LLMs to adapt to longer context lengths in a more efficient way. These methods can be in general\\ngrouped into four categories: extrapolation and interpolation, recurrent structure, segmentation and sliding\\nwindow, and memory-retrieval augmentation.\\nPositional Extrapolation and Interpolation. Standard positional encoding methods such as abso-\\nlute positional embeddings (APE) (Vaswani et al., 2017), learned positional embeddings (LPE) (Wang\\net al., 2022a), relative positional embeddings (RPE) (Shaw et al., 2018), and rotary position embeddings\\n(RoPE) (Su et al., 2023b) have advanced the integration of positional information in LLMs. For example,\\nLPE has been used by GPT-3 and OPT, RPE was used by Gopher (Rae et al., 2022) and Chinchilla (Hoff-\\nmann et al., 2022), whereas RoPE was used by LLaMA-1 and GLM-130B. However, it is still challenging to\\ntrain LLMs on sequences with a limited maximum length while still ensuring them to generalize well on sig-\\nnificantly longer sequences during inference. Given that, techniques based on positional extrapolation (Press\\net al., 2022; Sun et al., 2023c; Chen et al., 2024a) and positional interpolation (Chen et al., 2023d; Peng\\net al., 2024; Li et al., 2024a) have been proposed.\\nPositional extrapolation strategies extend the encoding of positional information beyond what the model\\nhas explicitly learned during training. For example, ALiBi (Press et al., 2022) applies attention with linear\\nbiases to attain extrapolation for sequences that exceed the maximum length seen during training. By ap-\\nplying negatively biased attention scores with a linearly diminishing penalty based on the distance between\\nthe pertinent key and query, it facilitates efficient length extrapolation. Different from ALiBi, xPOS (Sun\\net al., 2023c) characterizes attention resolution as a marker for extrapolation and utilizes a relative posi-\\ntion embedding to enhance attention resolution, thereby improving length extrapolation. However, these\\ntechniques have not been implemented in some of the recent LLMs such as GPT-4, LLaMA, and LLaMA-\\n2. CLEX (Chen et al., 2024a) proposes to generalize position embedding scaling with ordinary differential\\n23'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 23, 'page_label': '24'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nequations to model continuous dynamics over length scaling factors. In doing so, CLEX gets rid of the\\nlimitations of existing positional extrapolation scaling methods to enable long-sequence generation.\\nPositional interpolation strategies, on the other hand, reduce the scale of input position indices and extend\\nthe context window sizes, allowing LLMs to maintain their performance over longer text sequences. For\\nexample, Chen et al. (2023d) observe that extending beyond the trained context length could impair the\\nself-attention mechanism. They propose RoPE-PI to reduce the position indices through linear interpolation,\\naligning the maximum position index with the prior context window limit encountered during pre-training.\\nNTK interpolation (bloc97, 2023) modifies the base of the RoPE, effectively changing the rotational velocity\\nof each RoPE dimension. YaRN interpolation (Peng et al., 2024) uses a ramp function to blend linear\\nand NTK interpolation in varying proportions across dimensions and incorporates a temperature factor to\\ncounteract distribution shifts in the attention matrix caused by long inputs. Experimental results on long-\\ntext modeling shows that YaRN outperforms existing RoPE interpolation methods including RoPE-PI and\\nNTK. FIRE (Li et al., 2024a) proposes a functional relative position encoding using learnable mapping of\\ninput positions to biases and progressive interpolation, ensuring bounded input for encoding functions across\\nall sequence lengths to enable length generalization. It demonstrates competitive results compared to ALiBi,\\nRoPE, and RoPE-PI on long text benchmarks. Lastly, PoSE (Zhu et al., 2024) proposes positional skip-wise\\ntraining that simulates long inputs using a fixed context window and designs distinct skipping bias terms\\nto manipulate the position indices of each chunk. This strategy reduces memory and time consumption\\ncompared to full-length fine-tuning.\\nRecurrent Structure.LLMs’ ability to manage long sequences can also be enhanced through recurrence\\nstructure. For example, Transformer-XL (Dai et al., 2019) presents a segment-level recurrence mechanism\\nand utilizes enhanced relative positional encoding to capture long-term dependencies and address the long-\\ncontext fragmentation issue. Memformer (Wu et al., 2022a) leverages an external dynamic memory for\\nencoding and retrieving past information, achieving linear time and constant memory space complexity\\nfor long sequences. It also proposes Memory Replay Back-Propagation (MRBP) to facilitate long-range\\nback-propagation through time with significantly lower memory requirements, achieving better results than\\nTransformer-XL on language modeling and image generation benchmarks.∞-former (Martins et al., 2022)\\npresentsaTransformermodelaugmentedwithunboundedlong-termmemory(LTM).Itemploysacontinuous\\nspace attention framework to balance the quantity of information units accommodated in memory against\\nthe granularity of their representations, showing better results than Transformer-XL on long text sorting\\nand modeling. Recurrent Memory Transformer (RMT) (Bulatov et al., 2022) uses a recurrence mechanism\\nto retain information from the past segment level by incorporating special memory tokens into the input\\nor output sequence, and demonstrates superior performance compared to Transformer-XL in long context\\nmodeling. Block-Recurrent Transformer (BRT) (Hutchins et al., 2022) utilizes self-attention and cross-\\nattention to execute a recurrent function across a broad set of state vectors and tokens so as to model long\\nsequences through parallel computation. Lastly, Retentive Network (Sun et al., 2023b) introduces a multi-\\nscale retention mechanism as an alternative to multi-head attention. By leveraging parallel and chunk-wise\\nrecurrent representations, it enables effective scaling, achieves training parallelization and constant inference\\ncost, and offers linear long-sequence memory complexity compared to other Transformer models.\\nSegmentation and Sliding Window.Segmentation and sliding window techniques tackle the issue of\\nlong-context processing by dividing the input data into smaller segments, or applying a moving window to\\nslide through the long sequence. For instance, Mistral (Jiang et al., 2023a) uses sliding window attention\\nto handle sequences of arbitrary length with a reduced inference cost. StreamingLLM (Xiao et al., 2024)\\nidentifies an attention sink phenomenon, noting that retaining the Key-Value of initial tokens significantly\\nrestores the performance of window attention. Based on this observation, it suggests an efficient frame-\\nwork via merging window context and the first token, allowing LLMs trained with a finite length attention\\nwindow, but have the ability to generalize to infinite sequence lengths without any fine-tuning. Parallel\\nContext Windows (PCW) (Ratner et al., 2023) segments a long context into chunks, limiting the attention\\nmechanism to function only within each window, and then re-deploys the positional embeddings across these\\nwindows. LongNet (Ding et al., 2023a) proposes dilated attention, which exponentially expands the attentive\\nfield as the distance increases, enabling the handling of sequence lengths of more than one billion tokens.\\nSLED (Ivgi et al., 2023) handles long sequences by partitioning the long text into small chunks and leveraging\\n24'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 24, 'page_label': '25'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\npretrained short-text language models for encoding and decoding. Different from the methods mentioned\\nabove, MemWalker (Chen et al., 2023c) transforms lengthy texts into segmented summaries within a tree\\nstructure, leveraging LLMs as an interactive entity for guided reading through iterative prompts. It out-\\nperforms traditional methods based on extended context, recurrence, and retrieval. Similar to MemWalker,\\nRAPTOR (Sarthi et al., 2024) utilizes text chunks to construct a recursive tree to enable information in-\\ntegration from extensive documents across various abstraction levels during inference, achieving superior\\nperformance in multi-step reasoning.\\nMemory-Retrieval Augmentation.Lastly, several studies tackle the inference of extremely long text by\\nemployingmemory-retrievalaugmentationstrategies. AnotableexampleistheMemorizingTransformer(Wu\\net al., 2022c), which extends the attention context size by utilizing k-nearest-neighbor (KNN) lookup to fetch\\npreviously similar context embeddings. Additionally, Landmark Attention (Mohtashami & Jaggi, 2023) em-\\nploys a landmark token to represent each block of input and trains the attention mechanism to utilize it for\\nchoosing relevant blocks. This allows the direct retrieval of blocks through the attention mechanism while\\nmaintaining the random access flexibility of the previous context, demonstrating comparable perplexity as\\nTransformer-XL while reducing FLOPs for long-context modeling. LongMem (Wang et al., 2023e) proposes\\na decoupled network architecture with the original backbone LLM as a memory encoder and an adaptive\\nresidual side network as a memory retriever and reader, efficiently caching and updating long-term past\\ncontexts to prevent knowledge staleness. It outperforms Memorizing Transformer on long text modeling and\\nnatural language understanding tasks. Unlimiformer (Bertsch et al., 2023) enhances the KNN-augmented\\nTransformer by outputting attention dot-product scores as KNN distances to enable indexing of virtually\\nunlimited input sequences. Experimental results show that Unlimiformer outperforms Memorizing Trans-\\nformer on long document summarization benchmarks. Tworkowski et al. (2023) observe that the ratio of\\nrelevant keys to irrelevant ones diminishes as context length increases. Based on this observation, they\\npropose Focused Transformer (FoT), a contrastive learning-based technique to refine the structure of the\\nKey-Value space. Unlike Memorizing Transformer and Transformer-XL, FoT does not require training on\\nlong sequences and shows better performance on both long context and short context tasks. Lastly, Xu et al.\\n(2024a) discover that an LLM with a 4K context window, when augmented with simple retrieval during\\ngeneration, can match the performance of a fine-tuned LLM with a 16K context window using positional\\ninterpolation (Chen et al., 2023d) on long context tasks while requiring significantly less computation.\\n2.5.4 Transformer-Alternate Architectures\\nWhile Transformer-based architectures are now at the forefront of LLMs, some studies propose new archi-\\ntectures to supplant Transformer-based architectures.\\nState Space Models. A promising approach that aims to substitute the attention mechanism is state space\\nmodels (SSMs). SSM is formulated asx′(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t), which maps a single-\\ndimension input signalu(t) to an N-dimension latent statex(t) before projecting to a single-dimension output\\nsignal y(t), where A, B, C, D are parameters learned by gradient descent (Gu et al., 2022a). Compared\\nto attention that has quadratic complexity, SSMs provide near-linear computational complexity related to\\nthe length of the sequence. Given such advantage, a series of techniques have been proposed to improve\\nSSMs. For example, the Structured State Space (S4) sequence model (Gu et al., 2022a) refines SSMs by\\nconditioning matrix A with a low-rank correction. This enables stable diagonalization and simplifies the\\nSSM to the well-studied computation of a Cauchy kernel. Diagonal State Space (DSS) (Gupta et al., 2022)\\nimproves SSMs by proposing fully diagonal parameterization of state spaces instead of a diagonal plus\\nlow rank structure. To bridge the gap between SSMs and attention while adapting to modern hardware,\\nH3 (Hungry Hungry Hippo) (Fu et al., 2023a) stacks two SSMs to interact with their output and input\\nprojection, allowing it to log tokens and facilitate sequence-wide comparisons. Mehta et al. (2023) introduce\\na more efficient layer called Gated State Space (GSS), which has been empirically shown to be 2 to 3 times\\nfaster than DSS while maintaining the perplexity on multiple language modeling benchmarks. Block-State\\nTransformer (BST) (Pilault et al., 2023) designs a hybrid layer that combines an SSM sublayers for extended\\nrange contextualization with a Block Transformer sublayer for short-term sequence representation. Gu &\\nDao (2023) propose Mamba to enhance SSMs by designing a selection mechanism to eliminate irrelevant\\ndata and develop a hardware-aware parallel algorithm for recurrent operation, achieving 5x throughput than\\n25'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 25, 'page_label': '26'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nData Selection\\nData Selection for Efficient Pre-TrainingSSPT (Glass et al., 2020), Yao et al. (2022a), DSIR (Xie et al., 2023b), DoReMi (Xie et al., 2023a)\\nData Selection for Efficient Fine-TuningIvison et al. (2023), Instruction Mining (Cao et al., 2023), TS-DShapley (Schoch et al., 2023),\\nLTD Instruction Tuning (Chen et al., 2023b), AlpaGasus (Chen et al., 2024b), LIMA (Zhou et al., 2023a)\\nFigure 16: Summary of data selection techniques for LLMs.\\nLLMs \\nsubset subset\\nTraining Data Fine-Tuning Data\\nSelect Select Training Fine- \\nTuning \\n(a) Data Selection for Efﬁcient Pre-Training (b) Data Selection for Efﬁcient Fine-Tuning\\nFigure 17: Illustrations of data selection techniques for LLMs.\\nTransformers. Ren et al. (2023a) extend SSMs and propose a general modular activation mechanism named\\nSparse Modular Activation (SMA), which unifies MoE, adaptive computation, dynamic routing and sparse\\nattention, and further applies SMA to develop a novel architecture, SeqBoat, to achieve state-of-the-art\\nquality-efficiency trade-off.\\nOther Sequential Models. Some other architectures have been proposed to replace the Transformer\\nlayer. For instance, Receptance Weighted Key Value (RWKV) model (Peng et al., 2023b) combines the\\nadvantages of recurring neural networks (RNN) and Transformers. Such combination utilizes the effective\\nparallelizable training feature of Transformers coupled with the efficient inference ability of RNNs, thereby\\neffectively tackling the challenges associated with long sequence processing, outperforming Transformer-\\nbased models such as BLOOM and OPT. Poli et al. (2023) propose Hyena, a sub-quadratic alternative to\\nthe attention mechanism to mitigate the quadratic cost in long sequences. This operator includes two efficient\\nsub-quadratic primitives: an implicit long convolution and multiplicative element-wise gating of the input.\\nHyena facilitates the development of larger, more efficient convolutional language models for long sequences\\nand outperforms RWKV and GPT-Neo on SuperGLUE tasks (Wang et al., 2019). Lastly, MEGABYTE (YU\\net al., 2023) breaks down long sequences into fixed-sized patches akin to tokens, comprising a patch embedder\\nfor encoding, a global module acting as a large autoregressive Transformer for patch representations, and a\\nlocal module for predicting bytes within a patch.\\n3 Data-Centric Methods\\n3.1 Data Selection\\nData selection is a fundamental technique for enhancing efficiency. As summarized in Figure 16, in the\\ncontext of LLMs, data selection techniques have been primarily used for enhancing the efficiency of pre-\\ntraining and fine-tuning.\\n3.1.1 Data Selection for Efficient Pre-Training\\nData selection enhances LLMs pre-training efficiency by strategically selecting informative and diverse data\\nsamples during training. For example, SSPT (Glass et al., 2020) is a pre-training task based on the principles\\nof reading comprehension. It involves selecting answers from contextually relevant text passages, which has\\nshown notable improvements in performance across various Machine Reading Comprehension benchmarks.\\nYao et al. (2022a) propose a meta-learning-based method for selecting linguistically informative sentences\\nwhich significantly elevates the quality of machine-generated translations. Xie et al. (2023b) propose DSIR,\\n26'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 26, 'page_label': '27'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\na data selection method based on importance resampling for both general-purpose and specialized LLMs. It\\ncalculates how important different pieces of data are within a simpler set of features and chooses data based\\non these importance calculations. Experimental results demonstrate that DSIR achieves similar performance\\nto expert curation across eight different target distributions. In the context of pre-training general-domain\\nmodels, DSIR outperforms random selection and heuristic filtering baselines by 2–2.5% on the GLUE bench-\\nmark. Different from DSIR, Xie et al. (2023a) design DoReMi to address the distribution shift between\\npre-training and downstream tasks, which is also a critical problem for training data selection.\\n3.1.2 Data Selection for Efficient Fine-Tuning\\nData selection can also boost LLM fine-tuning efficiency given that only a curated subset of examples is\\nemployed to refine the model. For example, Ivison et al. (2023) propose to use a few unlabeled data samples\\nto retrieve similar labeled ones from a larger multitask dataset, improving task-specific model training. This\\nmethod outperforms standard multitask data sampling for fine-tuning and enhances few-shot fine-tuning,\\nyielding an 2-23% relative improvement. With the success of instruction tuning, many studies start focusing\\non the selection of high-quality instruction data to fine-tune LLMs. For example, Instruction Mining (Cao\\net al., 2023) presents a linear evaluation method to assess data quality in instruction-following tasks. It\\nhighlights the importance of high-quality data, showing that models trained with Instruction Mining-curated\\ndatasets outperform those trained on generic datasets in 42.5% of the considered cases. TS-DShapley (Schoch\\net al., 2023) is introduced to address the computational challenges of applying Shapley-based data valuation\\ntofine-tuningLLMs. Itemploysanefficientsampling-basedmethodthataggregatesShapleyvaluescomputed\\nfrom subsets to evaluate the entire training set. Low Training Data Instruction Tuning (LTD Instruction\\nTuning) (Chen et al., 2023b) challenges the need for large datasets in fine-tuning, showing that less than 0.5%\\nof the original dataset is able to effectively train task-specific models without compromising performance.\\nThis approach enables more resource-efficient practices in data-scarce environments, combining selective\\ndata strategies with tailored training protocols for optimal data efficiency. AlpaGasus (Chen et al., 2024b)\\nis a model fine-tuned on a mere 9K high-quality data samples, which are meticulously filtered from a larger\\ndataset of 52K. It outperforms the original model trained on the full dataset and reduces the fine-tuning time\\nby 5.7x, demonstrating the power of high-quality data in instruction-fine-tuning. Lastly, LIMA (Zhou et al.,\\n2023a) fine-tunes LLMs with a small, selected set of examples, showing strong performance and challenging\\nthe need for extensive tuning. It generalizes well to new tasks, matching or exceeding GPT-4 in 43% of the\\nconsidered cases.\\n3.2 Prompt Engineering\\nPrompt engineering (Liu et al., 2023a) focuses on designing effective inputs (i.e., prompts) to guide LLMs\\nin generating desired outputs. It enhances inference efficiency by tailoring the input prompts or queries\\nto better suit the capabilities of a specific language model. When used for some simple tasks, such as\\nsemantic classification, prompt engineering can even substitute fine-tuning to achieve high accuracy (Liu\\net al., 2022a). As summarized in Figure 18, prompt engineering techniques can in general be grouped into\\nfew-shot prompting, prompt compression, and prompt generation.\\n3.2.1 Few-Shot Prompting\\nFew-shot prompting aims to provide a LLM with a limited set of examples, referred to as demonstrations,\\nto steer its understanding to a task it is required to execute (Wei et al., 2022a). These demonstrations are\\nselectedfromthetrainingcorpusbasedontheirsimilaritytothetestexample, andtheLLMisexpectedtouse\\nthe knowledge gained from these similar demonstrations to make the correct prediction (Dong et al., 2023).\\nFew-shot prompting provides an efficient mechanism to use LLM by guiding the LLM to perform a wide\\nvariety of tasks without the need for additional training or fine-tuning. Furthermore, an effective few-shot\\nprompting approach can make the created prompt concise enough to allow LLMs to quickly adjust to the task\\nin high accuracy with only a slight increase of extra context, thus significantly improving inference efficiency.\\nAs illustrated in Figure 19, few-shot prompting techniques can in general be grouped into demonstration\\norganization and template formatting.\\n27'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 27, 'page_label': '28'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nPrompt Engineering\\nFew-Shot Prompting\\nDemonstration Organization\\nDemonstration Selection\\nKATE (Liu et al., 2022b), VoteK (SU et al., 2023), Wang et al. (2023f),IDS (Qin et al., 2023a), Min et al. (2022b), LENS (Li & Qiu, 2023),MDL (Wu et al., 2023c), Zhang et al. (2022b), EPR (Rubin et al., 2022),UDR (Li et al., 2023f), Wang et al. (2024b), Luo et al. (2023)\\nDemonstration OrderingLu et al. (2022)\\nTemplate Formatting\\nInstruction GenerationInstruction Induction (Honovich et al., 2023),Automatic Prompt Engineer (Zhou et al., 2023c), Self-Instruct (Wang et al., 2023h),OPRO (Yang et al., 2023a), TeGit (Chen et al., 2023h)\\nMulti-Step Reasoning\\nChain-of-Thought (Wei et al., 2022b), Auto-CoT (Zhang et al., 2023g), Self-Ask (Press et al., 2023),ReAct (Yao et al., 2023b), Least-to-Most Prompting (Zhou et al., 2023b),Tree-of-Thought (Yao et al., 2023a), CoT-SC (Wang et al., 2023h),Graph of Thoughts (Besta et al., 2024), Contrastive CoT (Chia et al., 2023),XoT (Ding et al., 2023b), Skeleton-of-Thought (Ning et al., 2024)\\nPrompt CompressionGisting (Mu et al., 2023), AutoCompressors (Chevalier et al., 2023), PCRL (Jung & Kim, 2023),ICAE (Ge et al., 2024), Nugget 2D (Qin et al., 2023b), LongLLMLingua (Jiang et al., 2024)\\nPrompt GenerationAutoPrompt (Shin et al., 2020), TempLM (Zhang et al., 2023e), PromptGen (Zhang et al., 2022c)\\nFigure 18: Summary of prompt engineering techniques for LLMs.\\nThis food tastes good.\\nThe review is __\\nTraining Data\\nIt is a nice restaurant.\\nThe review is positive.\\nThis t-shirt looks cool.\\nThe review is positive.\\nThe dining room is dirty.\\nThe review is negative.\\nembedding \\ngeneration \\nselect \\nInput\\nIt is a nice restaurant.\\nThe review is positive. \\nThe dining room is dirty.\\nThe review is\\xa0negative.\\nThis t-shirt looks cool.\\nThe review is\\xa0positive.\\nIs the review positive or negative?\\xa0\\nIt is a nice restaurant. The review is positive.\\nThe dining room is dirty. The review is\\xa0negative.\\nThis t-shirt looks cool. The review is\\xa0positive.\\nIs the review positive or negative? \\nIt is a nice restaurant. The sentence contains a positive\\nword \"nice\". The review is positive. \\nThe dining room is dirty. The sentence contains a negative\\nword \"dirty\".The review is negative. \\nThis t-shirt looks cool. The sentence contains a positive\\nword \"cool\". The review is positive.LLMs \\nThis food tastes good.\\nThe review is positive\\nOutput\\nDemonstration Selection Demonstration Ordering\\nInstruction GenerationMulti-Step Reasoning\\n(a) Demonstration Organization\\n(b) Template Formatting\\nembedding \\nFigure 19: Illustrations of few-shot prompting techniques for LLMs.\\nDemonstration Organization.Demonstration organization refers to organizing the demonstrations in an\\nappropriate way so as to form a suitable prompt for inference. Demonstration organization has a significant\\nimpact on the inference efficiency since improper organization may result in the processing of a considerable\\namount of unnecessary information, leading to significant slowdown. The optimization of demonstration\\norganization comes from its two main steps: demonstration selection and demonstration ordering.\\n• Demonstration Selection. Demonstration selection aims to choose the good examples for few-\\nshot prompting (Dong et al., 2023). In order to generate a satisfactory result, a good selection of\\ndemonstrations may only require a few number of demonstrations to be used for the prompt, thus\\nmakingthepromptconciseandstraightforwardforamoreefficientinference. Existingdemonstration\\nselection techniques can be grouped into unsupervised methods (Liu et al., 2022b; SU et al., 2023;\\nWang et al., 2023f; Qin et al., 2023a; Min et al., 2022b; Li & Qiu, 2023; Wu et al., 2023c; Zhang\\net al., 2022b) and supervised methods (Rubin et al., 2022; Li et al., 2023f; Wang et al., 2024b; Luo\\n28'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 28, 'page_label': '29'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\net al., 2023). Unsupervised methods aim to select the nearest examples from the training set using\\na predefined similarity function, such as L2 distance, cosine distance, and minimum description\\nlength (MDL) (Wu et al., 2023c). For example, KATE (Liu et al., 2022b) is an unsupervised\\nselection method that directly uses the nearest neighbors of a given test sample as the corresponding\\ndemonstrations. VoteK (SU et al., 2023) is an improved version of KATE to resolve its limitation\\nthatrequiresalarge setofexamplestoachievegoodperformance. Unlike KATE,VoteKincreasesthe\\ndiversity of the demonstrations by penalizing examples similar to those already selected. In contrast,\\nsupervised methods require training a domain-specific retriever from the training set and using it for\\ndemonstration selection. For example, EPR (Rubin et al., 2022) is trained to select demonstrations\\nfrom a small set of candidates initialized by the unsupervised retriever such as BM25 from the\\ntraining corpus. UDR (Li et al., 2023f) further enhances EPR by adopting a unified demonstration\\nretriever to unify the demonstration selection across different tasks. Compared to unsupervised\\nmethods, supervised methods often lead to a more satisfying generation result but require frequent\\nadjustment of the retriever for handling the out-of-domain data, making them relatively less efficient\\nfor inference.\\n• Demonstration Ordering.After selecting representative samples from the training set, the next\\nstep is ordering these samples in the prompt. The order of the demonstrations also has a significant\\nimpact on the performance of the model. Therefore, selecting the right order of demonstrations\\ncan help the model quickly reach a good generation quality with fewer samples, thus improving\\nthe inference efficiency. To date, only a few studies have delved into this area. For example, Liu\\net al. (2022b) suggest arranging demonstrations based on their distance from the input, placing the\\nclosest demonstration furthest to the right. Lu et al. (2022) propose to develop both global and\\nlocal entropy metrics and use the entropy metrics to set up the demonstration order.\\nTemplate Formatting.Template formatting aims to design a suitable template to form the prompt. A\\ngood template typically compiles all the information needed by LLMs into a brief statement, making the\\nprompt and the entire input context as succinct as possible, thus leading to a higher inference efficiency.\\nTemplate formatting can be divided into two parts: instruction generation and multi-step reasoning.\\n• Instruction Generation.The instruction of the template refers to a short description of the task.\\nBy adding instructions to the prompt, LLMs can quickly understand the context and the task they\\nare currently performing, and thus may require fewer demonstrations to create a desirable prompt.\\nThe performance of a given task is highly affected by the quality of the instructions. The instructions\\nvary not only between different datasets for the same task but also between different models. Unlike\\ndemonstrations that are usually included in traditional datasets, the generation of instructions is\\nheavily dependent on human efforts. To enhance the efficiency of instruction generation, automatic\\ninstructiongenerationtechniqueshavebeenproposed. Forexample, InstructionInduction(Honovich\\net al., 2023) and Automatic Prompt Engineer (Zhou et al., 2023c) demonstrate that LLMs can\\ngenerate task instructions. Wang et al. (2023h) propose Self-Instruct, an approach that allows LLMs\\nto align with self-generated instructions, highlighting their inherent adaptability. Experimental\\nresults show that it achieves an 33% improvement over the original model when applied on vanilla\\nGPT3. Yang et al. (2023a) also discover that LLMs can be treated as an optimizer to iteratively\\ngenerate better instructions for the target LLM and have applied this technique to various LLMs.\\nExperiments demonstrate that the best prompts optimized by this method outperform human-\\ndesigned prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Chen et al.\\n(2023h) develop TeGit for training language models as task designers, which can automatically\\ngenerate inputs and outputs together with high-quality instructions to better filter the noise based\\non a given human-written text for fine-tuning LLMs. Despite the promise of automatic instruction\\ngeneration methods, their complexity is still a major bottleneck for their real-world adoption.\\n• Multi-Step Reasoning. Multi-step reasoning (Huang & Chang, 2023) refers to techniques that\\nguide the LLMs to produce a sequence of intermediate steps before outputting the final answer.\\nCompared to fine-tuning, conducting specific task reasoning directly through this way is a more effi-\\ncient approach. At the same time, its accuracy improvement is often not as good as fine-tuning. One\\n29'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 29, 'page_label': '30'}, page_content=\"Published in Transactions on Machine Learning Research (May/2024)\\nLMs \\nCompress \\nA B C T est \\nA' C' B' T est \\nDemonstrations Input\\nT est \\nInput\\nA' T est LMs \\nGenerate \\n(a) Prompt Compression (b) Prompt Generation\\nFigure 20: Illustrations of prompt compression (a) and prompt generation (b) for LLMs.\\nof the widely used techniques in multi-step reasoning is Chain-of-Thought (CoT) prompting (Wei\\net al., 2022b), which adds a series of reasoning steps into the prompt to make it more informative\\nand comprehensive. Despite its advantages, it is still difficult for CoT to ensure the accuracy of\\nevery intermediate step (Dong et al., 2023). A number of techniques have been proposed to address\\nthis issue. For example, Auto-CoT (Zhang et al., 2023g) proposes to generate the CoT step by\\nstep from LLMs. Self-Ask (Press et al., 2023) incorporates the self-generated questions of each step\\ninto CoT. ReAct (Yao et al., 2023b) performs dynamic reasoning to create, maintain, and adjust\\nhigh-level plans for acting, while interacting with external environments to incorporate additional\\ninformation into reasoning. Least-to-Most Prompting (Zhou et al., 2023b) is a new milestone in\\nCoT. It breaks down the complex question into smaller ones and answers them iteratively within the\\ncontext of former questions and answers. Experimental results show that Least-to-Most Prompting\\ncan boost the accuracy of GPT-3 code-davinci-002 model with CoT prompting to 99.7% by using\\njust 14 exemplars. Tree-of-Thought (ToT) (Yao et al., 2023a) expends CoT to include exploration\\nover coherent units of text and deliberates decision-making processes. It outperforms GPT-4 with\\nCoT prompting in the Game of 24 benchmark. CoT-SC (Wang et al., 2023h) introduces a novel\\ndecoding approach called self-consistency to replace the greedy decoding in CoT prompting. It\\nstarts by sampling various reasoning paths instead of just the greedy one and then determines the\\nmost consistent answer by considering all the sampled paths. Graph of Thoughts (GoT) (Besta\\net al., 2024) represents information produced by an LLM as a generic graph, with “LLM thoughts”\\nas vertices and edges indicating dependencies between these vertices. Experimental results show\\nthat GoT increases the quality of sorting by 62% over ToT while reducing the cost by over 31%.\\nContrastive CoT (Chia et al., 2023) proposes to enhance language model reasoning by providing\\nboth valid and invalid reasoning demonstrations. XoT (Ding et al., 2023b) utilizes pretrained rein-\\nforcement learning and Monte Carlo Tree Search (MCTS) to integrate external domain knowledge\\ninto the thought processes of LLMs, thereby boosting their ability to efficiently generalize to new,\\nunseen problems. Lastly, Skeleton of Thought (SoT) (Ning et al., 2024) proposes a method that first\\nprompts the LLM to organize the output and then parallelizes the generation of different segments.\\nThrough splitting the serial generation into two different steps, it makes the generation workload\\nmore parallelizable. Therefore, it improves hardware utilization and provides speedups across LLM,\\nrevealing the possibility of exploiting data-level organization to enhance efficiency.\\n3.2.2 Prompt Compression\\nPrompt compression (Figure 20(a)) accelerates the processing of LLM inputs through either condensing\\nlengthy prompt inputs or learning compact prompt representations. Mu et al. (2023) propose to train LLMs\\nto distill prompts into a more concise set of tokens, referred to as gist tokens. These gist tokens encapsulate\\nthe knowledge of the original prompt and can be stored for future use. In doing so, it is able to compress\\nprompts by up to 26x, leading to a reduction in floating-point operations per second (FLOPs) by up to\\n30\"), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 30, 'page_label': '31'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\n40%. Chevalier et al. (2023) propose AutoCompressors to condense long textual contexts into compact\\nvectors, known as summary vectors, which can then be used as soft prompts for the language model. These\\nsummary vectors extend the model’s context window, allowing it to handle longer documents with much less\\ncomputational cost. In particular, AutoCompressors can utilize long contexts to improve perplexity of both\\nfine-tuned OPT and LLaMA-2 on sequences of up to 30,720 tokens. Jung & Kim (2023) propose Prompt\\nCompression with Reinforcement Learning (PCRL) that employs a policy network to directly edit prompts,\\naiming to reduce token count while preserving performance. It achieves an average reduction of 24.6% in\\ntoken count across various instruction prompts. Ge et al. (2024) propose In-context Autoencoder (ICAE),\\nwhich consists of a learnable encoder and a fixed decoder. The encoder compresses a long context into a\\nlimited number of memory slots, which the target language model can then condition on. With such design,\\nICAE is able to obtain 4x context compression. Nugget 2D (Qin et al., 2023b) represents the historical\\ncontext as compact “nuggets” that are trained to enable reconstruction. Furthermore, it has the flexibility\\nto be initialized using readily available models like LLaMA. Eperimental results show that Nugget 2D\\ncompresses context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly\\nlossless encoding. Lastly, LongLLMLingua (Jiang et al., 2024) introduces a prompt compression technique\\ncontaining question-aware coarse-to-fine compression, document reordering, dynamic compression ratios, and\\npost-compression sub-sequence recovery to enhance LLMs’ key information perception. Experimental results\\nshow that LongLLMLingua achieves 17.1% better performance over the original prompt with 4x fewer tokens\\nas input to GPT-3.5-Turbo.\\n3.2.3 Prompt Generation\\nPrompt generation (Figure 20(b)) enhances efficiency by automatically creating effective prompts that guide\\nthe model in generating specific and relevant responses. For instance, AutoPrompt (Shin et al., 2020)\\nproposes an automated method to generate prompts for a diverse set of tasks based on a gradient-guided\\nsearch. It underscores the significance of human-written text in refining the quality and authenticity of\\ndata, emphasizing its pivotal role in optimizing LLM performance. Experimental results demonstrate that\\nAutoPrompt outperforms manually created prompts on the LAMA benchmark in eliciting more precise\\nfactual knowledge from LLM. TempLM (Zhang et al., 2023e) proposes to combine generative and template-\\nbased methodologies to distill LLMs into template-based generators, offering a harmonized solution for\\ndata-to-text tasks. TempLM not only reduces the unfaithfulness rate of a fine-tuned BART model from 83%\\nto 0%, but also substantially improves upon human-written ones in BERTScore. Lastly, PromptGen (Zhang\\net al., 2022c) considers dynamic prompt generation for knowledge probing based on pre-trained LLMs. It\\nautomatically generates prompts conditional on the input sentence and outperforms AutoPrompt on the\\nLAMA benchmark.\\n4 LLM Frameworks\\nLLM frameworks can be in general grouped based on whether they support the tasks of training, fine-\\ntuning, and inference. Specifically, frameworks that support training and/or fine-tuning aim to provide\\nscalable, efficient, and flexible infrastructure that improves computation efficiency, reduces memory footprint,\\noptimizes communication efficiency, and ensures reliability of the training/fine-tuning process. Frameworks\\nthat support inference focus on optimizing inference throughput and reducing memory footprint and latency.\\nThese frameworks offer a variety of deployment features to serve LLM requests. Table 2 provides a summary\\nof existing LLM frameworks along with their key features.\\nDeepSpeed. Developed by Microsoft, DeepSpeed (Rasley et al., 2020) is an integrated framework for both\\ntraining and serving LLMs. It has been used to train large models like Megatron-Turing NLG 530B (Smith\\net al., 2022) (in a joint effort with Nvidia Megatron framework) and BLOOM (Scao et al., 2023). Within\\nthis framework, DeepSpeed-Inference is the foundational library. A pivotal feature of DeepSpeed-Inference is\\nZeRO-Inference (Rajbhandari et al., 2020; 2021), an optimization technique created to address GPU memory\\nconstraints for large model inference. ZeRO-Inference distributes model states across multiple GPUs and\\nCPUs, providing an approach to managing the memory constraints of GPUs.\\n31'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 31, 'page_label': '32'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTable 2: Comparison of LLM frameworks.\\nFramework Training Fine-Tuning Inference Key Features\\nDeepSpeed /check-circle /check-circle /check-circle3D Parallelism with ZeRO (Rasley et al., 2020),\\nZeRO-2 (Rajbhandari et al., 2020), ZeRO In-\\nfinity (Rajbhandari et al., 2021) and ZeRO-\\nOffload (Ren et al., 2021), Expert Parallelism (Ra-\\njbhandari et al., 2022), FlashAttention (Dao et al.,\\n2022), PagedAttention (Kwon et al., 2023), Dy-\\nnamic SplitFuse (Holmes et al., 2024), Continuous\\nBatching (Yu et al., 2022), ZeroQuant (Wu et al.,\\n2023b; Yao et al., 2023d), INT4 Quantization (Wu\\net al., 2023a), XTC (Ternary quantization) (Wu\\net al., 2022b), RLHF (Yao et al., 2023c), Kernel\\nOptimizations, Diverse Hardware Support.\\nMegatron /check-circle /check-circle /check-circle3D Parallelism (Shoeybi et al., 2020; Narayanan\\net al., 2021), Sequence Paralellism (Korthikanti\\net al., 2023; Li et al., 2023d), Expert Par-\\nallelism (Singh et al., 2023), FasterTrans-\\nformer (NVIDIA, 2023a), FlashAttention (Dao\\net al., 2022), Selective Activation Recomputa-\\ntion (Korthikanti et al., 2023).\\nColossal-AI /check-circle /check-circle /check-circle3D Parallelism (Xu & You, 2023; Wang et al.,\\n2023a; Bian et al., 2021), Sequence Parallelism (Li\\net al., 2023d), ZeRO Optimizer (Zhao et al.,\\n2022), Auto-Parallelism (Liu et al., 2023c), Het-\\nerogeneous Memory Management (Fang et al.,\\n2023), Expert Parallelism (Singh et al., 2023;\\nXue et al., 2023), PagedAttention (Kwon et al.,\\n2023), FlashAttention-2 (Dao, 2024), Quan-\\ntization (GPTQ (Frantar et al., 2023) and\\nSmoothQuant (Xiao et al., 2023)), RLHF (Grif-\\nfith et al., 2013; Singh et al., 2023).\\nNanotron /check-circle /check-circle /check-circle3D Parallelism (Narayanan et al., 2021; Huang\\net al., 2019), Expert Parallelism (Singh et al.,\\n2023), ZeRO Optimizer (Zhao et al., 2022),\\nSSM (Gu & Dao, 2023) Support, Spectral\\nµTransfer Parametrization (Yang et al., 2022),\\nDoReMi (Xie et al., 2023a).\\nMegaBlocks /check-circle /check-circle /check-circleFSDP (Zhao et al., 2023c), dropless-MoE (Gale\\net al., 2023), Integration with Megatron and\\nvLLM.\\nFairScale /check-circle /check-circle /check-circleFSDP (Zhao et al., 2023c), Pipeline Paral-\\nlelism (Huang et al., 2019), AdaScale opti-\\nmizer (Johnson et al., 2020).\\nPax /check-circle /check-circle /check-circleData Parallelism, Tensor Parallelism (Shoeybi\\net al., 2020).\\nComposer /check-circle /check-circle /check-circleFSDP (Zhao et al., 2023c), Elastic Sharded Check-\\npointing.\\nContinued on next page\\n32'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 32, 'page_label': '33'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTable 2: Comparison of LLM frameworks. (Continued)\\nOpenLLM /times-circle/check-circle /check-circleFSDP (Zhao et al., 2023c), Quantization\\n(GPTQ (Frantar et al., 2023), AWQ (Lin\\net al., 2023), SqueezeLLM (Kim et al.,\\n2024), SpQR (Dettmers et al., 2024),\\nLLM.int8 (Dettmers et al., 2022)), LangChain,\\nTransformers Agents, Prometheus Metrics.\\nLLM\\nFoundry\\n/times-circle/check-circle /check-circleFSDP (Zhao et al., 2023c), Continuous Batch-\\ning (Yu et al., 2022).\\nvLLM /times-circle /times-circle/check-circleData Parallelism, Tensor Parallelism (Shoeybi\\net al., 2020), PagedAttention (Kwon et al., 2023),\\nContinuous Batching (Yu et al., 2022), Quanti-\\nzation (GPTQ (Frantar et al., 2023), AWQ (Lin\\net al., 2023), SqueezeLLM (Kim et al., 2024)),\\nMulti-LoRa (Wang et al., 2023g).\\nTensorRT-\\nLLM\\n/times-circle /times-circle/check-circle3D Parallelism, PagedAttention (Kwon et al.,\\n2023), Continuous Batching (Yu et al., 2022),\\nQuantization (GPTQ (Frantar et al., 2023),\\nAWQ (Linet al.,2023), SmoothQuant(Xiao etal.,\\n2023)).\\nTGI /times-circle /times-circle/check-circleData Parallelism, Tensor Parallelism (Shoeybi\\net al., 2020), PagedAttention (Kwon et al.,\\n2023), Continuous Batching (Yu et al., 2022),\\nQuantization (BitsAndBytes Dettmers (2023),\\nGPTQ (Frantar et al., 2023)).\\nRayLLM /times-circle /times-circle/check-circleData Parallelism, Tensor Parallelism (Shoeybi\\net al., 2020), Continuous Batching (Yu et al.,\\n2022), Quantization (GPTQ (Frantar et al., 2023),\\nAWQ (Lin et al., 2023), SqueezeLLM (Kim et al.,\\n2024)), Prometheus Metrics.\\nMLC LLM /times-circle /times-circle/check-circleTVM-based Compiler Acceleration (Feng et al.,\\n2023; Chen et al., 2018), Continuous Batching (Yu\\net al., 2022), Quantized Model Support.\\nSax /times-circle /times-circle/check-circleData Parallelism, Tensor Parallelism (Shoeybi\\net al., 2020), Serves Pax, JAX, and PyTorch mod-\\nels, Slice Serving, Prometheus Metrics.\\nMosec /times-circle /times-circle/check-circleData Parallelism, Continuous Batching (Yu et al.,\\n2022), Rust-based Task Coordinator, Prometheus\\nMetrics.\\nAnother important feature of DeepSpeed-Inference is its deep fusion mechanism, which allows for the fu-\\nsion of operations without the necessity for global synchronization by tiling computations across iteration\\nspace dimensions (Ren et al., 2021; Tang et al., 2021; Li et al., 2022; Lu et al., 2023). Building on this,\\nthe DeepSpeed Model Implementations for Inference (DeepSpeed MII) module introduces Dynamic Split-\\nFuse (Holmes et al., 2024), which leverages continuous batching (Yu et al., 2022) and non-contiguous KV\\ncaches to enable increased occupancy and higher responsivity for LLM serving. It also supports scaling\\nbeyond tensor, data, and pipeline parallelism (3D Parallelism) with Zero Infinity (Rajbhandari et al., 2021)\\nand efficient post-training quantization to Int8 with ZeroQuant (Yao et al., 2022b), Int4 (W4A4) with Wu\\net al. (2023a) and ternary quantization with XTC (Wu et al., 2022b). Furthermore, the introduction of\\nDeepSpeed-Chat (Yao et al., 2023c) adds chat support to the framework. This module focuses on train-\\ning chatbot models across different scales, integrating techniques like Reinforcement Learning from Human\\nFeedback (RLHF) (Griffith et al., 2013) with the DeepSpeed training system. Notably, its integration of the\\n33'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 33, 'page_label': '34'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nZeRO-Offload optimizer (Ren et al., 2021) facilitates training on both CPUs and GPUs, irrespective of their\\nmemory capacities.\\nMegatron. Megatron (Shoeybi et al., 2020) is Nvidia’s efforts to streamline training and serving of LLMs\\nsuch as GPT (Radford et al., 2019) and T5 (Raffel et al., 2020). It is the underlying framework used for\\nNvidia Megatron models (Shoeybi et al., 2020; Narayanan et al., 2021; Korthikanti et al., 2023). Central to\\nMegatron’s design is the strategic decomposition of the model’s tensor operations, distributed across multiple\\nGPUs, to optimize both processing speed and memory utilization, thus enhancing training throughput\\nwithout compromising model quality (Shoeybi et al., 2020). In conjunction with 3D Parallelism, it also\\nimplements sequence parallelism and selective activation recomputation (Korthikanti et al., 2023), which\\nenhances training efficiency. Megatron also uses FasterTransformer (NVIDIA, 2023a) for optimizing the\\ninference process for large Transformer models and handling varying precision modes like FP16 and INT8,\\ncatering to diverse operational needs.\\nColossal-AI. Colossal-AI (Li et al., 2023c) is a framework mainly designed for large-scale distributed train-\\ning (Wang et al., 2023a). Colossal-AI unifies a wide range of parallelism techniques (Xu & You, 2023; Wang\\net al., 2023a; Bian et al., 2021) including sequence parallelism (Li et al., 2023d), auto-parallelism (Liu et al.,\\n2023c), and Zero Redundancy Optimizer (Zhao et al., 2022). It also implements heterogeneous memory\\nmanagement (Fang et al., 2023) through a streamlined API. This integrated approach mitigates the steep\\nlearning curve often associated with orchestrating large-scale training in distributed environments. In ad-\\ndition, the framework integrates several other features like quantization (Frantar et al., 2023; Xiao et al.,\\n2023), RLHF (Griffith et al., 2013), OpenMoE, and mixed-precision training.\\nNanotron. Nanotron (HuggingFace, 2023a), introduced by Huggingface, is anLLM trainingframeworkwith\\na primary focus on providing functionality with minimal overhead. As such, the library only subjectively\\nincorporates the best optimizations required for modern LLM training requirements. Some highlights are\\nits implementation of tensor, data, and pipeline parallelism with one-forward-one-backward pipeline engine,\\nZeRO-1 optimizer (Zhao et al., 2022), FP32 gradient accumulation, parameter tying/sharding and spectral\\nµTransfer parametrization (Yang et al., 2022) for scaling up neural networks. Nanotron also incorporates\\nDoReMi (Xie et al., 2023a) to further speed up training.\\nMegaBlocks. Developed by Databricks, MegaBlocks (Gale et al., 2023) is an LLM training framework for\\ntraining Mixture-of-Experts (MoE) models. The core of MegaBlocks is dropless-MoE, a reformulation of\\nMoE in terms of block-sparse operations, that avoids token dropping without sacrificing hardware efficiency.\\nThis design simplifies and accelerates training as it does not require the capacity factor as a hyper-parameter.\\nFairScale. Developed by Meta, FairScale (FairScale authors, 2021) is an extension library to PyTorch,\\ndedicated to high-performance and large-scale training. As a highlight, FairScale uses Fully Sharded Data\\nParallel (FSDP) (Zhao et al., 2023c) as the preferred method for scaling the training operations of large\\nneural networks. It uses AdaScale optimizer (Johnson et al., 2020) as its distributed optimizer.\\nPax. Developed by Google, Pax (Google, 2023a) is a JAX-based distributed training framework. Pax has\\nbeen used to train PaLM-2 (Anil et al., 2023) and Bard (Hsiao et al., 2023). It targets scalability and\\nhas reference examples for large model training, including across modalities (e.g., text, vision, speech). It\\nsupports data and tensor parallelism, and is heavily integrated with JAX and uses many libraries in the JAX\\necosystem. Several key components Pax contains include SeqIO to handle sequential data processing, Optax\\nfor optimization, Fiddle for configuration, Orbax for checkpointing, PyGLove for automatic differentiation,\\nand Flax for creating high-performance neural networks.\\nComposer. Composer (MosaicML, 2023a) is an LLM framework designed by Mosaic ML. It has been used\\nto train Mosaic ML’s MPT 7B and MPT 30B models and Replit’s Code V-1.5 3B. The framework is built\\non top of PyTorch and provides a collection of acceleration methods that users can incorporate into their\\ntraining loops or use with the Composer trainer. Composer supports FSDP for efficient parallelism, elastic\\nshared checkpointing for robust intermittent training, and a dataset streaming implementation allowing the\\ndownload of datasets from cloud blob storage on the fly during training. Composer also provides a functional\\nAPI for integrating methods directly into its training loops, as well as a Trainer API which automatically\\nimplements a PyTorch-based training loop, reducing the workload for ML developers.\\n34'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 34, 'page_label': '35'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nOpenLLM. OpenLLM (Pham et al., 2023) was made by BentoML to serve and fine-tune LLMs within\\nproduction environments. Anchored within the BentoML ecosystem, OpenLLM makes it easy to self-host\\nLLMs and integrate them with other cloud services. OpenLLM emphasizes on flexibility, SOTA LLM\\nsupport, and streamlined APIs for self-hosting. Recognizing the diverse needs of production environments,\\nOpenLLM supports the automatic generation of docker images. It also supports serving models as serverless\\nendpoints using BentoML’s cloud platform. OpenLLM further integrates advanced quantization techniques\\n(GPTQ (Frantar et al., 2023), AWQ (Lin et al., 2023), SqueezeLLM (Kim et al., 2024), SpQR (Dettmers\\net al., 2024), LLM.int8 (Dettmers et al., 2022)) for efficient inference. OpenLLM’s design also incorporates\\nrobust monitoring with Prometheus metrics and logging tools, ensuring that operational insights are readily\\navailable for performance tuning and troubleshooting.\\nLLM Foundry. LLM Foundry (MosaicML, 2023b) is a library developed by MosaicML for fine-tuning,\\nevaluating, and serving LLMs. It supports distributed inference via FSDP and continuous batching (Yu\\net al., 2022) for efficient serving. It also has cloud integration with the MosaicML platform.\\nvLLM. vLLM (Kwon et al., 2023) is an open-source library for LLM inference and serving. It adopts a\\ndifferent design in how KV cache is stored in memory. Central to this design is PagedAttention, a mechanism\\nthat segments the attention key and value (KV) cache for a set number of tokens. Unlike contiguous\\nspace storage, PagedAttention’s blocks for the KV cache are stored flexibly, similar to the virtual memory\\nmanagement in operating systems. This facilitates memory sharing at a block level across various sequences\\ntiedtothesamerequestorevendifferentrequests, thusenhancingmemorymanagementefficiencyinhandling\\nattention mechanisms. Hence, vLLM can reduce the total memory usage when using complex decoding\\ntechniques such as parallel sampling and beam search as the memory blocks can be shared across different\\ncandidate samples. It also allows on-demand buffer allocation, while also eliminating external fragmentation\\nas the blocks are uniformly sized. Furthermore, vLLM incorporates safeguards that prevent GPU memory\\noverflow due to the increasing size of KV cache by evicting and recovering blocks as needed via swapping\\nand recomputation. vLLM also supports Multi-LoRA (Wang et al., 2023g), continuous batching (Yu et al.,\\n2022) and quantization (GPTQ (Frantar et al., 2023), AWQ (Lin et al., 2023) and SqueezeLLM (Kim et al.,\\n2024)). Lastly, it implements efficient kernels for both Nvidia and AMD GPUs.\\nTensorRT-LLM.TensorRT-LLM (NVIDIA, 2023b) is a streamlined library to serve LLMs. Built on top of\\nthe TensorRT engine, TensorRT-LLM integrates optimized kernels from FasterTransformer (Timonin et al.,\\n2022) and employs tensor parallelism, facilitating efficient inference at scale across multiple GPUs and servers\\nwithout necessitating developer intervention or model changes. It also integrates seamlessly with the Nvidia\\nTriton Inference Server for serving LLMs. Additionally, it offers features like tensor parallelism, continuous\\nbatching (Yu et al., 2022), and PagedAttention (Kwon et al., 2023) as well as supports a wide range of\\nquantization modes and techniques.\\nText-Generation-Inference (TGI).Text-Generation-Inference (TGI) (HuggingFace, 2023b) is a high-\\nperformance LLM serving library developed by Huggingface and is used to power Hugging Chat. TGI\\nsupports a large variety of LLMs, and offers a wide range of features like tensor parallelism, continuous\\nbatching (Yu et al., 2022), efficient attention mechanisms like FlashAttention (Dao et al., 2022) and Page-\\ndAttention (Kwon et al., 2023), and quantization (BitsAndBytes (Dettmers, 2023), GPTQ (Frantar et al.,\\n2023)) support.\\nRayLLM.RayLLM (Ray Project, 2023) is an LLM serving framework as part of the Ray ecosystem (Moritz\\net al., 2018). Built with Ray Serve and the Ray library developed by AnyScale, RayLLM eases LLM\\nserving in multi-GPU and multi-node systems. At the core of RayLLM is the leveraging of Ray’s inherent\\ndistributed computing capabilities. RayLLM integrates Ray’s distributed task scheduling and execution\\nmechanisms, ensuring that LLM tasks are efficiently distributed across available resources. RayLLM also\\nsimplifies adding new LLMs for custom use cases, and offers auto-scaling support and high-performance\\nfeatures like continuous batching (Yu et al., 2022), quantization (GPTQ (Frantar et al., 2023), AWQ (Lin\\net al., 2023), SqueezeLLM (Kim et al., 2024)), and monitoring via Prometheus metrics. It also comes with\\nadvanced monitoring support as well which includes a CLI and a web frontend that can be used to compare\\nand rank models and get cost and latency estimates.\\n35'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 35, 'page_label': '36'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nMLC LLM.MLC LLM (Machine Learning Compilation for Large Language Models) (MLC-LLM, 2023)\\nallows individuals to develop, optimize, and deploy LLMs on a wide range of platforms such as mobile phones\\nand web browsers. Central to MLC LLM is its focus on machine learning compilation techniques. MLC-\\nLLM compiles LLMs and deploys them in a process that is inherently tailored to the specific capabilities\\nand constraints of each platform and hardware (Chen et al., 2018; Shao et al., 2022; Feng et al., 2023).\\nThis platform-native approach ensures that LLMs are not only efficient but also highly optimized for the\\nplatforms in which they operate.\\nSax. Sax (Google, 2023b) is a platform designed by Google for serving Pax, JAX, and PyTorch models for\\ninference tasks. It supports distributed inference with tensor and data parallelism. It also integrates easily\\nwith Google Cloud and cloud monitoring with Prometheus metrics.\\nMosec. Mosec (Yang et al., 2021) is a serving framework built to streamline the serving of machine learning\\nmodels into backend services and microservices. Mosec’s key features include continuous batching (Yu\\net al., 2022), pipelined stages for handling mixed workloads, and essential cloud features like model warmup,\\ngraceful shutdown, and Prometheus monitoring metrics, making it easily manageable by Kubernetes or other\\ncontainer systems.\\n5 Concluding Remarks\\nIn this survey, we provide a systematic review of efficient LLMs, an important area of research aimed at\\ndemocratizing LLMs. We start with motivating the necessity for efficient LLMs. Guided by a taxonomy, we\\nreview algorithm-level and system-level efficient techniques for LLMs from model-centric and data-centric\\nperspectives respectively. Furthermore, we review LLM frameworks with specific optimizations and features\\ncrucial for efficient LLMs. We believe that efficiency will play an increasingly important role in LLMs and\\nLLMs-oriented systems. We hope this survey could enable researchers and practitioners to quickly get started\\nin this field and act as a catalyst to inspire new research on efficient LLMs.\\n6 Acknowledgement\\nWe would like to thank the action editor Greg Durrett and anonymous reviewers of Transactions on Machine\\nLearning Research for their helpful and constructive comments.\\nReferences\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo\\nAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Bal-\\naji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine,\\nGabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-\\nLuisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Camp-\\nbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis\\nChantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey\\nChu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas\\nDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien\\nEcoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston\\nForte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh,\\nRapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,\\nShixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Jo-\\nhannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny\\nHsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger\\nJiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser,\\nAli Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook\\nKim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz\\nKondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael\\n36'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 36, 'page_label': '37'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nLampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,\\nStephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini,\\nSam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob Mc-\\nGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok\\nMehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa,\\nDaniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Ra-\\njeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub\\nPachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy\\nParparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,\\nMichael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong,\\nTolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya\\nRamesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,\\nNick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David\\nSchnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,\\nPranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Ben-\\njamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever,\\nJie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre-\\nston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,\\nChelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,\\nCJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,\\nClemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu,\\nKai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,\\nMarvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4\\ntechnical report, 2023,arXiv preprint arXiv:2303.08774.URL http://arxiv.org/abs/2303.08774.\\nRishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and\\nOlivier Bachem. Generalized knowledge distillation for auto-regressive language models. InThe Twelfth\\nInternational Conference on Learning Representations, 2024. URL https://openreview.net/forum?i\\nd=3zKtaqxLhW.\\nArash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen Gou, Phil Blunsom, Ahmet\\nÜstün, and Sara Hooker. Intriguing properties of quantization at scale. InThirty-seventh Conference on\\nNeural Information Processing Systems, 2023. URLhttps://openreview.net/forum?id=IYe8j7Gy8f.\\nJoshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai.\\nGQA: Training generalized multi-query transformer models from multi-head checkpoints. InProceedings\\nof the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore, 2023. URL\\nhttps://aclanthology.org/2023.emnlp-main.298.\\nSilas Alberti, Niclas Dern, Laura Thesing, and Gitta Kutyniok. Sumformer: Universal approximation for\\nefficient transformers. In Proceedings of 2nd Annual Workshop on Topology, Algebra, and Geometry in\\nMachine Learning (TAG-ML), volume 221, 2023. URLhttps://proceedings.mlr.press/v221/alber\\nti23a.html.\\nReza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng,\\nOlatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, and Yuxiong He. Deepspeed-inference: En-\\nabling efficient inference of transformer models at unprecedented scale. InProceedings of the International\\nConference on High Performance Computing, Networking, Storage and Analysis, Dallas, Texas, 2022. URL\\nhttps://dl.acm.org/doi/abs/10.5555/3571885.3571946.\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey,\\nYanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson,\\nSebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Jun-\\nwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks,\\nMichele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clé-\\nment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer,\\n37'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 37, 'page_label': '38'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nVlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lu-\\ncas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey\\nHui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,\\nMaxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li,\\nYaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,\\nAroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric\\nNi, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan\\nQiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar\\nSamuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha\\nValter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John\\nWieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report,\\n2023, arXiv preprint arXiv:2305.10403.URL http://arxiv.org/abs/2305.10403.\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman, Xian Li, Shuohui Chen,\\nHalil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo, Jeffrey Wang,\\nLuke Zettlemoyer, Mona Diab, Zornitsa Kozareva, and Veselin Stoyanov. Efficient large scale language\\nmodeling with mixtures of experts. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.),Pro-\\nceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11699–11732,\\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi:\\n10.18653/v1/2022.emnlp-main.804. URL https://aclanthology.org/2022.emnlp-main.804.\\nThomas Bachlechner, Bodhisattwa Prasad Majumder, Henry Mao, Gary Cottrell, and Julian McAuley.\\nRezero is all you need: fast convergence at large depth. InProceedings of the Thirty-Seventh Conference\\non Uncertainty in Artificial Intelligence, volume 161, 2021. URLhttps://proceedings.mlr.press/v1\\n61/bachlechner21a.html.\\nTrapit Bansal, Salaheddin Alzubi, Tong Wang, Jay-Yoon Lee, and Andrew McCallum. Meta-adapters:\\nParameter efficient few-shot fine-tuning through meta-learning. InInternational Conference on Automated\\nMachine Learning, Baltimore, US, 2022. URLhttps://openreview.net/forum?id=BCGNf-prLg5.\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020,\\narXiv preprint arXiv:2004.05150.URL http://arxiv.org/abs/2004.05150.\\nAmanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range trans-\\nformers with unlimited length input. In Thirty-seventh Conference on Neural Information Processing\\nSystems, 2023. URLhttps://openreview.net/forum?id=lJWUJWLCJo.\\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna\\nGajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts:\\nSolving elaborate problems with large language models.Proceedings of the AAAI Conference on Artificial\\nIntelligence, 38, 2024. URLhttps://ojs.aaai.org/index.php/AAAI/article/view/29720.\\nZhengda Bian, Qifan Xu, Boxiang Wang, and Yang You. Maximizing parallelism in distributed training for\\nhuge neural networks, 2021,arXiv preprint arXiv:2105.14450.URL http://arxiv.org/abs/2105.14450.\\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\\nConnor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit,\\nLaria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-NeoX-20B: An open-source\\nautoregressive language model. In Proceedings of BigScience Episode #5 – Workshop on Challenges &\\nPerspectives in Creating Large Language Models, virtual+Dublin, 2022. URLhttps://aclanthology.o\\nrg/2022.bigscience-1.9.\\nbloc97. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-\\ntuning and minimal perplexity degradation.https://www.reddit.com/r/LocalLLaMA/comments/14lz7\\nj5/ntkaware_scaled_rope_allows_llama_models_to_have/, 2023.\\n38'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 38, 'page_label': '39'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming the challenges\\nof efficient transformer quantization. In Proceedings of the 2021 Conference on Empirical Methods in\\nNatural Language Processing, Online and Punta Cana, Dominican Republic, 2021. URLhttps://aclant\\nhology.org/2021.emnlp-main.627.\\nTomBrown, BenjaminMann, NickRyder, MelanieSubbiah, JaredDKaplan, PrafullaDhariwal, ArvindNee-\\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-\\npher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\\nare few-shot learners. In Advances in Neural Information Processing Systems, volume 33, 2020. URL\\nhttps://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac14\\n2f64a-Paper.pdf.\\nAydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. Recurrent memory transformer. InAdvances in Neural\\nInformation Processing Systems, 2022. URLhttps://openreview.net/forum?id=Uynr3iPhksa.\\nNeil Burgess, Jelena Milanovic, Nigel Stephens, Konstantinos Monachopoulos, and David Mansell. Bfloat16\\nprocessing for neural networks. InIEEE Symposium on Computer Arithmetic, Kyoto, 2019. URLhttps:\\n//ieeexplore.ieee.org/document/8877390.\\nFederico Busato and Jeff Pool. Exploiting nvidia ampere structured sparsity with cusparselt. https:\\n//developer.nvidia.com/blog/exploiting-ampere-structured-sparsity-with-cusparselt , 2020.\\nLucas Caccia, Edoardo Ponti, Zhan Su, Matheus Pereira, Nicolas Le Roux, and Alessandro Sordoni. Multi-\\nhead adapter routing for cross-task generalization. InThirty-seventh Conference on Neural Information\\nProcessing Systems, 2023. URLhttps://openreview.net/forum?id=qcQhBli5Ho.\\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao.\\nMedusa: Simple llm inference acceleration framework with multiple decoding heads, 2024,arXiv preprint\\narXiv:2401.10774. URL http://arxiv.org/abs/2401.10774.\\nYihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun. Instruction mining: When data mining meets large\\nlanguage model finetuning, 2023,arXiv preprint arXiv:2307.06290.URL http://arxiv.org/abs/2307\\n.06290.\\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\\nCunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing\\nXie. A survey on evaluation of large language models. ACM Transactions on Intelligent Systems and\\nTechnology, 15, 2024. URLhttps://doi.org/10.1145/3641289.\\nJerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit quantization of large\\nlanguagemodelswithguarantees. In Thirty-seventh Conference on Neural Information Processing Systems,\\n2023. URL https://openreview.net/forum?id=xrk9g5vcXR.\\nBeidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Ré. Scatterbrain: Unifying\\nsparse and low-rank attention. Advances in Neural Information Processing Systems, 34, 2021a. URL\\nhttps://openreview.net/forum?id=SehIKudiIo1.\\nChang Chen, Min Li, Zhihua Wu, Dianhai Yu, and Chao Yang. TA-moe: Topology-aware large scale\\nmixture-of-expert training. InAdvances in Neural Information Processing Systems, 2022a. URLhttps:\\n//openreview.net/forum?id=FRDiimH26Tr.\\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John\\nJumper. Accelerating large language model decoding with speculative sampling, 2023a,arXiv preprint\\narXiv:2302.01318. URL http://arxiv.org/abs/2302.01318.\\n39'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 39, 'page_label': '40'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nCheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang, Yujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen, Zhiyuan\\nLiu, and Qun Liu. bert2BERT: Towards reusable pretrained language models. In Proceedings of the\\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin,\\nIreland, 2022b. URLhttps://aclanthology.org/2022.acl-long.151.\\nGuanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing. CLEX: Continuous length\\nextrapolation for large language models. InThe Twelfth International Conference on Learning Represen-\\ntations, 2024a. URLhttps://openreview.net/forum?id=wXpSidPpc5.\\nHao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo\\nZhao. Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning,\\n2023b, arXiv preprint arXiv:2305.09246.URL http://arxiv.org/abs/2305.09246.\\nHoward Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. Walking down the memory\\nmaze: Beyond context limit through interactive reading, 2023c,arXiv preprint arXiv:2310.05029.URL\\nhttp://arxiv.org/abs/2310.05029.\\nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vi-\\njay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training a better alpaca\\nwith fewer data. In The Twelfth International Conference on Learning Representations, 2024b. URL\\nhttps://openreview.net/forum?id=FdVXgSJhvz.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,\\nMichael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,\\nMikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Fe-\\nlipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-\\nVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir\\nBalaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam,\\nVedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie\\nMayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech\\nZaremba. Evaluating large language models trained on code, 2021b,arXiv preprint arXiv:2107.03374.\\nURL http://arxiv.org/abs/2107.03374.\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large\\nlanguage models via positional interpolation, 2023d,arXiv preprint arXiv:2306.15595.URL http://arxi\\nv.org/abs/2306.15595.\\nTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan,\\nLeyuan Wang, Yuwei Hu, Luis Ceze, Carlos Guestrin, and Arvind Krishnamurthy. TVM: An automated\\nEnd-to-End optimizing compiler for deep learning. InUSENIX Symposium on Operating Systems Design\\nand Implementation (OSDI), Carlsbad, CA, 2018. URLhttps://www.usenix.org/conference/osdi18\\n/presentation/chen.\\nTianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, and Luming Liang. Lorashear: Efficient large\\nlanguage model structured pruning and knowledge recovery, 2023e, arXiv preprint arXiv:2310.18356.\\nURL http://arxiv.org/abs/2310.18356.\\nWuyang Chen, Yanqi Zhou, Nan Du, Yanping Huang, James Laudon, Zhifeng Chen, and Claire Cui. Life-\\nlong language pretraining with distribution-specialized experts. InProceedings of the 40th International\\nConference on Machine Learning, Honolulu, Hawaii, USA, 2023f. URLhttps://dl.acm.org/doi/10.55\\n55/3618408.3618621.\\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang\\nLuong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V Le. Symbolic discovery of optimization algorithms. In\\nThirty-seventh Conference on Neural Information Processing Systems, 2023g. URLhttps://openreview\\n.net/forum?id=ne6zeqLFCZ.\\n40'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 40, 'page_label': '41'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-\\ncontext tuning. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers), Dublin, Ireland, 2022c. URLhttps://aclanthology.org/2022.acl-long.53.\\nYongrui Chen, Haiyun Jiang, Xinting Huang, Shuming Shi, and Guilin Qi. Tegit: Generating high-quality\\ninstruction-tuning data with text-grounded task design, 2023h,arXiv preprint arXiv:2309.05447. URL\\nhttp://arxiv.org/abs/2309.05447.\\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora:\\nEfficient fine-tuning of long-context large language models, 2023i,arXiv preprint arXiv:2309.12307.URL\\nhttp://arxiv.org/abs/2309.12307.\\nZeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, and Kyle Richardson. DISCO: Distilling\\ncounterfactuals with large language models. InProceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023j. URLhttps://aclant\\nhology.org/2023.acl-long.302.\\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress\\ncontexts. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\\nSingapore, 2023. URLhttps://aclanthology.org/2023.emnlp-main.232.\\nZewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj,\\nXia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation collapse of sparse mixture\\nof experts. In Advances in Neural Information Processing Systems, 2022. URL https://openreview.n\\net/forum?id=mWaYC6CZf5.\\nYew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, and Lidong Bing. Contrastive chain-of-\\nthought prompting, 2023,arXiv preprint arXiv:2311.09277.URL http://arxiv.org/abs/2311.09277.\\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse trans-\\nformers, 2019,arXiv preprint arXiv:1904.10509.URL http://arxiv.org/abs/1904.10509.\\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos,\\nPeter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling\\nfor proteins via linearly scalable long-context transformers, 2020,arXiv preprint arXiv:2006.03555.URL\\nhttp://arxiv.org/abs/2006.03555.\\nKrzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger,\\nLucy J Colwell, and Adrian Weller. Rethinking attention with performers, 2021. URLhttps://openre\\nview.net/forum?id=Ua6zuk0WRH.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,\\nSasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar\\nPrabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\\nIsard, GuyGur-Ari, PengchengYin, TojuDuke, AnselmLevskaya, SanjayGhemawat, SunipaDev, Henryk\\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,\\nBrennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022,\\narXiv preprint arXiv:2204.02311.URL http://arxiv.org/abs/2204.02311.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha\\n41'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 41, 'page_label': '42'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nValter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun\\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and\\nJason Wei. Scaling instruction-finetuned language models, 2022,arXiv preprint arXiv:2210.11416.URL\\nhttp://arxiv.org/abs/2210.11416.\\nJae-Won Chung, Yile Gu, Insu Jang, Luoxi Meng, Nikhil Bansal, and Mosharaf Chowdhury. Perseus:\\nRemoving energy bloat from large model training, 2023,arXiv preprint arXiv:2312.06902.URL http:\\n//arxiv.org/abs/2312.06902.\\nDamai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, and Furu Wei. StableMoE: Stable\\nrouting strategy for mixture of experts. InProceedings of the 60th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), Dublin, Ireland, 2022. URLhttps://aclantholo\\ngy.org/2022.acl-long.489.\\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-\\nXL: Attentive language models beyond a fixed-length context. InProceedings of the 57th Annual Meeting\\nof the Association for Computational Linguistics, Florence, Italy, 2019. URLhttps://aclanthology.o\\nrg/P19-1285.\\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. InThe Twelfth\\nInternational Conference on Learning Representations, 2024. URL https://openreview.net/forum?i\\nd=mZn2Xyh9Ec.\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-\\nefficient exact attention with io-awareness.Advances in Neural Information Processing Systems, 35, 2022.\\nURL https://openreview.net/forum?id=H4DqfPSibmx.\\nTri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference.\\nhttps://pytorch.org/blog/flash-decoding/, 2023.\\nSoham De and Samuel L. Smith. Batch normalization biases residual blocks towards the identity function\\nin deep networks. InProceedings of the 34th International Conference on Neural Information Processing\\nSystems, 2020. URLhttps://dl.acm.org/doi/abs/10.5555/3495724.3497400.\\nTim Dettmers. Bitsandbytes.https://github.com/TimDettmers/bitsandbytes, 2023.\\nTimDettmers, MikeLewis, YounesBelkada, andLukeZettlemoyer. GPT3.int8(): 8-bitmatrixmultiplication\\nfor transformers at scale. In Advances in Neural Information Processing Systems, 2022. URL https:\\n//openreview.net/forum?id=dXiGWqBoxaD.\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient finetuning of\\nquantized LLMs. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL\\nhttps://openreview.net/forum?id=OUIFPHEgJU.\\nTim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos,\\nAlexander Borzunov, Torsten Hoefler, and Dan Alistarh. SpQR: A sparse-quantized representation for\\nnear-lossless LLM weight compression. InThe Twelfth International Conference on Learning Representa-\\ntions, 2024. URLhttps://openreview.net/forum?id=Q1u25ahSuy.\\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and\\nFuru Wei. Longnet: Scaling transformers to 1,000,000,000 tokens, 2023a,arXiv preprint arXiv:2307.02486.\\nURL http://arxiv.org/abs/2307.02486.\\nRuomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan,\\nQingwei Lin, and Dongmei Zhang. Everything of thoughts: Defying the law of penrose triangle for thought\\ngeneration, 2023b,arXiv preprint arXiv:2311.04254.URL http://arxiv.org/abs/2311.04254.\\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li,\\nand Zhifang Sui. A survey on in-context learning, 2023,arXiv preprint arXiv:2301.00234.URL http:\\n//arxiv.org/abs/2301.00234.\\n42'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 42, 'page_label': '43'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,\\nYanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou,\\nTao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju\\nDuke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. GLaM: Efficient\\nscaling of language models with mixture-of-experts. InProceedings of the 39th International Conference\\non Machine Learning, 2022. URLhttps://proceedings.mlr.press/v162/du22c.html.\\nFeyza Duman Keles, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. On the computational com-\\nplexity of self-attention. In Proceedings of The 34th International Conference on Algorithmic Learning\\nTheory, 2023. URLhttps://proceedings.mlr.press/v201/duman-keles23a.html.\\nLance Eliot. Generative pre-trained transformers (gpt-3) pertain to ai in the law, 2021. URL http:\\n//dx.doi.org/10.2139/ssrn.3974887.\\nFacebook AI Research (FAIR). fairseq: Fp16 optimizer - line 468.https://github.com/facebookresearc\\nh/fairseq/blob/main/fairseq/optim/fp16_optimizer.py, 2023.\\nFairScale authors. Fairscale: A general purpose modular pytorch library for high performance and large\\nscale training. https://github.com/facebookresearch/fairscale, 2021.\\nJiaruiFang, ZilinZhu, ShengguiLi, HuiSu, YangYu, JieZhou, andYangYou. Paralleltrainingofpre-trained\\nmodels via chunk-based dynamic memory management.IEEE Transactions on Parallel and Distributed\\nSystems, 34(1):304–315, 2023. doi: 10.1109/TPDS.2022.3219819. URLhttps://ieeexplore.ieee.org/\\ndocument/9940581.\\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: scaling to trillion parameter models\\nwith simple and efficient sparsity.Journal of Machine Learning Research, 23, 2022. URLhttps://dl.a\\ncm.org/doi/abs/10.5555/3586589.3586709.\\nSiyuan Feng, Bohan Hou, Hongyi Jin, Wuwei Lin, Junru Shao, Ruihang Lai, Zihao Ye, Lianmin Zheng,\\nCody Hao Yu, Yong Yu, and Tianqi Chen. Tensorir: An abstraction for automatic tensorized program\\noptimization. InACM International Conference on Architectural Support for Programming Languages and\\nOperating Systems, 2023. URLhttps://doi.org/10.1145/3575693.3576933.\\nElias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quan-\\ntization and pruning. In Advances in Neural Information Processing Systems, New Orleans, Louisiana,\\n2022. URL https://openreview.net/forum?id=ksVGCOlOEba.\\nElias Frantar and Dan Alistarh. SparseGPT: Massive language models can be accurately pruned in one-\\nshot. In Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/frantar23a.html.\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for gen-\\nerative pre-trained transformers. InThe Eleventh International Conference on Learning Representations,\\n2023. URL https://openreview.net/forum?id=tcbBPnfwxS.\\nDaniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry\\nhungry hippos: Towards language modeling with state space models. In The Eleventh International\\nConference on Learning Representations, 2023a. URLhttps://openreview.net/forum?id=COZDy0WYGg.\\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models\\ntowards multi-step reasoning. InProceedings of the 40th International Conference on Machine Learning,\\nHonolulu, Hawaii, 2023b. URLhttps://proceedings.mlr.press/v202/fu23d.html.\\nYichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking the sequential dependency of llm inference\\nusing lookahead decoding, 2023c. URLhttps://lmsys.org/blog/2023-11-21-lookahead-decoding/ .\\n43'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 43, 'page_label': '44'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTrevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with\\nmixture-of-experts. Proceedings of Machine Learning and Systems, 5, 2023. URLhttps://proceeding\\ns.mlsys.org/paper_files/paper/2023/hash/5a54f79333768effe7e8927bcccffe40-Abstract-mlsys\\n2023.html.\\nTao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context\\ncompression in a large language model. InThe Twelfth International Conference on Learning Represen-\\ntations, 2024. URLhttps://openreview.net/forum?id=uREj4ZuGJE.\\nMichael Glass, Alfio Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, G P Shrivatsa Bhargav, Dinesh\\nGarg, and Avi Sil. Span selection pre-training for question answering. InProceedings of the 58th Annual\\nMeeting of the Association for Computational Linguistics, 2020. URLhttps://aclanthology.org/202\\n0.acl-main.247.\\nLinyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of BERT\\nby progressively stacking. In Proceedings of the 36th International Conference on Machine Learning,\\nvolume 97, Long Beach, California, 2019. URLhttps://proceedings.mlr.press/v97/gong19a.html.\\nGoogle. Pax: A jax-based machine learning framework for large scale models.https://github.com/googl\\ne/paxml, 2023a. URLhttps://github.com/google/paxml. GitHub repository.\\nGoogle. Sax. https://github.com/google/saxml, 2023b.\\nJianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey.\\nInternational Journal of Computer Vision, 129, 2021. URLhttps://doi.org/10.1007/s11263-021-0\\n1453-z.\\nShane Griffith, Kaushik Subramanian, Jonathan Scholz, Charles L Isbell, and Andrea L Thomaz. Policy\\nshaping: Integrating human feedback with reinforcement learning. In Advances in Neural Information\\nProcessing Systems, volume 26, 2013. URLhttps://proceedings.neurips.cc/paper%5Ffiles/paper\\n/2013/file/e034fb6b66aacc1d48f445ddfb08da98-Paper.pdf.\\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023,arXiv\\npreprint arXiv:2312.00752.URL http://arxiv.org/abs/2312.00752.\\nAlbert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state\\nspaces. In International Conference on Learning Representations, 2022a. URL https://openreview.n\\net/forum?id=uYLFoz1vlAC.\\nXiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han. On the transformer growth\\nfor progressive BERT training. In Proceedings of the 2021 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Technologies, Online, 2021. URL\\nhttps://aclanthology.org/2021.naacl-main.406.\\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. PPT: Pre-trained prompt tuning for few-shot learning.\\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), Dublin, Ireland, 2022b. URLhttps://aclanthology.org/2022.acl-long.576.\\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. MiniLLM: Knowledge distillation of large language\\nmodels. In The Twelfth International Conference on Learning Representations, 2024. URL https:\\n//openreview.net/forum?id=5h0qf7IBZZ.\\nCong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo,\\nand Yuhao Zhu. Olive: Accelerating large language models via hardware-friendly outlier-victim pair\\nquantization. In Proceedings of the 50th Annual International Symposium on Computer Architecture,\\n2023. URL https://doi.org/10.1145/3579371.3589038.\\nAhan Gupta, Yueming Yuan, Yanqi Zhou, and Charith Mendis. Flurka: Fast fused low-rank & kernel\\nattention, 2023,arXiv preprint arXiv:2306.15799.URL http://arxiv.org/abs/2306.15799.\\n44'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 44, 'page_label': '45'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nAnkit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state\\nspaces. In Advances in Neural Information Processing Systems, 2022. URLhttps://openreview.net/f\\norum?id=RjS0j6tsSrf.\\nInsu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David Woodruff, and Amir Zandieh. Hyperat-\\ntention: Long-context attention in near-linear time. InThe Twelfth International Conference on Learning\\nRepresentations, 2024. URLhttps://openreview.net/forum?id=Eh0Od2BJIM.\\nJiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. Fastmoe: A fast mixture-of-\\nexpert training system, 2021,arXiv preprint arXiv:2103.13262.URL http://arxiv.org/abs/2103.132\\n62.\\nJiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, and Qin Li. Fastermoe:\\nmodeling and optimizing training of large-scale dynamic pre-trained models. InProceedings of the 27th\\nACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 2022a. URL https:\\n//dl.acm.org/doi/10.1145/3503221.3508418.\\nKai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. A Survey of\\nLarge Language Models for Healthcare: from Data, Technology, and Applications to Accountability and\\nEthics, 2023,arXiv preprint arXiv:2310.05694.URL http://arxiv.org/abs/2310.05694.\\nShwai He, Liang Ding, Daize Dong, Miao Zhang, and Dacheng Tao. Sparseadapter: An easy approach for\\nimproving the parameter-efficiency of adapters. InFindings of EMNLP, 2022b. URLhttps://aclantho\\nlogy.org/2022.findings-emnlp.160.\\nNamgyuHo, LauraSchmid, andSe-YoungYun. Large languagemodelsarereasoningteachers. In Proceedings\\nof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\\nToronto, Canada, 2023. URLhttps://aclanthology.org/2023.acl-long.830.\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\\nDiego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,\\nKatherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen\\nSimonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. An empirical analysis of\\ncompute-optimal large language model training. InAdvances in Neural Information Processing Systems,\\n2022. URL https://openreview.net/forum?id=iBBcRUlOAPR.\\nConnor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam Rajbhandari,\\nReza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, and Yuxiong He. Deepspeed-\\nfastgen: High-throughput text generation for llms via mii and deepspeed-inference, 2024,arXiv preprint\\narXiv:2401.08671. URL http://arxiv.org/abs/2401.08671.\\nKe Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, and\\nYu Wang. Flashdecoding++: Faster large language model inference on gpus, 2023, arXiv preprint\\narXiv:2311.01282. URL http://arxiv.org/abs/2311.01282.\\nOr Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction induction: From few examples\\nto natural language task descriptions. InProceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023. URLhttps://aclantholo\\ngy.org/2023.acl-long.108.\\nColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt\\nKeutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache\\nquantization, 2024,arXiv preprint arXiv:2401.18079.URL http://arxiv.org/abs/2401.18079.\\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Ges-\\nmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Pro-\\nceedings of the 36th International Conference on Machine Learning, volume 97, 2019. URL https:\\n//proceedings.mlr.press/v97/houlsby19a.html.\\n45'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 45, 'page_label': '46'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nSissie Hsiao, Yury Pinsky, and Sundar Pichai. Bard: Google’s generative language model.https://blog.g\\noogle/products/search/bard-updates/, 2023.\\nCheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Kr-\\nishna, Chen-YuLee, andTomasPfister. Distillingstep-by-step! outperforminglargerlanguagemodelswith\\nless training data and smaller model sizes. InFindings of the Association for Computational Linguistics:\\nACL 2023, Toronto, Canada, 2023. URLhttps://aclanthology.org/2023.findings-acl.507.\\nYen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language model com-\\npression with weighted low-rank factorization. InInternational Conference on Learning Representations,\\n2022. URL https://openreview.net/forum?id=uPv9Y3gmAI5.\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\\nWeizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on\\nLearning Representations, 2022. URLhttps://openreview.net/forum?id=nZeVKeeFYf9.\\nShengdingHu, NingDing, WeilinZhao, XingtaiLv, ZhenZhang, ZhiyuanLiu, andMaosongSun. OpenDelta:\\nA plug-and-play library for parameter-efficient adaptation of pre-trained models. InProceedings of the 61st\\nAnnual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations),\\nToronto, Canada, 2023a. URLhttps://aclanthology.org/2023.acl-demo.26.\\nZhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and\\nRoy Lee. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large language models.\\nIn Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Singapore,\\n2023b. URL https://aclanthology.org/2023.emnlp-main.319.\\nChengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficient cross-\\ntask generalization via dynamic lora composition, 2023,arXiv preprint arXiv:2307.13269. URL http:\\n//arxiv.org/abs/2307.13269.\\nJie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. In\\nFindings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, 2023. URL\\nhttps://aclanthology.org/2023.findings-acl.67.\\nXiaoShiHuang, FelipePerez, JimmyBa, andMaksimsVolkovs. Improvingtransformeroptimizationthrough\\nbetter initialization. In Proceedings of the 37th International Conference on Machine Learning, volume\\n119, 2020. URLhttps://proceedings.mlr.press/v119/huang20f.html.\\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong\\nLee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant\\nneural networks using pipeline parallelism. InProceedings of the 33rd International Conference on Neural\\nInformation Processing Systems, Red Hook, NY, USA, 2019.\\nYukun Huang, Yanda Chen, Zhou Yu, and Kathleen McKeown. In-context learning distillation: Transferring\\nfew-shot learning ability of pre-trained language models, 2022,arXiv preprint arXiv:2212.10670. URL\\nhttp://arxiv.org/abs/2212.10670.\\nHuggingFace. text-generation-inference. https://github.com/huggingface/nanotron , 2023a. Accessed:\\n2024-05-10.\\nHuggingFace. text-generation-inference. https://github.com/huggingface/text-generation-inferen\\nce, 2023b. Accessed: 2024-05-10.\\nDeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent\\ntransformers. In Advances in Neural Information Processing Systems, 2022. URLhttps://openreview\\n.net/forum?id=uloenYmLCAo.\\nChangho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin\\nJose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, and Yongqiang Xiong. Tutel: Adaptive\\nmixture-of-experts at scale.Proceedings of Machine Learning and Systems, 5, 2023.\\n46'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 46, 'page_label': '47'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nRégis Pierrard Ilyas Moutawwakil. Llm-perf leaderboard.https://huggingface.co/spaces/optimum/ll\\nm-perf-leaderboard, 2023.\\nMaor Ivgi, Uri Shaham, and Jonathan Berant. Efficient long-text understanding with short-text models.\\nTransactions of the Association for Computational Linguistics, 11, 2023. URLhttps://aclanthology.o\\nrg/2023.tacl-1.17.\\nHamish Ivison, Noah A. Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. Data-efficient finetuning using\\ncross-task nearest neighbors. In Findings of the Association for Computational Linguistics, Toronto,\\nCanada, 2023. URLhttps://aclanthology.org/2023.findings-acl.576.\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud,\\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and\\nWilliam El Sayed. Mistral 7b, 2023a,arXiv preprint arXiv:2310.06825.URL http://arxiv.org/abs/23\\n10.06825.\\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.\\nLongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. In\\nICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models, 2024. URL\\nhttps://openreview.net/forum?id=9YvfRrpmyw.\\nYuxin Jiang, Chunkit Chan, Mingyang Chen, and Wei Wang. Lion: Adversarial distillation of proprietary\\nlarge language models. InProceedings of the 2023 Conference on Empirical Methods in Natural Language\\nProcessing, Singapore, 2023b. URLhttps://aclanthology.org/2023.emnlp-main.189.\\nYunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. $s^3$: Increasing GPU utilization during\\ngenerativeinferenceforhigherthroughput. In Thirty-seventh Conference on Neural Information Processing\\nSystems, 2023. URLhttps://openreview.net/forum?id=zUYfbdNl1m.\\nTyler Johnson, Pulkit Agrawal, Haijie Gu, and Carlos Guestrin. AdaScale SGD: A user-friendly algorithm\\nfor distributed training. In Hal Daumé III and Aarti Singh (eds.),Proceedings of the 37th International\\nConference on Machine Learning, volume119of Proceedings of Machine Learning Research, pp.4911–4920.\\nPMLR, 13–18 Jul 2020. URLhttps://proceedings.mlr.press/v119/johnson20a.html.\\nHoyoun Jung and Kyung-Joong Kim. Discrete prompt compression with reinforcement learning, 2023,arXiv\\npreprint arXiv:2308.08758.URL http://arxiv.org/abs/2308.08758.\\nJean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy.\\nChallenges and applications of large language models, 2023,arXiv preprint arXiv:2307.10169.URL http:\\n//arxiv.org/abs/2307.10169.\\nDhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth\\nAvancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang,\\nJongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha\\nSmelyanskiy, Bharat Kaul, and Pradeep Dubey. A study of bfloat16 for deep learning training, 2019,\\narXiv preprint arXiv:1905.12322.URL http://arxiv.org/abs/1905.12322.\\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hyper-\\ncomplex adapter layers. InAdvances in Neural Information Processing Systems, volume 34, New Orleans,\\nLouisiana, 2021. URLhttps://proceedings.neurips.cc/paper%5Ffiles/paper/2021/file/081be9f\\ndff07f3bc808f935906ef70c0-Paper.pdf.\\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are RNNs: Fast\\nautoregressive transformers with linear attention. InProceedings of the 37th International Conference on\\nMachine Learning, volume 119, 2020. URLhttps://proceedings.mlr.press/v119/katharopoulos20\\na.html.\\n47'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 47, 'page_label': '48'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nJeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo\\nLee. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization.\\nIn Thirty-seventh Conference on Neural Information Processing Systems, 2023a. URL https://openre\\nview.net/forum?id=2jUKhUrBxP.\\nMinsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, and Jungwook\\nChoi. Token-scaled logit distillation for ternary weight generative language models. InThirty-seventh\\nConference on Neural Information Processing Systems, 2023b. URLhttps://openreview.net/forum?i\\nd=FUnEkOkodU.\\nSehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami,\\nand Kurt Keutzer. Speculative decoding with big little decoder. InThirty-seventh Conference on Neural\\nInformation Processing Systems, 2023c. URLhttps://openreview.net/forum?id=EfMyf9MC3t.\\nSehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney,\\nand Kurt Keutzer. Squeezellm: Dense-and-sparse quantization, 2024,arXiv preprint arXiv:2306.07629.\\nURL http://arxiv.org/abs/2306.07629.\\nYoung Jin Kim, Rawn Henry, Raffy Fahim, and Hany Hassan Awadalla. Finequant: Unlocking efficiency\\nwith fine-grained weight-only quantization for llms, 2023d,arXiv preprint arXiv:2308.09723.URL http:\\n//arxiv.org/abs/2308.09723.\\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017,arXiv preprint\\narXiv:1412.6980. URL http://arxiv.org/abs/1412.6980.\\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. InInternational\\nConference on Learning Representations, 2020. URLhttps://openreview.net/forum?id=rkgNKkHtvB.\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language\\nmodels are zero-shot reasoners. In Advances in Neural Information Processing Systems, 2022. URL\\nhttps://openreview.net/forum?id=e2TBb5y0yFf.\\nVijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad\\nShoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. In\\nProceedings of Machine Learning and Systems, volume 5, 2023. URLhttps://proceedings.mlsys.or\\ng/paper_files/paper/2023/hash/e851ca7b43815718fbbac8afb2246bf8-Abstract-mlsys2023.html.\\nSiddharth Krishna Kumar. On weight initialization in deep neural networks, 2017, arXiv preprint\\narXiv:1704.08863. URL http://arxiv.org/abs/1704.08863.\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonza-\\nlez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with\\npagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, 2023. URL\\nhttps://doi.org/10.1145/3600006.3613165.\\nChanghun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park. Owq: Lessons learned from ac-\\ntivation outliers for weight quantization in large language models, 2023,arXiv preprint arXiv:2306.02272.\\nURL http://arxiv.org/abs/2306.02272.\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim\\nKrikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation\\nand automatic sharding. In International Conference on Learning Representations, 2021. URL https:\\n//openreview.net/forum?id=qrwe7XHTmYb.\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.\\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. URL\\nhttps://aclanthology.org/2021.emnlp-main.243.\\n48'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 48, 'page_label': '49'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative\\ndecoding. In Proceedings of the 40th International Conference on Machine Learning, 2023. URL\\nhttps://dl.acm.org/doi/10.5555/3618408.3619203.\\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. BASE Layers: Simplifying\\ntraining of large, sparse models. InProceedings of the 38th International Conference on Machine Learning,\\nvolume 139, 2021. URLhttps://proceedings.mlr.press/v139/lewis21a.html.\\nConglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and Yuxiong He. 1-bit lamb:\\nCommunication efficient large-scale large-batch training with lamb’s convergence speed. In IEEE In-\\nternational Conference on High Performance Computing, Data, and Analytics (HiPC) , 2022. URL\\nhttps://ieeexplore.ieee.org/abstract/document/10106313.\\nJiamin Li, Yimin Jiang, Yibo Zhu, Cong Wang, and Hong Xu. Accelerating distributed MoE training and\\ninference with lina. InUSENIX Annual Technical Conference (USENIX ATC), Boston, MA, 2023a. URL\\nhttps://www.usenix.org/conference/atc23/presentation/li-jiamin.\\nLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Symbolic chain-\\nof-thought distillation: Small models can also “think” step-by-step. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada,\\n2023b. URL https://aclanthology.org/2023.acl-long.150.\\nShanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai,\\nYiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli. Functional interpolation for relative positions\\nimproves long context transformers. InThe Twelfth International Conference on Learning Representations,\\n2024a. URL https://openreview.net/forum?id=rR03qFesqk.\\nShenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, and\\nYang You. Colossal-ai: A unified deep learning system for large-scale parallel training. InProceedings of\\nthe 52nd International Conference on Parallel Processing, 2023c. URLhttps://doi.org/10.1145/3605\\n573.3605613.\\nShenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long\\nsequence training from system perspective. InProceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023d. URLhttps://aclant\\nhology.org/2023.acl-long.134.\\nShiyang Li, Jianshu Chen, yelong shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin\\nPeng, Yi Mao, Wenhu Chen, and Xifeng Yan. Explanations from large language models make small\\nreasoners better. InWorkshop on Sustainable AI, 2024b. URLhttps://openreview.net/forum?id=rH\\n8ZUcfL9r.\\nShiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong\\nYang, and Yu Wang. Evaluating quantized large language models, 2024c,arXiv preprint arXiv:2402.18158.\\nURL http://arxiv.org/abs/2402.18158.\\nXiang Li, Yiqun Yao, Xin Jiang, Xuezhi Fang, Xuying Meng, Siqi Fan, Peng Han, Jing Li, Li Du, Bowen\\nQin, Zheng Zhang, Aixin Sun, and Yequan Wang. Flm-101b: An open llm and how to train it with $100k\\nbudget, 2023e,arXiv preprint arXiv:2309.03852.URL http://arxiv.org/abs/2309.03852.\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. InProceedings\\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), 2021. URL https://acla\\nnthology.org/2021.acl-long.353.\\nXiaonan Li and Xipeng Qiu. Finding support examples for in-context learning. InFindings of the Association\\nfor Computational Linguistics: EMNLP 2023, Singapore, 2023. URLhttps://aclanthology.org/202\\n3.findings-emnlp.411.\\n49'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 49, 'page_label': '50'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng\\nQiu. Unified demonstration retriever for in-context learning. InProceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023f. URL\\nhttps://aclanthology.org/2023.acl-long.256.\\nYixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, and Tuo Zhao. LoSparse:\\nStructured compression of large language models based on low-rank and sparse approximation. InPro-\\nceedings of the 40th International Conference on Machine Learning, volume 202, pp. 20336–20350, 23–29\\nJul 2023g. URLhttps://proceedings.mlr.press/v202/li23ap.html.\\nYixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, and Tuo Zhao. Loftq:\\nLoRA-fine-tuning-aware quantization for large language models. InThe Twelfth International Conference\\non Learning Representations, 2024d. URLhttps://openreview.net/forum?id=LzPWWPAdY4.\\nChen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, and Tuo Zhao. Less is more: Task-\\naware layer-wise distillation for language model compression. In Proceedings of the 40th International\\nConference on Machine Learning, volume 202, 2023. URLhttps://proceedings.mlr.press/v202/lia\\nng23j.html.\\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang Gan, and Song Han.\\nAwq: Activation-aware weight quantization for llm compression and acceleration, 2023,arXiv preprint\\narXiv:2306.00978. URL http://arxiv.org/abs/2306.00978.\\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A\\nRaffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. InAdvances\\nin Neural Information Processing Systems, volume 35, 2022a. URLhttps://proceedings.neurips.cc\\n/paper_files/paper/2022/file/0cde695b83bd186c1fd456302888454c-Paper-Conference.pdf.\\nHong Liu, Zhiyuan Li, David Leo Wright Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic\\nsecond-order optimizer for language model pre-training. In The Twelfth International Conference on\\nLearning Representations, 2024a. URLhttps://openreview.net/forum?id=3xHDeA8Noi.\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes\\ngood in-context examples for GPT-3? InProceedings of Deep Learning Inside Out: The 3rd Workshop on\\nKnowledge Extraction and Integration for Deep Learning Architectures, 2022b. URL https://aclantho\\nlogy.org/2022.deelio-1.10.\\nJiachen Liu, Zhiyu Wu, Jae-Won Chung, Fan Lai, Myungjin Lee, and Mosharaf Chowdhury. Andes:\\nDefining and enhancing quality-of-experience in llm-based text streaming services, 2024b,arXiv preprint\\narXiv:2404.16283. URL http://arxiv.org/abs/2404.16283.\\nJing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, and Bohan Zhuang. QLLM: Accurate and\\nefficient low-bitwidth quantization for large language models. InThe Twelfth International Conference on\\nLearning Representations, 2024c. URLhttps://openreview.net/forum?id=FIplmUWdm3.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\\nprompt, and predict: A systematic survey of prompting methods in natural language processing.ACM\\nComputing Surveys, 55, 2023a. URLhttps://doi.org/10.1145/3560815.\\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning: Prompt\\ntuning can be comparable to fine-tuning across scales and tasks. InProceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume 2: Short Papers), Dublin, Ireland, 2022c. URL\\nhttps://aclanthology.org/2022.acl-short.8.\\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands,\\ntoo, 2023b,arXiv preprint arXiv:2103.10385.URL http://arxiv.org/abs/2103.10385.\\n50'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 50, 'page_label': '51'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nYuliang Liu, Shenggui Li, Jiarui Fang, Yanjun Shao, Boyuan Yao, and Yang You. Colossal-auto: Uni-\\nfied automation of parallelization and activation checkpoint for large-scale models, 2023c,arXiv preprint\\narXiv:2302.02599. URL http://arxiv.org/abs/2302.02599.\\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,\\nRaghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for\\nlarge language models, 2023d,arXiv preprint arXiv:2305.17888.URL http://arxiv.org/abs/2305.178\\n88.\\nZichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis,\\nand Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for LLM\\nKV cache compression at test time. In Thirty-seventh Conference on Neural Information Processing\\nSystems, 2023e. URLhttps://openreview.net/forum?id=JZfg6wGi6g.\\nZichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,\\nYuandong Tian, Christopher Re, and Beidi Chen. Deja vu: Contextual sparsity for efficient LLMs at\\ninference time. In Proceedings of the 40th International Conference on Machine Learning, volume 202,\\n2023f. URL https://proceedings.mlr.press/v202/liu23am.html.\\nZirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Tang, Zhimeng Jiang,\\nKaixiong Zhou, Vipin Chaudhary, Shuai Xu, and Xia Hu. Winner-take-all column row sampling for\\nmemory efficient adaptation of language model. In Thirty-seventh Conference on Neural Information\\nProcessing Systems, 2023g. URLhttps://openreview.net/forum?id=SquMNyrk1O.\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. InInternational Conference on\\nLearning Representations, 2019. URLhttps://openreview.net/forum?id=Bkg6RiCqY7.\\nYaoLu, MaxBartolo, AlastairMoore, SebastianRiedel, andPontusStenetorp. Fantasticallyorderedprompts\\nand where to find them: Overcoming few-shot prompt order sensitivity. InProceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, 2022.\\nURL https://aclanthology.org/2022.acl-long.556.\\nYucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, and Yuxiong He. Maximizing communication\\nefficiency for large-scale training via 0/1 adam. InThe Eleventh International Conference on Learning\\nRepresentations, 2023. URLhttps://openreview.net/forum?id=-CefY2EOupj.\\nMan Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and\\nVincent Y Zhao. Dr.ICL: Demonstration-retrieved in-context learning. InR0-FoMo:Robustness of Few-\\nshot and Zero-shot Learning in Large Foundation Models, 2023. URL https://openreview.net/forum\\n?id=NDNb6L5xjI.\\nKai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. Full parameter fine-\\ntuning for large language models with limited resources, 2023,arXiv preprint arXiv:2306.09782. URL\\nhttp://arxiv.org/abs/2306.09782.\\nXinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-pruner: On the structural pruning of large language\\nmodels. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:\\n//openreview.net/forum?id=J8Ajf9WfXP.\\nSadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev\\nArora. Fine-tuning language models with just forward passes. InThirty-seventh Conference on Neural\\nInformation Processing Systems, 2023. URLhttps://openreview.net/forum?id=Vota6rFhBQ.\\nPedro Henrique Martins, Zita Marinho, and Andre Martins.∞-former: Infinite memory transformer. In\\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), Dublin, Ireland, 2022. URLhttps://aclanthology.org/2022.acl-long.375.\\n51'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 51, 'page_label': '52'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nHarsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via\\ngated state spaces. In The Eleventh International Conference on Learning Representations, 2023. URL\\nhttps://openreview.net/forum?id=5MkYIYCbva.\\nMeta. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https:\\n//ai.meta.com/blog/meta-llama-3/.\\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee\\nWong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna\\nAbhyankar, and Zhihao Jia. Specinfer: Accelerating generative large language model serving with\\ntree-based speculative inference and verification, 2024, arXiv preprint arXiv:2305.09781. URL http:\\n//arxiv.org/abs/2305.09781.\\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris\\nGinsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training.\\nIn International Conference on Learning Representations, 2018. URL https://openreview.net/forum\\n?id=r1gs9JgRZ.\\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn in context.\\nInProceedings of the 2022 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Seattle, United States, 2022a. URLhttps://aclanthology\\n.org/2022.naacl-main.201.\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\\nmoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings\\nof the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United Arab\\nEmirates, 2022b. URLhttps://aclanthology.org/2022.emnlp-main.759.\\nMLC-LLM, MLC-LLM, 2023,https://github.com/mlc-ai/mlc-llm.\\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for\\ntransformers, 2023,arXiv preprint arXiv:2305.16300.URL http://arxiv.org/abs/2305.16300.\\nGiovanni Monea, Armand Joulin, and Edouard Grave. Pass: Parallel speculative sampling, 2023,arXiv\\npreprint arXiv:2311.13581.URL http://arxiv.org/abs/2311.13581.\\nPhilipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih\\nElibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: A distributed framework for\\nemerging AI applications. In13th USENIX Symposium on Operating Systems Design and Implementation\\n(OSDI), Carlsbad, CA, 2018. URLhttps://www.usenix.org/conference/osdi18/presentation/mori\\ntz.\\nMosaicML. Composer. https://github.com/mosaicml/composer, 2023a. GitHub repository.\\nMosaicML. Llm foundry.https://github.com/mosaicml/llm-foundry, 2023b. GitHub repository.\\nJesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. InThirty-\\nseventh Conference on Neural Information Processing Systems, 2023. URLhttps://openreview.net/f\\norum?id=2DtxPCL3T5.\\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Kor-\\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee,\\nand Matei Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. In\\nProceedings of the International Conference for High Performance Computing, Networking, Storage and\\nAnalysis, 2021. URLhttps://doi.org/10.1145/3458817.3476209.\\nXuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeleton-of-thought:\\nPrompting LLMs for efficient parallel generation. InThe Twelfth International Conference on Learning\\nRepresentations, 2024. URLhttps://openreview.net/forum?id=mqVgBbNCm9.\\n52'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 52, 'page_label': '53'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nNVIDIA. Fastertransformer: High performance transformer kernels.https://github.com/NVIDIA/Faster\\nTransformer, 2023a. GitHub repository.\\nNVIDIA. Tensorrt-llm. https://github.com/NVIDIA/TensorRT-LLM, 2023b. Accessed: 2024-05-10.\\nOpenAI. Gpt base model.https://platform.openai.com/docs/models/gpt-base, 2023.\\nShankar Padmanabhan, Yasumasa Onoe, Michael JQ Zhang, Greg Durrett, and Eunsol Choi. Propagating\\nknowledge updates to LMs through distillation. In Thirty-seventh Conference on Neural Information\\nProcessing Systems, 2023. URLhttps://openreview.net/forum?id=DFaGf3O7jf.\\nMatteo Pagliardini, Daniele Paliotta, Martin Jaggi, and François Fleuret. Fast attention over long sequences\\nwith dynamic sparse flash attention. In Thirty-seventh Conference on Neural Information Processing\\nSystems, 2023. URLhttps://openreview.net/forum?id=UINHuKeWUa.\\nYu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng Shang, Xin Jiang, and Qun Liu. Reusing pretrained\\nmodels by multi-linear operators for efficient training. InThirty-seventh Conference on Neural Information\\nProcessing Systems, 2023. URLhttps://openreview.net/forum?id=RgNXKIrWyU.\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4,\\n2023a, arXiv preprint arXiv:2304.03277.URL http://arxiv.org/abs/2304.03277.\\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao,\\nXin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He,\\nHaowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju\\nLin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind,\\nStanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing\\nRNNs for the transformer era. In Findings of the Association for Computational Linguistics: EMNLP\\n2023, Singapore, 2023b. URLhttps://aclanthology.org/2023.findings-emnlp.936.\\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. YaRN: Efficient context window extension\\nof large language models. In The Twelfth International Conference on Learning Representations, 2024.\\nURL https://openreview.net/forum?id=wHBfxhZu1u.\\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random\\nfeature attention. InInternational Conference on Learning Representations, 2021. URLhttps://openre\\nview.net/forum?id=QtTKTdVrFBB.\\nAaron Pham, Chaoyu Yang, Sean Sheng, Shenyang Zhao, Sauyon Lee, Bo Jiang, Fog Dong, Xipeng Guan,\\nand Frost Ming, OpenLLM: Operating LLMs in production, 2023,https://github.com/bentoml/OpenL\\nLM.\\nJason Phang, Yi Mao, Pengcheng He, and Weizhu Chen. Hypertuning: toward adapting large language mod-\\nels without back-propagation. InProceedings of the 40th International Conference on Machine Learning,\\n2023. URL https://dl.acm.org/doi/10.5555/3618408.3619566.\\nJonathan Pilault, Mahan Fathi, Orhan Firat, Christopher Pal, Pierre-Luc Bacon, and Ross Goroshin. Block-\\nstate transformers. InThirty-seventh Conference on Neural Information Processing Systems, New Orleans,\\nLouisiana, 2023. URLhttps://openreview.net/forum?id=XRTxIBs2eu.\\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio,\\nStefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. In\\nProceedings of the 40th International Conference on Machine Learning, volume 202, 2023. URLhttps:\\n//proceedings.mlr.press/v202/poli23a.html.\\nEdoardo Maria Ponti, Alessandro Sordoni, Yoshua Bengio, and Siva Reddy. Combining parameter-efficient\\nmodules for task-level generalisation. InProceedings of the 17th Conference of the European Chapter of\\nthe Association for Computational Linguistics, Dubrovnik, Croatia, 2023. URLhttps://aclanthology\\n.org/2023.eacl-main.49.\\n53'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 53, 'page_label': '54'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan\\nXiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.Proceedings of Machine\\nLearning and Systems, 5, 2023.\\nRamya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, and Ashish Panwar. vatten-\\ntion: Dynamic memory management for serving llms without pagedattention, 2024, arXiv preprint\\narXiv:2405.04437. URL http://arxiv.org/abs/2405.04437.\\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables\\ninput length extrapolation. InInternational Conference on Learning Representations, 2022. URLhttps:\\n//openreview.net/forum?id=R8sQPpGCv0.\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. Measuring and\\nnarrowing the compositionality gap in language models. InFindings of the Association for Computational\\nLinguistics: EMNLP, Singapore, 2023. URLhttps://aclanthology.org/2023.findings-emnlp.378.\\nChengwei Qin, Aston Zhang, Anirudh Dagar, and Wenming Ye. In-context learning with iterative demon-\\nstration selection, 2023a,arXiv preprint arXiv:2310.09881.URL http://arxiv.org/abs/2310.09881.\\nGuanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, and Benjamin Van Durme. Nugget 2d: Dynamic\\ncontextual compression for scaling decoder-only language models, 2023b,arXiv preprint arXiv:2310.02409.\\nURL http://arxiv.org/abs/2310.02409.\\nYujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng\\nLi, Maosong Sun, and Jie Zhou. Knowledge inheritance for pre-trained language models. InProceedings\\nof the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Seattle, United States, 2022. URLhttps://aclanthology.org/2022.na\\nacl-main.288.\\nZhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-\\n2: A free lunch for handling unlimited sequence lengths in large language models, 2024,arXiv preprint\\narXiv:2401.04658. URL http://arxiv.org/abs/2401.04658.\\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for\\nlong document understanding. In Findings of the Association for Computational Linguistics: EMNLP,\\n2020. URL https://aclanthology.org/2020.findings-emnlp.232.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\\nare unsupervised multitask learners. OpenAI blog, 2019. URLhttps://cdn.openai.com/better-langu\\nage-models/language_models_are_unsupervised_multitask_learners.pdf.\\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob\\nMenick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh,\\nPo-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato,\\nJohn Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar,\\nElena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre,\\nLena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic\\nDonato, AngelikiLazaridou, ArthurMensch, Jean-BaptisteLespiau, MariaTsimpoukelli, NikolaiGrigorev,\\nDoug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien\\nde Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego\\nde Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura\\nWeidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Ge-\\noffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2022,arXiv\\npreprint arXiv:2112.11446.URL http://arxiv.org/abs/2112.11446.\\n54'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 54, 'page_label': '55'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\\nJournal of Machine Learning Research, 21, 2020. URLhttps://dl.acm.org/doi/abs/10.5555/34557\\n16.3455856.\\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward\\ntraining trillion parameter models. InProceedings of the International Conference for High Performance\\nComputing, Networking, Storage and Analysis, 2020. URLhttps://dl.acm.org/doi/10.5555/3433701\\n.3433727.\\nSamyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. Zero-infinity: Breaking\\nthe gpu memory wall for extreme scale deep learning. InProceedings of the International Conference for\\nHigh Performance Computing, Networking, Storage and Analysis, 2021. URLhttps://doi.org/10.114\\n5/3458817.3476205.\\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad\\nAwan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture-of-experts inference and train-\\ning to power next-generation AI scale. InProceedings of the 39th International Conference on Machine\\nLearning, volume 162, 2022. URLhttps://proceedings.mlr.press/v162/rajbhandari22a.html.\\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations\\nenable training deep learning models with over 100 billion parameters. InProceedings of the 26th ACM\\nSIGKDD International Conference on Knowledge Discovery & Data Mining, 2020. URL https://doi.\\norg/10.1145/3394486.3406703.\\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon\\nShashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows for large language models.\\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:\\nLong Papers), Toronto, Canada, 2023. URLhttps://aclanthology.org/2023.acl-long.352.\\nRay Project, RayLLM - LLMs on Ray, GitHub repository, 2023,https://github.com/ray-project/ray\\n-llm, Accessed on: 2023-10-02.\\nJie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang,\\nDong Li, and Yuxiong He. ZeRO-Offload: Democratizing Billion-Scale model training. InUSENIX Annual\\nTechnical Conference (USENIX ATC, 2021. URL https://www.usenix.org/conference/atc21/pres\\nentation/ren-jie.\\nLiliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and ChengXiang Zhai. Sparse\\nmodular activation for efficient sequence modeling. InThirty-seventh Conference on Neural Information\\nProcessing Systems, 2023a. URLhttps://openreview.net/forum?id=TfbzX6I14i.\\nXiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda\\nZhang, Alexander Podolskiy, Grigory Arshinov, Andrey Bout, Irina Piontkovskaya, Jiansheng Wei, Xin\\nJiang, Teng Su, Qun Liu, and Jun Yao. Pangu-Σ: Towards trillion parameter language model with sparse\\nheterogeneous computing, 2023b,arXiv preprint arXiv:2303.10845.URL http://arxiv.org/abs/2303\\n.10845.\\nAdithya Renduchintala, Tugrul Konuk, and Oleksii Kuchaiev. Tied-lora: Enhacing parameter efficiency of\\nlorawithweighttying, 2023, arXiv preprint arXiv:2311.09578.URLhttp://arxiv.org/abs/2311.09578.\\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention\\nwith routing transformers.Transactions of the Association for Computational Linguistics, 9, 2021. URL\\nhttps://aclanthology.org/2021.tacl-1.4.\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In\\nProceedings of the 2022 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Seattle, United States, 2022. URLhttps://aclanthology\\n.org/2022.naacl-main.191.\\n55'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 55, 'page_label': '56'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nAndrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin,\\nand Emanuele Rodola. Accelerating transformer inference for translation via parallel decoding. InPro-\\nceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\\nPapers), Toronto, Canada, 2023. URLhttps://aclanthology.org/2023.acl-long.689.\\nParth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning.\\nRAPTOR: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International\\nConference on Learning Representations, 2024. URLhttps://openreview.net/forum?id=GN921JHCRw.\\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné,\\nAlexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella\\nBiderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muen-\\nnighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-\\nMajor, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lau-\\nrençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor\\nSoroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\\nChris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, Dragomir\\nRadev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni,\\nGérard Dupont, Germán Kruszewski, Giada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu,\\nIdris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian\\nZhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo\\nChen, Kyle Lo, Leandro Von Werra, Leon Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan\\nDey, Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin\\nCoavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A. Jauhar, Mustafa Ghaleb,\\nNishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert,\\nPauloVillegas, PeterHenderson, PierreColombo, PriscillaAmuok, QuentinLhoest, RhezaHarliman, Rishi\\nBommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik\\nBose, Shamsuddeen Hassan Muhammad, Shanya Sharma, Shayne Longpre, Somaieh Nikpoor, Stanislav\\nSilberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin\\nDanchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak\\nTalat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Davut Emre Taşar, Elizabeth Salesky, Sabrina J.\\nMielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti\\nDatta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan\\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal\\nNayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim,\\nTali Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-Xin\\nYong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jae-\\nsung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff\\nRasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas\\nPatry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre François Lavallée,\\nRémi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim\\nDettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramo-\\nnian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Takta-\\nsheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo,\\nJekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Ma-\\nrine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der\\nWal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina,\\nThomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov,\\nYada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdeněk Kasner, Alice Rueda, Amanda Pes-\\ntana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash\\nAghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade,\\nBharat Saxena, Carlos Muñoz Ferrandis, Daniel McDuff, Danish Contractor, David Lansky, Davis David,\\nDouwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline\\nOnoniwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\\njadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri,\\n56'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 56, 'page_label': '57'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nMargot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri,\\nMykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An,\\nRasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Vigu-\\nier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo\\nPalasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz,\\nBo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán,\\nDaniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyased-\\ndin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas\\nGolde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato,\\nMadeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, Maria A Castillo, Marianna Nezhurina,\\nMario Sänger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihalj-\\ncic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio\\nBroad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert\\nMartin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh,\\nShubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil\\nBharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Ba-\\njaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras,\\nYounes Belkada, and Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model,\\n2023, arXiv preprint arXiv:2211.05100.URL http://arxiv.org/abs/2211.05100.\\nStephanie Schoch, Ritwick Mishra, and Yangfeng Ji. Data selection for fine-tuning large language mod-\\nels using transferred shapley values. In Proceedings of the 61st Annual Meeting of the Association\\nfor Computational Linguistics (Volume 4: Student Research Workshop), Toronto, Canada, 2023. URL\\nhttps://aclanthology.org/2023.acl-srw.37.\\nHang Shao, Bei Liu, and Yanmin Qian. One-shot sensitivity-aware mixed sparsity pruning for large language\\nmodels, 2024,arXiv preprint arXiv:2310.09499.URL http://arxiv.org/abs/2310.09499.\\nJunru Shao, Xiyou Zhou, Siyuan Feng, Bohan Hou, Ruihang Lai, Hongyi Jin, Wuwei Lin, Masahiro Masuda,\\nCody Hao Yu, and Tianqi Chen. Tensor program optimization with probabilistic programs. InAdvances\\nin Neural Information Processing Systems, volume 35, 2022. URLhttps://proceedings.neurips.cc/p\\naper%5Ffiles/paper/2022/file/e894eafae43e68b4c8dfdacf742bcbf3-Paper-Conference.pdf.\\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In\\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Volume 2 (Short Papers), New Orleans, Louisiana, 2018.\\nURL https://aclanthology.org/N18-2074.\\nNoam Shazeer. Fast transformer decoding: One write-head is all you need, 2019, arXiv preprint\\narXiv:1911.02150. URL http://arxiv.org/abs/1911.02150.\\nSheng Shen, Pete Walsh, Kurt Keutzer, Jesse Dodge, Matthew Peters, and Iz Beltagy. Staged training for\\ntransformer language models. InProceedings of the 39th International Conference on Machine Learning,\\nvolume 162, 2022. URLhttps://proceedings.mlr.press/v162/shen22f.html.\\nSheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph,\\nWilliam Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Y\\nZhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. Mixture-of-experts meets instruction\\ntuning: A winning combination for large language models. InThe Twelfth International Conference on\\nLearning Representations, 2024. URLhttps://openreview.net/forum?id=6mLjDwYte5.\\nYing Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie,\\nBeidi Chen, Clark W. Barrett, Joseph Gonzalez, Percy Liang, Christopher Ré, Ioan Cristian Stoica,\\nand Ce Zhang. High-throughput generative inference of large language models with a single gpu. In\\nInternational Conference on Machine Learning, 2023. URL https://dl.acm.org/doi/10.5555/36184\\n08.3619696.\\n57'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 57, 'page_label': '58'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: Eliciting\\nKnowledge from Language Models with Automatically Generated Prompts. InProceedings of the 2020\\nConference on Empirical Methods in Natural Language Processing (EMNLP), Online, 2020. URLhttps:\\n//aclanthology.org/2020.emnlp-main.346.\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\\nMegatron-lm: Training multi-billion parameter language models using model parallelism, 2020, arXiv\\npreprint arXiv:1909.08053.URL http://arxiv.org/abs/1909.08053.\\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling reasoning capabilities into smaller\\nlanguage models. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto,\\nCanada, 2023. URLhttps://aclanthology.org/2023.findings-acl.441.\\nAntoine Simoulin, Namyong Park, Xiaoyi Liu, and Grey Yang. Memory-efficient selective fine-tuning. In\\nWorkshop on Efficient Systems for Foundation Models, 2023. URL https://openreview.net/forum?i\\nd=zaNbLceVwm.\\nSiddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, and Abhinav\\nBhatele. A hybrid tensor-expert-data parallelism approach to optimize mixture-of-experts training. In\\nICS 2023, 2023.\\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,\\nZhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yaz-\\ndani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh\\nTiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-\\nscale generative language model, 2022,arXiv preprint arXiv:2201.11990.URL http://arxiv.org/abs/\\n2201.11990.\\nSaleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith\\nPeris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar,\\nFabian Triefenbach, Apurv Verma, Gokhan Tur, and Prem Natarajan. Alexatm 20b: Few-shot learning\\nusing a large-scale multilingual seq2seq model, 2022,arXiv preprint arXiv:2208.01448.URL http://ar\\nxiv.org/abs/2208.01448.\\nJ.C. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation.\\nIEEE Transactions on Automatic Control, 37, 1992. URLhttps://ieeexplore.ieee.org/document/1\\n19632.\\nBenjamin Spector and Chris Re. Accelerating llm inference with staged speculative decoding, 2023,arXiv\\npreprint arXiv:2308.04623.URL http://arxiv.org/abs/2308.04623.\\nHongjin SU, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\\nLuke Zettlemoyer, Noah A. Smith, and Tao Yu. Selective annotation makes language models better\\nfew-shot learners. In The Eleventh International Conference on Learning Representations, 2023. URL\\nhttps://openreview.net/forum?id=qY1hlv7gwg.\\nHui Su, Xiao Zhou, Houjin Yu, Xiaoyu Shen, Yuwen Chen, Zilin Zhu, Yang Yu, and Jie Zhou. Welm: A\\nwell-read pre-trained language model for chinese, 2023a,arXiv preprint arXiv:2209.10372.URL http:\\n//arxiv.org/abs/2209.10372.\\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\\ntransformer with rotary position embedding, 2023b,arXiv preprint arXiv:2104.09864.URL http://arxi\\nv.org/abs/2104.09864.\\nMingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large\\nlanguage models. In The Twelfth International Conference on Learning Representations, 2024. URL\\nhttps://openreview.net/forum?id=PxoFut3dWW.\\n58'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 58, 'page_label': '59'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nTianxiang Sun, Zhengfu He, Qin Zhu, Xipeng Qiu, and Xuanjing Huang. Multitask pre-training of modular\\nprompt for Chinese few-shot learning. InProceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023a. URLhttps://aclantho\\nlogy.org/2023.acl-long.625.\\nYutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu\\nWei. Retentive network: A successor to transformer for large language models, 2023b,arXiv preprint\\narXiv:2307.08621. URL http://arxiv.org/abs/2307.08621.\\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia\\nSong, and Furu Wei. A length-extrapolatable transformer. InProceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023c. URL\\nhttps://aclanthology.org/2023.acl-long.816.\\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for rein-\\nforcement learning with function approximation. InAdvances in Neural Information Processing Systems,\\nvolume 12, 1999. URLhttps://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b8\\n5b0bed98e80ade0a5c43b0f-Paper.pdf.\\nWeng Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Jiahua Liu, Tao Li, Yuxiao Dong, and Jie Tang. Parameter-\\nefficient prompt tuning makes generalized and calibrated neural text retrievers. InFindings of the Asso-\\nciation for Computational Linguistics: EMNLP 2023, Singapore, 2023. URLhttps://aclanthology.o\\nrg/2023.findings-emnlp.874.\\nHanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian,\\nJi Liu, Ce Zhang, and Yuxiong He. 1-bit adam: Communication efficient large-scale training with adam’s\\nconvergence speed. InProceedings of the 38th International Conference on Machine Learning, volume 139,\\n2021. URL https://proceedings.mlr.press/v139/tang21a.html.\\nChaofanTao, LuHou, WeiZhang, LifengShang, XinJiang, QunLiu, PingLuo, andNgaiWong. Compression\\nof generative pre-trained language models via quantization. InProceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ireland, 2022. URL\\nhttps://aclanthology.org/2022.acl-long.331.\\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. InProceed-\\nings of the 37th International Conference on Machine Learning, 2020. URLhttps://dl.acm.org/doi/a\\nbs/10.5555/3524938.3525813.\\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM\\nComputing Surveys, 55, 2022. URLhttps://doi.org/10.1145/3530811.\\nGemini Team and Google. Gemini: A family of highly capable multimodal models.https://storage.go\\nogleapis.com/deepmind-media/gemini/gemini_1_report.pdf, 2023.\\nThe MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms.\\nhttps://www.mosaicml.com/blog/mpt-7b, 2023.\\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin\\nGhafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,\\nYuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching\\nChang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-\\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben\\nZevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina,\\nErin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya\\nKuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,\\nMarian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022, arXiv\\npreprint arXiv:2201.08239.URL http://arxiv.org/abs/2201.08239.\\n59'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 59, 'page_label': '60'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nInar Timiryasov and Jean-Loup Tastet. Baby llama: knowledge distillation from an ensemble of teachers\\ntrained on a small dataset with no performance penalty. InProceedings of the BabyLM Challenge at the\\n27th Conference on Computational Natural Language Learning, Singapore, 2023. URLhttps://aclant\\nhology.org/2023.conll-babylm.24.\\nDenis Timonin, Bo Yang Hsueh, and Vinh Nguyen. Accelerated inference for large transformer models using\\nnvidia triton inference server, 2022. URLhttps://developer.nvidia.com/blog/accelerated-infer\\nence-for-large-transformer-models-using-nvidia-fastertransformer-and-nvidia-triton-inf\\nerence-server.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin,\\nEdouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models.ArXiv,\\nabs/2302.13971, 2023a. URLhttps://api.semanticscholar.org/CorpusID:257219404.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Fer-\\nrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh\\nKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,\\nXavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy\\nReizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subra-\\nmanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,\\nRobert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat\\nmodels, 2023b,arXiv preprint arXiv:2307.09288.URL http://arxiv.org/abs/2307.09288.\\nLewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi\\nHuang, Leandro von Werra, Clémentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero,\\nAlexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023,arXiv preprint\\narXiv:2310.16944. URL http://arxiv.org/abs/2310.16944.\\nSzymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr\\nMiłoś. Focused transformer: Contrastive training for context scaling. InThirty-seventh Conference on\\nNeural Information Processing Systems, 2023. URLhttps://openreview.net/forum?id=s1FjXzJ0jy.\\nMojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. DyLoRA: Parameter-efficient\\ntuning of pre-trained models using dynamic search-free low-rank adaptation. InProceedings of the 17th\\nConference of the European Chapter of the Association for Computational Linguistics, Dubrovnik, Croatia,\\n2023. URL https://aclanthology.org/2023.eacl-main.239.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need. InAdvances in Neural Information Processing Systems,\\nvolume 30, 2017. URLhttps://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee2435\\n47dee91fbd053c1c4a845aa-Paper.pdf.\\nApoorv Vyas, Angelos Katharopoulos, and François Fleuret. Fast transformers with clustered attention. In\\nAdvances in Neural Information Processing Systems, volume 33, 2020. URLhttps://proceedings.neur\\nips.cc/paper_files/paper/2020/file/f6a8dd1c954c8506aadc764cc32b895e-Paper.pdf.\\nZhongwei Wan, Yichun Yin, Wei Zhang, Jiaxin Shi, Lifeng Shang, Guangyong Chen, Xin Jiang, and Qun\\nLiu. G-MAP: General memory-augmented pre-trained language model for domain tasks. InProceedings\\nof the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United Arab\\nEmirates, 2022. URLhttps://aclanthology.org/2022.emnlp-main.441.\\n60'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 60, 'page_label': '61'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nZhongwei Wan, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, César Quilodrán-Casas, and\\nRossella Arcucci. Med-unic: Unifying cross-lingual medical vision-language pre-training by diminishing\\nbias. In Advances in Neural Information Processing Systems, volume 36, 2023. URLhttps://proceedi\\nngs.neurips.cc/paper_files/paper/2023/file/af38fb8e90d586f209235c94119ba193-Paper-Confe\\nrence.pdf.\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,\\nand Samuel R. Bowman. Superglue: a stickier benchmark for general-purpose language understanding\\nsystems. In Proceedings of the 33rd International Conference on Neural Information Processing Systems,\\n2019. URL https://dl.acm.org/doi/10.5555/3454287.3454581.\\nBoxiang Wang, Qifan Xu, Zhengda Bian, and Yang You. Tesseract: Parallelize the tensor parallelism\\nefficiently. InProceedings of the 51st International Conference on Parallel Processing, 2023a. URLhttps:\\n//doi.org/10.1145/3545008.3545087.\\nGuoxin Wang, Yijuan Lu, Lei Cui, Tengchao Lv, Dinei Florencio, and Cha Zhang. A simple yet effective\\nlearnable positional encoding method for improving document transformer model. In Findings of the\\nAssociation for Computational Linguistics: AACL-IJCNLP 2022, 2022a. URLhttps://aclanthology.o\\nrg/2022.findings-aacl.42.\\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping\\nWang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models, 2023b,arXiv\\npreprint arXiv:2310.11453.URL http://arxiv.org/abs/2310.11453.\\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling\\ntransformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024a.\\nURL https://ieeexplore.ieee.org/document/10496231.\\nLiang Wang, Nan Yang, and Furu Wei. Learning to retrieve in-context examples for large language models.\\nIn Proceedings of the 18th Conference of the European Chapter of the Association for Computational\\nLinguistics (Volume 1: Long Papers), St. Julian’s, Malta, 2024b. URLhttps://aclanthology.org/202\\n4.eacl-long.105.\\nNingning Wang, Guobing Gan, Peng Zhang, Shuai Zhang, Junqiu Wei, Qun Liu, and Xin Jiang. Clus-\\nterFormer: Neural clustering attention for efficient and effective transformer. InProceedings of the 60th\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Dublin, Ire-\\nland, 2022b. URLhttps://aclanthology.org/2022.acl-long.170.\\nPeifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. SCOTT: Self-consistent\\nchain-of-thought distillation. InProceedings of the 61st Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 1: Long Papers), Toronto, Canada, 2023c. URLhttps://aclanthology.org\\n/2023.acl-long.304.\\nPeihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio\\nFeris, David Daniel Cox, Zhangyang Wang, and Yoon Kim. Learning to grow pretrained models for\\nefficient transformer training. In The Eleventh International Conference on Learning Representations,\\n2023d. URL https://openreview.net/forum?id=cDYRS5iZ16f.\\nShuohuan Wang, Yu Sun, Yang Xiang, Zhihua Wu, Siyu Ding, Weibao Gong, Shikun Feng, Junyuan Shang,\\nYanbin Zhao, Chao Pang, Jiaxiang Liu, Xuyi Chen, Yuxiang Lu, Weixin Liu, Xi Wang, Yangfan Bai,\\nQiuliang Chen, Li Zhao, Shiyong Li, Peng Sun, Dianhai Yu, Yanjun Ma, Hao Tian, Hua Wu, Tian\\nWu, Wei Zeng, Ge Li, Wen Gao, and Haifeng Wang. Ernie 3.0 titan: Exploring larger-scale knowledge\\nenhanced pre-training for language understanding and generation, 2021,arXiv preprint arXiv:2112.12731.\\nURL http://arxiv.org/abs/2112.12731.\\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear\\ncomplexity, 2020,arXiv preprint arXiv:2006.04768.URL http://arxiv.org/abs/2006.04768.\\n61'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 61, 'page_label': '62'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nWeizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting\\nlanguage models with long-term memory. InThirty-seventh Conference on Neural Information Processing\\nSystems, 2023e. URLhttps://openreview.net/forum?id=BryMFPQ4L6.\\nXin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. Svd-llm: Truncation-aware singular value decom-\\nposition for large language model compression, 2024c, arXiv preprint arXiv:2403.07378. URL http:\\n//arxiv.org/abs/2403.07378.\\nXinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. Large language models\\nare latent variable models: Explaining and finding good demonstrations for in-context learning. InThirty-\\nseventh Conference on Neural Information Processing Systems, 2023f. URLhttps://openreview.net/f\\norum?id=BGvkwZEGt7.\\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah,\\nand Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient model tuning. InProceedings\\nof the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United Arab\\nEmirates, 2022c. URLhttps://aclanthology.org/2022.emnlp-main.388.\\nYiming Wang, Yu Lin, Xiaodong Zeng, and Guannan Zhang. Multilora: Democratizing lora for better\\nmulti-task learning, 2023g,arXiv preprint arXiv:2311.11501.URL http://arxiv.org/abs/2311.11501.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh\\nHajishirzi. Self-instruct: Aligning language models with self-generated instructions. InProceedings of the\\n61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto,\\nCanada, 2023h. URLhttps://aclanthology.org/2023.acl-long.754.\\nYufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin\\nJiang, and Qun Liu. Aligning large language models with human: A survey, 2023i, arXiv preprint\\narXiv:2307.12966. URL http://arxiv.org/abs/2307.12966.\\nZhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, and Yoon Kim. Multitask prompt\\ntuning enables parameter-efficient transfer learning. InThe Eleventh International Conference on Learning\\nRepresentations, 2023j. URLhttps://openreview.net/forum?id=Nk2pDtuhTq.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy\\nLiang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on\\nMachine Learning Research, 2022a. URLhttps://openreview.net/forum?id=yzkSU5zdwD.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\\nand Denny Zhou. Chain of thought prompting elicits reasoning in large language models. InAdvances in\\nNeural Information Processing Systems, 2022b. URLhttps://openreview.net/forum?id=_VjQlMeSB_J.\\nXiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu.\\nOutlier suppression+: Accurate quantization of large language models by equivalent and effective shifting\\nand scaling. InProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,\\nSingapore, 2023. URLhttps://aclanthology.org/2023.emnlp-main.102.\\nGenta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, and Pascale Fung. Lightweight and\\nefficient end-to-end speech recognition using low-rank transformer. InIEEE International Conference on\\nAcoustics, Speech and Signal Processing (ICASSP), 2020. URLhttps://ieeexplore.ieee.org/docume\\nnt/9053878.\\nQingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu. Memformer: A\\nmemory-augmented transformer for sequence modeling. InFindings of the Association for Computational\\nLinguistics: AACL-IJCNLP 2022, 2022a. URLhttps://aclanthology.org/2022.findings-aacl.29.\\nXiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme compression for pre-trained\\ntransformers made simple and efficient. InNeurIPS 2022, 2022b.\\n62'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 62, 'page_label': '63'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nXiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong He. Understanding int4 quanti-\\nzation for language models: latency speedup, composability, and failure cases. InProceedings of the 40th\\nInternational Conference on Machine Learning, 2023a. URLhttps://dl.acm.org/doi/10.5555/36184\\n08.3619970.\\nXiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap forward in llms post-training w4a8 quan-\\ntization using floating-point formats, 2023b,arXiv preprint arXiv:2307.09782.URL http://arxiv.org/\\nabs/2307.09782.\\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers.\\nIn International Conference on Learning Representations, 2022c. URLhttps://openreview.net/forum\\n?id=TrjbxzRcnf-.\\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. Self-adaptive in-context learning: An\\ninformation compression perspective for in-context example selection and ordering. InProceedings of the\\n61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Toronto,\\nCanada, 2023c. URLhttps://aclanthology.org/2023.acl-long.79.\\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared LLaMA: Accelerating language model\\npre-training via structured pruning. InWorkshop on Advancing Neural Network Training: Computational\\nEfficiency, Scalability, and Resource Optimization, 2023. URLhttps://openreview.net/forum?id=6s\\n77hjBNfS.\\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate\\nand efficient post-training quantization for large language models. InProceedings of the 40th International\\nConference on Machine Learning, volume 202, 2023. URLhttps://proceedings.mlr.press/v202/xia\\no23c.html.\\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language\\nmodels with attention sinks. InThe Twelfth International Conference on Learning Representations, 2024.\\nURL https://openreview.net/forum?id=NG7sS51zVF.\\nSang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc V\\nLe, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model\\npretraining. InThirty-seventh Conference on Neural Information Processing Systems, 2023a. URLhttps:\\n//openreview.net/forum?id=lXuByUeHhd.\\nSang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via\\nimportance resampling. InThirty-seventh Conference on Neural Information Processing Systems, 2023b.\\nURL https://openreview.net/forum?id=uPSQv0leAu.\\nMingxue Xu, Yao Lei Xu, and Danilo P. Mandic. Tensorgpt: Efficient compression of the embedding\\nlayer in llms based on the tensor-train decomposition, 2023a, arXiv preprint arXiv:2307.00526. URL\\nhttp://arxiv.org/abs/2307.00526.\\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\\nBakhturina, Mohammad Shoeybi, and Bryan Catanzaro. Retrieval meets long context large language\\nmodels. In The Twelfth International Conference on Learning Representations, 2024a. URL https:\\n//openreview.net/forum?id=xw5nxFWMlo.\\nQifan Xu and Yang You. An efficient 2d method for training super-large deep learning models. In2023\\nIEEE International Parallel and Distributed Processing Symposium (IPDPS), pp. 222–232, 2023. doi:\\n10.1109/IPDPS54959.2023.00031.\\nYuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, XIAOPENG\\nZHANG, and Qi Tian. QA-loRA: Quantization-aware low-rank adaptation of large language models. In\\nThe Twelfth International Conference on Learning Representations, 2024b. URLhttps://openreview.n\\net/forum?id=WvFoJccpo8.\\n63'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 63, 'page_label': '64'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nZhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, and Anshumali Shri-\\nvastava. Compress, then prompt: Improving accuracy-efficiency trade-off of llm inference with transferable\\nprompt, 2023b,arXiv preprint arXiv:2305.11186.URL http://arxiv.org/abs/2305.11186.\\nFuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe:\\nOpen mixture-of-experts language models, 2023. URLhttps://github.com/XueFuzhao/OpenMoE.\\nCheng Yang, Shengnan Wang, Chao Yang, Yuechuan Li, Ru He, and Jingqiao Zhang. Progressively\\nstacking 2.0: A multi-stage layerwise training method for bert training speedup, 2020,arXiv preprint\\narXiv:2011.13635. URL http://arxiv.org/abs/2011.13635.\\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large\\nlanguage models as optimizers, 2023a,arXiv preprint arXiv:2309.03409.URL http://arxiv.org/abs/\\n2309.03409.\\nGreg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, David Farhi, Jakub Pachocki, Xiaodong Liu,\\nWeizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyper-\\nparameter transfer. In NeurIPS 2021, March 2022. URLhttps://www.microsoft.com/en-us/resear\\nch/publication/tuning-large-neural-networks-via-zero-shot-hyperparameter-transfer/ .\\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong,\\nBing Yin, and Xia Hu. Harnessing the power of llms in practice: A survey on chatgpt and beyond.ACM\\nTransactions on Knowledge Discovery from Data, 2024. URLhttps://doi.org/10.1145/3649506.\\nKeming Yang, Zichen Liu, and Philip Cheng, MOSEC: Model Serving made Efficient in the Cloud, 2021,\\nhttps://github.com/mosecorg/mosec.\\nNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu\\nWei. Inference with reference: Lossless acceleration of large language models, 2023b, arXiv preprint\\narXiv:2304.04487. URL http://arxiv.org/abs/2304.04487.\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R\\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. InThirty-seventh\\nConference on Neural Information Processing Systems, 2023a. URLhttps://openreview.net/forum?i\\nd=5Xc1ecxO1h.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. Re-\\nact: Synergizing reasoning and acting in language models. InThe Eleventh International Conference on\\nLearning Representations, 2023b. URLhttps://openreview.net/forum?id=WE_vluYUL-X.\\nXingcheng Yao, Yanan Zheng, Xiaocong Yang, and Zhilin Yang. NLP from scratch without large-scale\\npretraining: A simple and efficient framework. In Proceedings of the 39th International Conference on\\nMachine Learning, volume 162, 2022a. URLhttps://proceedings.mlr.press/v162/yao22c.html.\\nYiqun Yao, Zheng Zhang, Jing Li, and Yequan Wang. Masked structural growth for 2x faster language\\nmodel pre-training. In The Twelfth International Conference on Learning Representations, 2024. URL\\nhttps://openreview.net/forum?id=rL7xsg1aRn.\\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zeroquant:\\nEfficient and affordable post-training quantization for large-scale transformers. In Advances in Neural\\nInformation Processing Systems, volume 35, 2022b. URLhttps://proceedings.neurips.cc/paper_f\\niles/paper/2022/file/adf7fa39d65e2983d724ff7da57f00ac-Paper-Conference.pdf.\\nZhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad\\nAwan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly\\nSmith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, and Yuxiong He.\\nDeepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales, 2023c,arXiv\\npreprint arXiv:2308.01320.URL http://arxiv.org/abs/2308.01320.\\n64'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 64, 'page_label': '65'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nZhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2: Exploring post-\\ntraining quantization in llms from comprehensive study to low rank compensation, 2023d,arXiv preprint\\narXiv:2303.08302. URL http://arxiv.org/abs/2303.08302.\\nRongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. Edgemoe: Fast on-\\ndevice inference of moe-based large language models, 2023,arXiv preprint arXiv:2308.14352.URL http:\\n//arxiv.org/abs/2308.14352.\\nJie You, Jae-Won Chung, and Mosharaf Chowdhury. Zeus: Understanding and optimizing GPU energy\\nconsumption of DNN training. In 20th USENIX Symposium on Networked Systems Design and Imple-\\nmentation (NSDI), Boston, MA, 2023. URLhttps://www.usenix.org/conference/nsdi23/presentat\\nion/you.\\nGyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed\\nserving system for Transformer-Based generative models. In 16th USENIX Symposium on Operating\\nSystems Design and Implementation (OSDI), Carlsbad, CA, 2022. URLhttps://www.usenix.org/con\\nference/osdi22/presentation/yu.\\nLILI YU, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis.\\nMEGABYTE: Predicting million-byte sequences with multiscale transformers. In Thirty-seventh Con-\\nference on Neural Information Processing Systems, 2023. URLhttps://openreview.net/forum?id=JT\\nmO2V9Xpz.\\nZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang\\nWu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language\\nmodels, 2023a,arXiv preprint arXiv:2304.01089.URL http://arxiv.org/abs/2304.01089.\\nZhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activation-\\naware singular value decomposition for compressing large language models, 2023b, arXiv preprint\\narXiv:2312.05821. URL http://arxiv.org/abs/2312.05821.\\nManzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\\nPham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: transformers for longer\\nsequences. InProceedings of the 34th International Conference on Neural Information Processing Systems,\\n2020. URL https://dl.acm.org/doi/abs/10.5555/3495724.3497174.\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi\\nZheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu,\\nPeng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pre-trained model. In The\\nEleventh International Conference on Learning Representations, 2023. URLhttps://openreview.net/f\\norum?id=-Aw0rrrPUF.\\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao, Zhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\\nWang, Xiaoda Zhang, Chen Li, Ziyan Gong, Yifan Yao, Xinjing Huang, Jun Wang, Jianfeng Yu, Qi Guo,\\nYue Yu, Yan Zhang, Jin Wang, Hengtao Tao, Dasen Yan, Zexuan Yi, Fang Peng, Fangqing Jiang, Han\\nZhang, Lingfeng Deng, Yehong Zhang, Zhe Lin, Chao Zhang, Shaojie Zhang, Mingyue Guo, Shanzhi\\nGu, Gaojun Fan, Yaowei Wang, Xuefeng Jin, Qun Liu, and Yonghong Tian. Pangu- α: Large-scale\\nautoregressive pretrained chinese language models with auto-parallel computation, 2021,arXiv preprint\\narXiv:2104.12369. URL http://arxiv.org/abs/2104.12369.\\nMingshu Zhai, Jiaao He, Zixuan Ma, Zan Zong, Runqing Zhang, and Jidong Zhai. SmartMoE: Efficiently\\ntraining Sparsely-Activated models through combining offline and online parallelization. In2023 USENIX\\nAnnual Technical Conference (USENIX ATC), Boston, MA, 2023. URLhttps://www.usenix.org/con\\nference/atc23/presentation/zhai.\\nChen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. Towards the law of capacity gap in distilling language\\nmodels, 2023a,arXiv preprint arXiv:2311.07052.URL http://arxiv.org/abs/2311.07052.\\n65'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 65, 'page_label': '66'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nHang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. Pool-\\ningformer: Long document modeling with pooling attention. In Proceedings of the 38th International\\nConference on Machine Learning, volume 139, 2021. URLhttps://proceedings.mlr.press/v139/zha\\nng21h.html.\\nHongyi Zhang, Yann N. Dauphin, and Tengyu Ma. Residual learning without normalization via better\\ninitialization. In International Conference on Learning Representations, 2019. URL https://openrevi\\new.net/forum?id=H1gsz30cKX.\\nLongteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient low-rank\\nadaptation for large language models fine-tuning, 2023b,arXiv preprint arXiv:2308.03303.URL http:\\n//arxiv.org/abs/2308.03303.\\nMingyangZhang, HaoChen, ChunhuaShen, ZhenYang, LinlinOu, XinyiYu, andBohanZhuang. Loraprune:\\nPruning meets low-rank parameter-efficient fine-tuning, 2023c, arXiv preprint arXiv:2305.18403. URL\\nhttp://arxiv.org/abs/2305.18403.\\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.\\nAdaptive budget allocation for parameter-efficient fine-tuning. InThe Eleventh International Conference\\non Learning Representations, 2023d. URLhttps://openreview.net/forum?id=lq62uWRJjiY.\\nRenrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, and Peng Gao.\\nLLaMA-adapter: Efficient fine-tuning of large language models with zero-initialized attention. In The\\nTwelfth International Conference on Learning Representations, 2024. URLhttps://openreview.net/f\\norum?id=d4UiXAHN2W.\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\\nMona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained\\ntransformer language models, 2022a,arXiv preprint arXiv:2205.01068.URL http://arxiv.org/abs/22\\n05.01068.\\nTianyi Zhang, Mina Lee, Xiang Lisa Li, Ende Shen, and Tatsunori Hashimoto. TempLM: Distilling language\\nmodels into template-based generators. InFindings of the Association for Computational Linguistics: ACL\\n2023, Toronto, Canada, 2023e. URLhttps://aclanthology.org/2023.findings-acl.124.\\nYiming Zhang, Shi Feng, and Chenhao Tan. Active example selection for in-context learning. InProceedings\\nof the 2022 Conference on Empirical Methods in Natural Language Processing, Abu Dhabi, United Arab\\nEmirates, 2022b. URLhttps://aclanthology.org/2022.emnlp-main.622.\\nYue Zhang, Hongliang Fei, Dingcheng Li, and Ping Li. PromptGen: Automatically generate prompts using\\ngenerative models. InFindings of the Association for Computational Linguistics: NAACL 2022, Seattle,\\nUnited States, 2022c. URLhttps://aclanthology.org/2022.findings-naacl.3.\\nZhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong\\nTian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for\\nefficient generative inference of large language models. InWorkshop on Efficient Systems for Foundation\\nModels, 2023f. URLhttps://openreview.net/forum?id=ctPizehA9D.\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large\\nlanguage models. In The Eleventh International Conference on Learning Representations, 2023g. URL\\nhttps://openreview.net/forum?id=5NTt8GFjUHkr.\\nJiawei Zhao, Florian Tobias Schaefer, and Anima Anandkumar. ZerO Initialization: Initializing neural\\nnetworks with only zeros and ones. Transactions on Machine Learning Research, 2022. URL https:\\n//openreview.net/forum?id=1AxQpKmiTc.\\n66'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 66, 'page_label': '67'}, page_content='Published in Transactions on Machine Learning Research (May/2024)\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A Survey\\nof Large Language Models, 2023a,arXiv preprint arXiv:2303.18223.URL http://arxiv.org/abs/2303\\n.18223.\\nWeilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, and Maosong Sun. CPET: Effective\\nparameter-efficient tuning for compressed large language models, 2023b,arXiv preprint arXiv:2307.07705.\\nURL http://arxiv.org/abs/2307.07705.\\nYanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid\\nShojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen,\\nGeeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. PyTorch FSDP: Experiences on Scaling Fully\\nSharded Data Parallel.Proceedings of the VLDB Endowment, 16, 2023c. URLhttps://doi.org/10.147\\n78/3611540.3611569.\\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang,\\nYang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generation with\\nmultilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining, 2023. URLhttps://doi.org/10.1145/3580305.3599790.\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping\\nYu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less\\nis more for alignment. InThirty-seventh Conference on Neural Information Processing Systems, 2023a.\\nURL https://openreview.net/forum?id=KBMOKmX2he.\\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\\nCui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning\\nin large language models. InThe Eleventh International Conference on Learning Representations, 2023b.\\nURL https://openreview.net/forum?id=WZH7099tgfM.\\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, zhifeng Chen,\\nQuoc V Le, and James Laudon. Mixture-of-experts with expert choice routing. InAdvances in Neural\\nInformation Processing Systems, volume 35, 2022. URLhttps://proceedings.neurips.cc/paper_fil\\nes/paper/2022/file/2f00ecd787b432c1d36f3de9800728eb-Paper-Conference.pdf.\\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy\\nBa. Large language models are human-level prompt engineers. InThe Eleventh International Conference\\non Learning Representations, 2023c. URLhttps://openreview.net/forum?id=92gvk82DE-.\\nDawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. PoSE: Efficient con-\\ntext window extension of LLMs via positional skip-wise training. InThe Twelfth International Conference\\non Learning Representations, 2024. URLhttps://openreview.net/forum?id=3Z1gxuAQrA.\\nBohan Zhuang, Jing Liu, Zizheng Pan, Haoyu He, Yuetian Weng, and Chunhua Shen. A survey on efficient\\ntraining of transformers, 2023,arXiv preprint arXiv:2302.01107.URL http://arxiv.org/abs/2302.0\\n1107.\\nZirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and\\nXia Hu. Kivi : Plug-and-play 2bit kv cache quantization with streaming asymmetric quantization.Arxiv,\\n2023. doi: 10.13140/RG.2.2.28167.37282. URLhttps://rgdoi.net/10.13140/RG.2.2.28167.37282.\\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Jianfeng Gao, and Tuo\\nZhao. Taming sparsely activated transformer with stochastic experts. In International Conference on\\nLearning Representations, 2022. URLhttps://openreview.net/forum?id=B72HXs80q4.\\n67'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 0, 'page_label': '1'}, page_content='Large Language Models: A Survey\\nShervin Minaee1, Tomas Mikolov2, Narjes Nikzad 3, Meysam Chenaghlu 4\\nRichard Socher5, Xavier Amatriain 6, Jianfeng Gao 7\\n1 Applied Scientist, Amazon Inc\\n2 Senior Researcher, CIIRC CTU\\n3 Cologne University of Applied Sciences\\n4 Staff Machine Learning Scientist, Ultimate.ai\\n5 CEO, You.com\\n6 VP of Product, AI and Compute Enablement, Google Inc\\n7 VP of Deep Learning Group, Microsoft Research\\nAbstract—Large Language Models (LLMs) have drawn a\\nlot of attention due to their strong performance on a wide\\nrange of natural language tasks, since the release of ChatGPT\\nin November 2022. LLMs’ ability of general-purpose language\\nunderstanding and generation is acquired by training billions of\\nmodel’s parameters on massive amounts of text data, as predicted\\nby scaling laws [1], [2]. The research area of LLMs, while very\\nrecent, is evolving rapidly in many different ways. In this paper,\\nwe review some of the most prominent LLMs, including three\\npopular LLM families (GPT, LLaMA, PaLM), and discuss their\\ncharacteristics, contributions and limitations. We also give an\\noverview of techniques developed to build, and augment LLMs.\\nWe then survey popular datasets prepared for LLM training,\\nfine-tuning, and evaluation, review widely used LLM evaluation\\nmetrics, and compare the performance of several popular LLMs\\non a set of representative benchmarks. Finally, we conclude\\nthe paper by discussing open challenges and future research\\ndirections.\\nI. I NTRODUCTION\\nLanguage modeling is a long-standing research topic, dat-\\ning back to the 1950s with Shannon’s application of informa-\\ntion theory to human language, where he measured how well\\nsimple n-gram language models predict or compress natural\\nlanguage text [3]. Since then, statistical language modeling\\nbecame fundamental to many natural language understanding\\nand generation tasks, ranging from speech recognition, ma-\\nchine translation, to information retrieval [4], [5], [6].\\nThe recent advances on transformer-based large language\\nmodels (LMs), pretrained on Web-scale text corpora, signifi-\\ncantly extended the capabilities of language models (LLMs).\\nFor example, OpenAI’s ChatGPT and GPT-4 can be used not\\nonly for natural language processing, but also as general task\\nsolvers to power Microsoft’s Co-Pilot systems, for instance,\\ncan follow human instructions of complex new tasks per-\\nforming multi-step reasoning when needed. LLMs are thus\\nbecoming the basic building block for the development of\\ngeneral-purpose AI agents or artificial general intelligence\\n(AGI).\\nAs the field of LLMs is moving fast, with new findings,\\nmodels and techniques being published in a matter of months\\nor weeks [7], [8], [9], [10], [11], AI researchers and practi-\\ntioners often find it challenging to figure out the best recipes\\nto build LLM-powered AI systems for their tasks. This paper\\ngives a timely survey of the recent advances on LLMs. We\\nhope this survey will prove a valuable and accessible resource\\nfor students, researchers and developers.\\nLLMs are large-scale, pre-trained, statistical language mod-\\nels based on neural networks. The recent success of LLMs is\\nan accumulation of decades of research and development of\\nlanguage models, which can be categorized into four waves\\nthat have different starting points and velocity: statistical lan-\\nguage models, neural language models, pre-trained language\\nmodels and LLMs.\\nStatistical language models (SLMs) view text as a sequence\\nof words, and estimate the probability of text as the product\\nof their word probabilities. The dominating form of SLMs\\nare Markov chain models known as the n-gram models,\\nwhich compute the probability of a word conditioned on its\\nimmediate proceeding n − 1 words. Since word probabilities\\nare estimated using word and n-gram counts collected from\\ntext corpora, the model needs to deal with data sparsity (i.e.,\\nassigning zero probabilities to unseen words or n-grams) by\\nusing smoothing, where some probability mass of the model\\nis reserved for unseen n-grams [12]. N-gram models are\\nwidely used in many NLP systems. However, these models\\nare incomplete in that they cannot fully capture the diversity\\nand variability of natural language due to data sparsity.\\nEarly neural language models (NLMs) [13], [14], [15], [16]\\ndeal with data sparsity by mapping words to low-dimensional\\ncontinuous vectors (embedding vectors) and predict the next\\nword based on the aggregation of the embedding vectors of\\nits proceeding words using neural networks. The embedding\\nvectors learned by NLMs define a hidden space where the\\nsemantic similarity between vectors can be readily computed\\nas their distance. This opens the door to computing semantic\\nsimilarity of any two inputs regardless their forms (e.g., queries\\nvs. documents in Web search [17], [18], sentences in different\\nlanguages in machine translation [19], [20]) or modalities (e.g.,\\nimage and text in image captioning [21], [22]). Early NLMs are\\ntask-specific models, in that they are trained on task-specific\\ndata and their learned hidden space is task-specific.\\nPre-trained language models (PLMs), unlike early NLMs,\\nare task-agnostic. This generality also extends to the learned\\narXiv:2402.06196v3  [cs.CL]  23 Mar 2025'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 1, 'page_label': '2'}, page_content='hidden embedding space. The training and inference of PLMs\\nfollows the pre-training and fine-tuning paradigm, where lan-\\nguage models with recurrent neural networks [23] or trans-\\nformers [24], [25], [26] are pre-trained on Web-scale unlabeled\\ntext corpora for general tasks such as word prediction, and then\\nfinetuned to specific tasks using small amounts of (labeled)\\ntask-specific data. Recent surveys on PLMs include [8], [27],\\n[28].\\nLarge language models mainly refer to transformer-based\\nneural language models 1 that contain tens to hundreds of\\nbillions of parameters, which are pre-trained on massive text\\ndata, such as PaLM [31], LLaMA [32], and GPT-4 [33], as\\nsummarized in Table III. Compared to PLMs, LLMs are not\\nonly much larger in model size, but also exhibit stronger\\nlanguage understanding and generation abilities, and more\\nimportantly, emergent abilities that are not present in smaller-\\nscale language models. As illustrated in Fig. 1, these emergent\\nabilities include (1) in-context learning, where LLMs learn\\na new task from a small set of examples presented in the\\nprompt at inference time, (2) instruction following, where\\nLLMs, after instruction tuning, can follow the instructions\\nfor new types of tasks without using explicit examples, and\\n(3) multi-step reasoning, where LLMs can solve a complex\\ntask by breaking down that task into intermediate reasoning\\nsteps as demonstrated in the chain-of-thought prompt [34].\\nLLMs can also be augmented by using external knowledge\\nand tools [35], [36] so that they can effectively interact with\\nusers and environment [37], and continually improve itself\\nusing feedback data collected through interactions (e.g. via\\nreinforcement learning with human feedback (RLHF)).\\nThrough advanced usage and augmentation techniques,\\nLLMs can be deployed as so-called AI agents: artificial entities\\nthat sense their environment, make decisions, and take actions.\\nPrevious research has focused on developing agents for specific\\ntasks and domains. The emergent abilities demonstrated by\\nLLMs make it possible to build general-purpose AI agents\\nbased on LLMs. While LLMs are trained to produce responses\\nin static settings, AI agents need to take actions to interact with\\ndynamic environment. Therefore, LLM-based agents often\\nneed to augment LLMs to e.g., obtain updated information\\nfrom external knowledge bases, verify whether a system action\\nproduces the expected result, and cope with when things do\\nnot go as expected, etc. We will discuss in detail LLM-based\\nagents in Section IV.\\nIn the rest of this paper, Section II presents an overview of\\nstate of the art of LLMs, focusing on three LLM families (GPT,\\nLLaMA and PaLM) and other representative models. Section\\nIII discusses how LLMs are built. Section IV discusses how\\nLLMs are used, and augmented for real-world applications\\nSections V and VI review popular datasets and benchmarks for\\nevaluating LLMs, and summarize the reported LLM evaluation\\nresults. Finally, Section VII concludes the paper by summa-\\nrizing the challenges and future research directions.\\nII. L ARGE LANGUAGE MODELS\\nIn this section we start with a review of early pre-trained\\nneural language models as they are the base of LLMs, and\\n1Recently, several very promising non-transformer LLMs have been pro-\\nposed, such as the LLMs based on structured state space models [29], [30].\\nSee Section VII for more details.\\nthen focus our discussion on three families of LLMs: GPT,\\nLlaMA, and PaLM. Table I provides an overview of some of\\nthese models and their characteristics.\\nA. Early Pre-trained Neural Language Models\\nLanguage modeling using neural networks was pioneered\\nby [38], [39], [40]. Bengio et al. [13] developed one of the first\\nneural language models (NLMs) that are comparable to n-gram\\nmodels. Then, [14] successfully applied NLMs to machine\\ntranslation. The release of RNNLM (an open source NLM\\ntoolkit) by Mikolov [41], [42] helped significantly popularize\\nNLMs. Afterwards, NLMs based on recurrent neural networks\\n(RNNs) and their variants, such as long short-term memory\\n(LSTM) [19] and gated recurrent unit (GRU) [20], were widely\\nused for many natural language applications including machine\\ntranslation, text generation and text classification [43].\\nThen, the invention of the Transformer architecture [44]\\nmarks another milestone in the development of NLMs. By\\napplying self-attention to compute in parallel for every word\\nin a sentence or document an “attention score” to model the\\ninfluence each word has on another, Transformers allow for\\nmuch more parallelization than RNNs, which makes it possible\\nto efficiently pre-train very big language models on large\\namounts of data on GPUs. These pre-trained language models\\n(PLMs) can be fine-tuned for many downstream tasks.\\nWe group early popular Transformer-based PLMs, based on\\ntheir neural architectures, into three main categories: encoder-\\nonly, decoder-only, and encoder-decoder models. Comprehen-\\nsive surveys of early PLMs are provided in [43], [28].\\n1) Encoder-only PLMs: As the name suggests, the encoder-\\nonly models only consist of an encoder network. These models\\nare originally developed for language understanding tasks,\\nsuch as text classification, where the models need to predict a\\nclass label for an input text. Representative encoder-only mod-\\nels include BERT and its variants, e.g., RoBERTa, ALBERT,\\nDeBERTa, XLM, XLNet, UNILM, as to be described below.\\nBERT (Birectional Encoder Representations from Trans-\\nformers) [24] is one of the most widely used encoder-only\\nlanguage models. BERT consists of three modules: (1) an\\nembedding module that converts input text into a sequence\\nof embedding vectors, (2) a stack of Transformer encoders\\nthat converts embedding vectors into contextual representation\\nvectors, and (3) a fully connected layer that converts the\\nrepresentation vectors (at the final layer) to one-hot vectors.\\nBERT is pre-trained uses two objectives: masked language\\nmodeling (MLM) and next sentence prediction. The pre-trained\\nBERT model can be fine-tuned by adding a classifier layer\\nfor many language understanding tasks, ranging from text\\nclassification, question answering to language inference. A\\nhigh-level overview of BERT framework is shown in Fig 3. As\\nBERT significantly improved state of the art on a wide range\\nof language understanding tasks when it was published, the AI\\ncommunity was inspired to develop many similar encoder-only\\nlanguage models based on BERT.\\nRoBERTa [25] significantly improves the robustness of\\nBERT using a set of model design choices and training strate-\\ngies, such as modifying a few key hyperparameters, removing\\nthe next-sentence pre-training objective and training with much'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 2, 'page_label': '3'}, page_content='Emerging\\nBasic\\n Augmented\\nLLM Capabilities\\nReasoning\\nCoding\\nComprehension\\nMultilingual\\nTool\\nutilization\\nWorld\\nknowledge\\nInstruction\\nfollowing\\n In-context\\nlearning\\nInteracting\\nwith users\\nSelf-improvement\\nMulti choice QA\\nWikipedia QA\\nXNLI\\nCrosslingual QA\\nCrosslingual Tasks\\nTranslation\\nReading Comprehension\\nMulti choice QA\\nBoolean QA\\nSimplification\\nSummarization\\nFunction Calling\\nAPI calling\\nLogical\\nSymbolic\\nCommon Sense\\nArithmetic\\nTurn based\\nCompletion\\nTask definition\\nFew-shot\\nSymbolic\\nreference\\nPos/Neg example\\nStep by step\\nsolving\\nTool planning\\nTask\\ndecomposition\\nVirtual acting\\nPhysical acting\\nKnowledge base\\nutilization\\nAssignment\\nplanning\\nSelf-cirtisim\\nSelf-refinement\\nFig. 1: LLM Capabilities.\\nlarger mini-batches and learning rates. ALBERT [45] uses two\\nparameter-reduction techniques to lower memory consumption\\nand increase the training speed of BERT: (1) splitting the\\nembedding matrix into two smaller matrices, and (2) using\\nrepeating layers split among groups. DeBERTa (Decoding-\\nenhanced BERT with disentangled attention) [26] improves the\\nBERT and RoBERTa models using two novel techniques. The\\nfirst is the disentangled attention mechanism, where each word\\nis represented using two vectors that encode its content and\\nposition, respectively, and the attention weights among words\\nare computed using disentangled matrices on their contents and\\nrelative positions, respectively. Second, an enhanced mask de-\\ncoder is used to incorporate absolute positions in the decoding\\nlayer to predict the masked tokens in model pre-training. In\\naddition, a novel virtual adversarial training method is used for\\nfine-tuning to improve models’ generalization. ELECTRA [46]\\nuses a new pre-training task, known as replaced token detection\\n(RTD), which is empirically proven to be more sample-efficient\\nthan MLM. Instead of masking the input, RTD corrupts it by\\nreplacing some tokens with plausible alternatives sampled from\\na small generator network. Then, instead of training a model\\nthat predicts the original identities of the corrupted tokens, a\\ndiscriminative model is trained to predict whether a token in\\nthe corrupted input was replaced by a generated sample or not.\\nRTD is more sample-efficient than MLM because the former\\nis defined over all input tokens rather than just the small subset\\nbeing masked out, as illustrated in Fig 4.\\nXLMs [47] extended BERT to cross-lingual language\\nmodels using two methods: (1) a unsupervised method that\\nonly relies on monolingual data, and (2) a supervised method\\nthat leverages parallel data with a new cross-lingual language\\nmodel objective, as illustrated in Fig 5. XLMs had obtained\\nstate-of-the-art results on cross-lingual classification, unsuper-\\nvised and supervised machine translation, at the time they were\\nproposed.\\nThere are also encoder-only language models that leverage\\nthe advantages of auto-regressive (decoder) models for model\\ntraining and inference. Two examples are XLNet and UNILM.\\nXLNet [48] is based on Transformer-XL, pre-trained using a\\ngeneralized autoregressive method that enables learning bidi-\\nrectional contexts by maximizing the expected likelihood over\\nall permutations of the factorization order. UNILM (UNIfied\\npre-trained Language Model) [49] is pre-trained using three\\ntypes of language modeling tasks: unidirectional, bidirectional,\\nand sequence-to-sequence prediction. This is achieved by\\nemploying a shared Transformer network and utilizing specific\\nself-attention masks to control what context the prediction is\\nconditioned on, as illustrated in Fig 6. The pre-trained model\\ncan be fine-tuned for both natural language understanding and\\ngeneration tasks.\\n2) Decoder-only PLMs: Two of the most widely used\\ndecoder-only PLMs are GPT-1 and GPT-2, developed by\\nOpenAI. These models lay the foundation to more powerful\\nLLMs subsequently, i.e., GPT-3 and GPT-4.\\nGPT-1 [50] demonstrates for the first time that good\\nperformance over a wide range of natural language tasks can be\\nobtained by Generative Pre-Training (GPT) of a decoder-only\\nTransformer model on a diverse corpus of unlabeled text in a\\nself-supervised learning fashion (i.e., next word/token predic-\\ntion), followed by discriminative fine-tuning on each specific\\ndownstream task (with much fewer samples), as illustrated in\\nFig 7. GPT-1 paves the way for subsequent GPT models, with\\neach version improving upon the architecture and achieving\\nbetter performance on various language tasks.\\nGPT-2 [51] shows that language models are able to learn\\nto perform specific natural language tasks without any explicit\\nsupervision when trained on a large WebText dataset consisting\\nof millions of webpages. The GPT-2 model follows the model\\ndesigns of GPT-1 with a few modifications: Layer normal-\\nization is moved to the input of each sub-block, additional\\nlayer normalization is added after the final self-attention block,'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 3, 'page_label': '4'}, page_content='Paper Strcuture\\nEarly Pre-trained\\nLanguage Models\\nII Large Language Models\\nA III HOW LLMS ARE BUILT\\nA\\nData CleaningB\\nLarge Language\\nModel FamiliesB\\nOther Representative\\nLLMsC\\nDominant LLM\\nArchitectures\\nTokenizationsC\\nPositional EncodingD\\nModel Pre-trainingE\\nFine-tuning and\\nInstruction TuningF\\nAlignmentG\\nDecoding StrategiesH\\nI HOW LLMS ARE USED AND AUGMENTED\\nA\\nB\\nLLM limitations\\nCost-Effective Training/Inference,\\nAdaptation & CompressionI\\nUsing LLMs: Prompt Design\\nand Engineering\\nC Augmenting LLMs through\\nexternal knowledge - RAG\\nD Using External Tools\\nE LLM Agents\\nV  POPULAR DATASETS FOR LLMS\\nA Datasets for Basic Tasks: language\\nmodeling/understanding/generation\\nB  Datasets for Emergent: ICL, reasoning,\\ninstruction following\\nC Datasets for Augmented: using\\nexternal knowledge/tools\\nVI  PROMINENT LLMS’ PERFORMANCE\\nON BENCHMARKS\\nA\\nB\\nVII CHALLENGES AND FUTURE DIRECTIONS\\nA Smaller and more efficient\\nLanguage Models\\nLLMs’ Performance on Different Tasks\\nPopular Metrics for Evaluating LLMs\\nB New Post-attention\\nArchitectural Paradigms\\nC Multi-modal Models\\nD Improved LLM Usage and\\nAugmentation techniques\\nD Security and\\nEthical/Responsible AI\\nFig. 2: The paper structure.\\ninitialization is modified to account for the accumulation on\\nthe residual path and scaling the weights of residual layers,\\nvocabulary size is expanded to 50,25, and context size is\\nincreased from 512 to 1024 tokens.\\n3) Encoder-Decoder PLMs: In [52], Raffle et al. shows that\\nalmost all NLP tasks can be cast as a sequence-to-sequence\\ngeneration task. Thus, an encoder-decoder language model, by\\ndesign, is a unified model in that it can perform all natural\\nlanguage understanding and generation tasks. Representative\\nencoder-decoder PLMs we will review below are T5, mT5,\\nMASS, and BART.\\nT5 [52] is a Text-to-Text Transfer Transformer (T5) model,\\nwhere transfer learning is effectively exploited for NLP via an\\nintroduction of a unified framework in which all NLP tasks are\\ncast as a text-to-text generation task. mT5 [53] is a multilingual\\nvariant of T5, which is pre-trained on a new Common Crawl-\\nbased dataset consisting of texts in 101 languages.\\nMASS (MAsked Sequence to Sequence pre-training) [54]\\nadopts the encoder-decoder framework to reconstruct a sen-\\ntence fragment given the remaining part of the sentence. The\\nencoder takes a sentence with randomly masked fragment\\n(several consecutive tokens) as input, and the decoder predicts\\nthe masked fragment. In this way, MASS jointly trains the'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 4, 'page_label': '5'}, page_content='TABLE I: High-level Overview of Popular Language Models\\nType Model Name #Parameters Release Base Models Open\\nSource\\n#Tokens Training dataset\\nBERT 110M, 340M 2018 - ✓ 137B BooksCorpus, English Wikipedia\\nRoBERTa 355M 2019 - ✓ 2.2T BooksCorpus, English Wikipedia, CC-NEWS,\\nSTORIES (a subset of Common Crawl), Reddit\\nEncoder-Only ALBERT 12M, 18M, 60M,\\n235M\\n2019 - ✓ 137B BooksCorpus, English Wikipedia\\nDeBERTa - 2020 - ✓ - BooksCorpus, English Wikipedia, STORIES, Red-\\ndit content\\nXLNet 110M, 340M 2019 - ✓ 32.89B BooksCorpus, English Wikipedia, Giga5, Com-\\nmon Crawl, ClueWeb 2012-B\\nDecoder-only GPT-1 120M 2018 - ✓ 1.3B BooksCorpus\\nGPT-2 1.5B 2019 - ✓ 10B Reddit outbound\\nT5 (Base) 223M 2019 - ✓ 156B Common Crawl\\nEncoder-Decoder MT5 (Base) 300M 2020 - ✓ - New Common Crawl-based dataset in 101 lan-\\nguages (m Common Crawl)\\nBART (Base) 139M 2019 - ✓ - Corrupting text\\nGPT-3 125M, 350M,\\n760M, 1.3B, 2.7B,\\n6.7B, 13B, 175B\\n2020 × 300B Common Crawl (filtered), WebText2, Books1,\\nBooks2, Wikipedia\\nGPT Family CODEX 12B 2021 GPT ✓ - Public GitHub software repositories\\nWebGPT 760M, 13B, 175B 2021 GPT-3 × - ELI5\\nGPT-4 1.76T 2023 - × 13T -\\nLLaMA1 7B, 13B, 33B, 65B 2023 - ✓ 1T, 1.4T Online sources\\nLLaMA2 7B, 13B, 34B, 70B 2023 - ✓ 2T Online sources\\nAlpaca 7B 2023 LLaMA1 ✓ - GPT-3.5\\nVicuna-13B 13B 2023 LLaMA1 ✓ - GPT-3.5\\nLLaMA Family Koala 13B 2023 LLaMA ✓ - Dialogue data\\nMistral-7B 7.3B 2023 ✓ - -\\nCode Llama 34 2023 LLaMA2 ✓ 500B Publicly available code\\nLongLLaMA 3B, 7B 2023 OpenLLaMA ✓ 1T -\\nLLaMA-Pro-8B 8.3B 2024 LLaMA2-7B ✓ 80B Code and math corpora\\nTinyLlama-1.1B 1.1B 2024 LLaMA1.1B ✓ 3T SlimPajama, Starcoderdata\\nPaLM 8B, 62B, 540B 2022 - × 780B Web documents, books, Wikipedia, conversations,\\nGitHub code\\nU-PaLM 8B, 62B, 540B 2022 - × 1.3B Web documents, books, Wikipedia, conversations,\\nGitHub code\\nPaLM Family PaLM-2 340B 2023 - ✓ 3.6T Web documents, books, code, mathematics, con-\\nversational data\\nMed-PaLM 540B 2022 PaLM × 780B HealthSearchQA, MedicationQA, LiveQA\\nMed-PaLM 2 - 2023 PaLM 2 × - MedQA, MedMCQA, HealthSearchQA, LiveQA,\\nMedicationQA\\nFLAN 137B 2021 LaMDA-PT ✓ - Web documents, code, dialog data, Wikipedia\\nGopher 280B 2021 - × 300B MassiveText\\nERNIE 4.0 10B 2023 - × 4TB Chinese text\\nRetro 7.5B 2021 - × 600B MassiveText\\nLaMDA 137B 2022 - × 168B public dialog data and web documents\\nChinChilla 70B 2022 - × 1.4T MassiveText\\nGalactia-120B 120B 2022 - 450B\\nOther Popular LLMs CodeGen 16.1B 2022 - ✓ - THE PILE, BIGQUERY , BIGPYTHON\\nBLOOM 176B 2022 - ✓ 366B ROOTS\\nZephyr 7.24B 2023 Mistral-7B ✓ 800B Synthetic data\\nGrok-0 33B 2023 - × - Online source\\nORCA-2 13B 2023 LLaMA2 - 2001B -\\nStartCoder 15.5B 2023 - ✓ 35B GitHub\\nMPT 7B 2023 - ✓ 1T RedPajama, m Common Crawl, S2ORC, Common\\nCrawl\\nMixtral-8x7B 46.7B 2023 - ✓ - Instruction dataset\\nFalcon 180B 180B 2023 - ✓ 3.5T RefinedWeb\\nGemini 1.8B, 3.25B 2023 ✓ - Web documents, books, and code, image data,\\naudio data, video data\\nDeepSeek-Coder 1.3B, 6.7B, 33B 2024 - ✓ 2T GitHub’s Markdown and StackExchange\\nDocLLM 1B,7B 2024 - × 2T IIT-CDIP Test Collection 1.0, DocBank\\nencoder and decoder for language embedding and generation,\\nrespectively.\\nBART [55] uses a standard sequence-to-sequence transla-\\ntion model architecture. It is pre-trained by corrupting text with\\nan arbitrary noising function, and then learning to reconstruct\\nthe original text.\\nB. Large Language Model Families\\nLarge language models (LLMs) mainly refer to\\ntransformer-based PLMs that contain tens to hundreds\\nof billions of parameters. Compared to PLMs reviewed above,\\nLLMs are not only much larger in model size, but also exhibit\\nstronger language understanding and generation and emergent\\nabilities that are not present in smaller-scale models. In what\\nfollows, we review three LLM families: GPT, LLaMA, and\\nPaLM, as illustrated in Fig 8.\\n1) The GPT Family: Generative Pre-trained Transform-\\ners (GPT) are a family of decoder-only Transformer-based\\nlanguage models, developed by OpenAI. This family con-\\nsists of GPT-1, GPT-2, GPT-3, InstrucGPT, ChatGPT, GPT-4,'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 5, 'page_label': '6'}, page_content='Fig. 3: Overall pre-training and fine-tuning procedures for\\nBERT. Courtesy of [24]\\nFig. 4: A comparison between replaced token detection and\\nmasked language modeling. Courtesy of [46].\\nCODEX, and WebGPT. Although early GPT models, such as\\nGPT-1 and GPT-2, are open-source, recent models, such as\\nGPT-3 and GPT-4, are close-source and can only be accessed\\nvia APIs. GPT-1 and GPT-2 models have been discussed in\\nthe early PLM subsection. We start with GPT-3 below.\\nGPT-3 [56] is a pre-trained autoregressive language model\\nwith 175 billion parameters. GPT-3 is widely considered as\\nthe first LLM in that not only it is much larger than previous\\nPLMs, but also for the first time demonstrates emergent\\nabilities that are not observed in previous smaller PLMs. GPT-\\n3 shows the emergent ability of in-context learning, which\\nmeans GPT-3 can be applied to any downstream tasks without\\nany gradient updates or fine-tuning, with tasks and few-shot\\ndemonstrations specified purely via text interaction with the\\nmodel. GPT-3 achieved strong performance on many NLP\\ntasks, including translation, question-answering, and the cloze\\ntasks, as well as several ones that require on-the-fly reasoning\\nor domain adaptation, such as unscrambling words, using a\\nnovel word in a sentence, 3-digit arithmetic. Fig 9 plots the\\nperformance of GPT-3 as a function of the number of examples\\nin in-context prompts.\\nCODEX [57], released by OpenAI in March 2023, is a\\ngeneral-purpose programming model that can parse natural\\nlanguage and generate code in response. CODEX is a de-\\nscendant of GPT-3, fine-tuned for programming applications\\non code corpora collected from GitHub. CODEX powers\\nMicrosoft’s GitHub Copilot.\\nWebGPT [58] is another descendant of GPT-3, fine-tuned to\\nanswer open-ended questions using a text-based web browser,\\nfacilitating users to search and navigate the web. Specifically,\\nFig. 5: Cross-lingual language model pretraining. The MLM\\nobjective is similar to BERT, but with continuous streams\\nof text as opposed to sentence pairs. The TLM objective\\nextends MLM to pairs of parallel sentences. To predict a\\nmasked English word, the model can attend to both the English\\nsentence and its French translation, and is encouraged to align\\nEnglish and French representations. Courtesy of [47].\\nFig. 6: Overview of unified LM pre-training. The model\\nparameters are shared across the LM objectives (i.e., bidirec-\\ntional LM, unidirectional LM, and sequence-to-sequence LM).\\nCourtesy of [49].\\nWebGPT is trained in three steps. The first is for WebGPT\\nto learn to mimic human browsing behaviors using human\\ndemonstration data. Then, a reward function is learned to\\npredict human preferences. Finally, WebGPT is refined to\\noptimize the reward function via reinforcement learning and\\nrejection sampling.\\nTo enable LLMs to follow expected human instructions,\\nInstructGPT [59] is proposed to align language models with\\nuser intent on a wide range of tasks by fine-tuning with\\nhuman feedback. Starting with a set of labeler-written prompts\\nand prompts submitted through the OpenAI API, a dataset\\nof labeler demonstrations of the desired model behavior is\\ncollected. Then GPT-3 is fine-tuned on this dataset. Then, a\\ndataset of human-ranked model outputs is collected to further\\nfine-tune the model using reinforcement learning. The method\\nis known Reinforcement Learning from Human Feedback\\n(RLHF), as shown in 10. The resultant InstructGPT models\\nhave shown improvements in truthfulness and reductions in\\ntoxic output generation while having minimal performance'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 6, 'page_label': '7'}, page_content='Fig. 7: High-level overview of GPT pretraining, and fine-tuning\\nsteps. Courtesy of OpenAI.\\nregressions on public NLP datasets.\\nThe most important milestone of LLM development is the\\nlaunch of ChatGPT (Chat Generative Pre-trained Transformer)\\n[60] on November 30, 2022. ChatGPT is chatbot that enables\\nusers to steer a conversation to complete a wide range of\\ntasks such as question answering, information seeking, text\\nsummarization, and more. ChatGPT is powered by GPT-3.5\\n(and later by GPT-4), a sibling model to InstructGPT, which\\nis trained to follow an instruction in a prompt and provide a\\ndetailed response.\\nGPT-4 [33] is the latest and most powerful LLM in the\\nGPT family. Launched in March, 2023, GPT-4 is a multi-\\nmodal LLM in that it can take image and text as inputs and\\nproduce text outputs. While still less capable than humans\\nin some of the most challenging real-world scenarios, GPT-4\\nexhibits human-level performance on various professional and\\nacademic benchmarks, including passing a simulated bar exam\\nwith a score around the top 10% of test takers, as shown in\\nFig 11. Like early GPT models, GPT-4 was first pre-trained to\\npredict next tokens on large text corpora, and then fine-tuned\\nwith RLHF to align model behaviors with human-desired ones.\\n2) The LLaMA Family: LLaMA is a collection of founda-\\ntion language models, released by Meta. Unlike GPT models,\\nLLaMA models are open-source, i.e., model weights are\\nreleased to the research community under a noncommercial\\nlicense. Thus, the LLaMA family grows rapidly as these\\nmodels are widely used by many research groups to develop\\nbetter open-source LLMs to compete the closed-source ones or\\nto develop task-specific LLMs for mission-critical applications.\\nThe first set of LLaMA models [32] was released in Febru-\\nary 2023, ranging from 7B to 65B parameters. These models\\nare pre-trained on trillions of tokens, collected from publicly\\navailable datasets. LLaMA uses the transformer architecture of\\nGPT-3, with a few minor architectural modifications, including\\n(1) using a SwiGLU activation function instead of ReLU,\\n(2) using rotary positional embeddings instead of absolute\\npositional embedding, and (3) using root-mean-squared layer-\\nnormalization instead of standard layer-normalization. The\\nopen-source LLaMA-13B model outperforms the proprietary\\nGPT-3 (175B) model on most benchmarks, making it a good\\nbaseline for LLM research.\\nIn July 2023, Meta, in partnership with Microsoft, released\\nthe LLaMA-2 collection [61], which include both foundation\\nlanguage models and Chat models finetuned for dialog, known\\nas LLaMA-2 Chat. The LLaMA-2 Chat models were reported\\nto outperform other open-source models on many public\\nbenchmarks. Fig 12 shows the training process of LLaMA-2\\nChat. The process begins with pre-training LLaMA-2 using\\npublicly available online data. Then, an initial version of\\nLLaMA-2 Chat is built via supervised fine-tuning. Subse-\\nquently, the model is iteratively refined using RLHF, rejection\\nsampling and proximal policy optimization. In the RLHF stage,\\nthe accumulation of human feedback for revising the reward\\nmodel is crucial to prevent the reward model from being\\nchanged too much, which could hurt the stability of LLaMA\\nmodel training.\\nAlpaca [62] is fine-tuned from the LLaMA-7B model using\\n52K instruction-following demonstrations generated in the\\nstyle of self-instruct using GPT-3.5 (text-davinci-003). Alpaca\\nis very cost-effective for training, especially for academic\\nresearch. On the self-instruct evaluation set, Alpaca performs\\nsimilarly to GPT-3.5, despite that Alpaca is much smaller.\\nThe Vicuna team has developed a 13B chat model, Vicuna-\\n13B, by fine-tuning LLaMA on user-shared conversations\\ncollected from ShareGPT. Preliminary evaluation using GPT-\\n4 as a evaluator shows that Vicuna-13B achieves more than\\n90% quality of OpenAI’s ChatGPT, and Google’s Bard while\\noutperforming other models like LLaMA and Stanford Alpaca\\nin more than 90% of cases. 13 shows the relative response\\nquality of Vicuna and a few other well-known models by\\nGPT-4. Another advantage of Vicuna-13B is its relative limited\\ncomputational demand for model training. The training cost of\\nVicuna-13B is merely $300.\\nLike Alpaca and Vicuna, the Guanaco models [63] are also\\nfinetuned LLaMA models using instruction-following data. But\\nthe finetuning is done very efficiently using QLoRA such\\nthat finetuning a 65B parameter model can be done on a\\nsingle 48GB GPU. QLoRA back-propagates gradients through\\na frozen, 4-bit quantized pre-trained language model into Low\\nRank Adapters (LoRA). The best Guanaco model outperforms\\nall previously released models on the Vicuna benchmark,\\nreaching 99.3% of the performance level of ChatGPT while\\nonly requiring 24 hours of fine-tuning on a single GPU.\\nKoala [64] is yet another instruction-following language\\nmodel built on LLaMA, but with a specific focus on interaction\\ndata that include user inputs and responses generated by highly\\ncapable closed-source chat models such as ChatGPT. The\\nKoala-13B model performs competitively with state-of-the-art\\nchat models according to human evaluation based on real-\\nworld user prompts.\\nMistral-7B [65] is a 7B-parameter language model engi-\\nneered for superior performance and efficiency. Mistral-7B\\noutperforms the best open-source 13B model (LLaMA-2-13B)\\nacross all evaluated benchmarks, and the best open-source\\n34B model (LLaMA-34B) in reasoning, mathematics, and code\\ngeneration. This model leverages grouped-query attention for\\nfaster inference, coupled with sliding window attention to\\neffectively handle sequences of arbitrary length with a reduced\\ninference cost.\\nThe LLaMA family is growing rapidly, as more instruction-\\nfollowing models have been built on LLaMA or LLaMA-\\n2, including Code LLaMA [66], Gorilla [67], Giraffe [68],'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 7, 'page_label': '8'}, page_content='GPT Family PaLM Family   LLaMA 1/2 Family\\nGPT \\nGPT1\\nGPT2\\nGPT3\\nGPT4\\nGPT3.5 Turbo\\ntext-davinci\\n code-davinci\\nCODEX\\nInstructGPT\\nWebGPT\\nGPT4 Vision\\n GPT4 Turbo\\nGorilla\\nMistral\\nVigogne\\nStable Beluga2\\nKoala\\nCode LLaMA\\nVicuna\\n Alpaca\\nBaize\\nLong LLaMA\\nGiraffe\\nGuanaco\\nTulu\\nWizardLM\\nMed-PaLM\\nPaLM-E\\nMed-PaLM2\\nFLAN-PaLM\\nU-PaLM\\nPaLM2\\nPaLM\\nFig. 8: Popular LLM Families.\\nFig. 9: GPT-3 shows that larger models make increasingly\\nefficient use of in-context information. It shows in-context\\nlearning performance on a simple task requiring the model to\\nremove random symbols from a word, both with and without\\na natural language task description. Courtesy of [56].\\nFig. 10: The high-level overview of RLHF. Courtesy of [59].\\nVigogne [69], Tulu 65B [70], Long LLaMA [71], and Stable\\nBeluga2 [72], just to name a few.\\n3) The PaLM Family: The PaLM (Pathways Language\\nModel) family are developed by Google. The first PaLM\\nmodel [31] was announced in April 2022 and remained private\\nFig. 11: GPT-4 performance on academic and professional\\nexams, compared with GPT 3.5. Courtesy of [33].\\nuntil March 2023. It is a 540B parameter transformer-based\\nLLM. The model is pre-trained on a high-quality text corpus\\nconsisting of 780 billion tokens that comprise a wide range\\nof natural language tasks and use cases. PaLM is pre-trained\\non 6144 TPU v4 chips using the Pathways system, which\\nenables highly efficient training across multiple TPU Pods.\\nPaLM demonstrates continued benefits of scaling by achiev-\\ning state-of-the-art few-shot learning results on hundreds of\\nlanguage understanding and generation benchmarks. PaLM-\\n540B outperforms not only state-of-the-art fine-tuned models\\non a suite of multi-step reasoning tasks, but also on par with\\nhumans on the recently released BIG-bench benchmark.\\nThe U-PaLM models of 8B, 62B, and 540B scales are\\ncontinually trained on PaLM with UL2R, a method of continue\\ntraining LLMs on a few steps with UL2’s mixture-of-denoiser\\nobjective [73]. An approximately 2x computational savings\\nrate is reported.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 8, 'page_label': '9'}, page_content='Fig. 12: Training of LLaMA-2 Chat. Courtesy of [61].\\nFig. 13: Relative Response Quality of Vicuna and a few other\\nwell-known models by GPT-4. Courtesy of Vicuna Team.\\nU-PaLM is later instruction-finetuned as Flan-PaLM [74].\\nCompared to other instruction finetuning work mentioned\\nabove, Flan-PaLM’s finetuning is performed using a much\\nlarger number of tasks, larger model sizes, and chain-of-\\nthought data. As a result, Flan-PaLM substantially outperforms\\nprevious instruction-following models. For instance, Flan-\\nPaLM-540B, which is instruction-finetuned on 1.8K tasks,\\noutperforms PaLM-540B by a large margin (+9.4% on av-\\nerage). The finetuning data comprises 473 datasets, 146 task\\ncategories, and 1,836 total tasks, as illustrated in Fig 14.\\nFig. 14: Flan-PaLM finetuning consist of 473 datasets in above\\ntask categories. Courtesy of [74].\\nPaLM-2 [75] is a more compute-efficient LLM with bet-\\nter multilingual and reasoning capabilities, compared to its\\npredecessor PaLM. PaLM-2 is trained using a mixture of\\nobjectives. Through extensive evaluations on English, multi-\\nlingual, and reasoning tasks, PaLM-2 significantly improves\\nthe model performance on downstream tasks across different\\nmodel sizes, while simultaneously exhibiting faster and more\\nefficient inference than PaLM.\\nMed-PaLM [76] is a domain-specific PaLM, and is de-\\nsigned to provide high-quality answers to medical questions.\\nMed-PaLM is finetuned on PaLM using instruction prompt\\ntuning, a parameter-efficient method for aligning LLMs to\\nnew domains using a few exemplars. Med-PaLM obtains very\\nencouraging results on many healthcare tasks, although it is\\nstill inferior to human clinicians. Med-PaLM 2 improves Med-\\nPaLM via med-domain finetuning and ensemble prompting\\n[77]. Med-PaLM 2 scored up to 86.5% on the MedQA\\ndataset (i.e., a benchmark combining six existing open ques-\\ntion answering datasets spanning professional medical exams,\\nresearch, and consumer queries), improving upon Med-PaLM\\nby over 19% and setting a new state-of-the-art.\\nC. Other Representative LLMs\\nIn addition to the models discussed in the previous sub-\\nsections, there are other popular LLMs which do not belong\\nto those three model families, yet they have achieved great\\nperformance and have pushed the LLMs field forward. We\\nbriefly describe these LLMs in this subsection.\\nFLAN: In [78], Wei et al. explored a simple method for\\nimproving the zero-shot learning abilities of language models.\\nThey showed that instruction tuning language models on a\\ncollection of datasets described via instructions substantially\\nimproves zero-shot performance on unseen tasks. They take\\na 137B parameter pretrained language model and instruction\\ntune it on over 60 NLP datasets verbalized via natural language\\ninstruction templates. They call this instruction-tuned model\\nFLAN. Fig 15 provides a comparison of instruction tuning\\nwith pretrain–finetune and prompting.\\nFig. 15: comparison of instruction tuning with pre-\\ntrain–finetune and prompting. Courtesy of [78].\\nGopher: In [79], Rae et al. presented an analysis of\\nTransformer-based language model performance across a wide\\nrange of model scales — from models with tens of millions of\\nparameters up to a 280 billion parameter model called Gopher.\\nThese models were evaluated on 152 diverse tasks, achieving\\nstate-of-the-art performance across the majority. The number\\nof layers, the key/value size, and other hyper-parameters of\\ndifferent model sizes are shown in Fig 16.\\nT0: In [80], Sanh et al. developed T0, a system for easily\\nmapping any natural language tasks into a human-readable\\nprompted form. They converted a large set of supervised'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 9, 'page_label': '10'}, page_content='Fig. 16: Model architecture details of Gopher with different\\nnumber of parameters. Courtesy of [78].\\ndatasets, each with multiple prompts with diverse wording.\\nThese prompted datasets allow for benchmarking the ability\\nof a model to perform completely held-out tasks. Then, a\\nT0 encoder-decoder model is developed to consume textual\\ninputs and produces target responses. The model is trained on\\na multitask mixture of NLP datasets partitioned into different\\ntasks.\\nERNIE 3.0: In [81], Sun et al. proposed a unified frame-\\nwork named ERNIE 3.0 for pre-training large-scale knowledge\\nenhanced models. It fuses auto-regressive network and auto-\\nencoding network, so that the trained model can be easily tai-\\nlored for both natural language understanding and generation\\ntasks using zero-shot learning, few-shot learning or fine-tuning.\\nThey have trained ERNIE 3.0 with 10 billion parameters\\non a 4TB corpus consisting of plain texts and a large-scale\\nknowledge graph. Fig 17 illustrates the model architecture of\\nErnie 3.0.\\nFig. 17: High-level model architecture of ERNIE 3.0. Courtesy\\nof [81].\\nRETRO: In [82], Borgeaud et al. enhanced auto-regressive\\nlanguage models by conditioning on document chunks re-\\ntrieved from a large corpus, based on local similarity with pre-\\nceding tokens. Using a 2-trillion-token database, the Retrieval-\\nEnhanced Transformer (Retro) obtains comparable perfor-\\nmance to GPT-3 and Jurassic-1 [83] on the Pile, despite using\\n25% fewer parameters. As shown in Fig 18, Retro combines\\na frozen Bert retriever, a differentiable encoder and a chunked\\ncross-attention mechanism to predict tokens based on an order\\nof magnitude more data than what is typically consumed\\nduring training.\\nGLaM: In [84], Du et al. proposed a family of LLMs\\nnamed GLaM (Generalist Language Model), which use a\\nsparsely activated mixture-of-experts architecture to scale the\\nFig. 18: Retro architecture. Left: simplified version where a\\nsequence of length n = 12 is split into l = 3 chunks of size\\nm = 4. For each chunk, we retrieve k = 2 neighbours of r =\\n5 tokens each. The retrieval pathway is shown on top. Right:\\nDetails of the interactions in the CCA operator. Causality is\\nmaintained as neighbours of the first chunk only affect the last\\ntoken of the first chunk and tokens from the second chunk.\\nCourtesy of [82].\\nmodel capacity while also incurring substantially less training\\ncost compared to dense variants. The largest GLaM has 1.2\\ntrillion parameters, which is approximately 7x larger than GPT-\\n3. It consumes only 1/3 of the energy used to train GPT-3 and\\nrequires half of the computation flops for inference, while still\\nachieving better overall zero, one and few-shot performance\\nacross 29 NLP tasks. Fig 19 shows the high-level architecture\\nof GLAM.\\nFig. 19: GLaM model architecture. Each MoE layer (the\\nbottom block) is interleaved with a Transformer layer (the\\nupper block). Courtesy of [84].\\nLaMDA: In [85], Thoppilan et al. presented LaMDA, a\\nfamily of Transformer-based neural language models special-\\nized for dialog, which have up to 137B parameters and are\\npre-trained on 1.56T words of public dialog data and web text.\\nThey showed that fine-tuning with annotated data and enabling\\nthe model to consult external knowledge sources can lead to\\nsignificant improvements towards the two key challenges of\\nsafety and factual grounding.\\nOPT: In [86], Zhang et al. presented Open Pre-trained\\nTransformers (OPT), a suite of decoder-only pre-trained trans-\\nformers ranging from 125M to 175B parameters, which they'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 10, 'page_label': '11'}, page_content='share with researchers. The OPT models’ parameters are\\nshown in 20\\nFig. 20: Different OPT Models’ architecture details. Courtesy\\nof [86].\\nChinchilla: In [2], Hoffmann et al. investigated the optimal\\nmodel size and number of tokens for training a transformer\\nlanguage model under a given compute budget. By training\\nover 400 language models ranging from 70 million to over\\n16 billion parameters on 5 to 500 billion tokens, they found\\nthat for compute-optimal training, the model size and the\\nnumber of training tokens should be scaled equally: for every\\ndoubling of model size the number of training tokens should\\nalso be doubled. They tested this hypothesis by training a\\npredicted compute-optimal model, Chinchilla, that uses the\\nsame compute budget as Gopher but with 70B parameters and\\n4% more more data.\\nGalactica: In [87], Taylor et al. introduced Galactica, a\\nlarge language model that can store, combine and reason about\\nscientific knowledge. They trained on a large scientific corpus\\nof papers, reference material, knowledge bases and many other\\nsources. Galactica performed well on reasoning, outperforming\\nChinchilla on mathematical MMLU by 41.3% to 35.7%, and\\nPaLM 540B on MATH with a score of 20.4% versus 8.8%.\\nCodeGen: In [88], Nijkamp et al. trained and released\\na family of large language models up to 16.1B parameters,\\ncalled CODEGEN, on natural language and programming\\nlanguage data, and open sourced the training library JAX-\\nFORMER. They showed the utility of the trained model by\\ndemonstrating that it is competitive with the previous state-of-\\nthe-art on zero-shot Python code generation on HumanEval.\\nThey further investigated the multi-step paradigm for program\\nsynthesis, where a single program is factorized into multi-\\nple prompts specifying sub-problems. They also constructed\\nan open benchmark, Multi-Turn Programming Benchmark\\n(MTPB), consisting of 115 diverse problem sets that are\\nfactorized into multi-turn prompts.\\nAlexaTM: In [89], Soltan et al. demonstrated that mul-\\ntilingual large-scale sequence-to-sequence (seq2seq) models,\\npre-trained on a mixture of denoising and Causal Language\\nModeling (CLM) tasks, are more efficient few-shot learners\\nthan decoder-only models on various task. They trained a\\n20 billion parameter multilingual seq2seq model called Alexa\\nTeacher Model (AlexaTM 20B) and showed that it achieves\\nstate-of-the-art (SOTA) performance on 1-shot summarization\\ntasks, outperforming a much larger 540B PaLM decoder\\nmodel. AlexaTM consist of 46 encoder layers, 32 decoder\\nlayers, 32 attention heads, and dmodel = 4096.\\nSparrow: In [90], Glaese et al. presented Sparrow, an\\ninformation-seeking dialogue agent trained to be more helpful,\\ncorrect, and harmless compared to prompted language model\\nbaselines. They used reinforcement learning from human feed-\\nback to train their models with two new additions to help\\nhuman raters judge agent behaviour. The high-level pipeline\\nof Sparrow model is shown in Fig 21.\\nFig. 21: Sparrow pipeline relies on human participation to\\ncontinually expand a training set. Courtesy of [90].\\nMinerva: In [91], Lewkowycz et al. introduced Minerva,\\na large language model pretrained on general natural language\\ndata and further trained on technical content, to tackle previous\\nLLM struggle with quantitative reasoning (such as solving\\nmathematics, science, and engineering problems).\\nMoD: In [92], Tay et al. presented a generalized and\\nunified perspective for self-supervision in NLP and show how\\ndifferent pre-training objectives can be cast as one another\\nand how interpolating between different objectives can be\\neffective. They proposed Mixture-of-Denoisers (MoD), a pre-\\ntraining objective that combines diverse pre-training paradigms\\ntogether. This framework is known as Unifying Language\\nLearning (UL2). An overview of UL2 pretraining paradigm\\nis shown in Fig 21.\\nFig. 22: An overview of UL2 pretraining paradigm. Courtesy\\nof [92].\\nBLOOM: In [93], Scao et al. presented BLOOM, a 176B-\\nparameter open-access language model designed and built\\nthanks to a collaboration of hundreds of researchers. BLOOM\\nis a decoder-only Transformer language model trained on the\\nROOTS corpus, a dataset comprising hundreds of sources in\\n46 natural and 13 programming languages (59 in total). An\\noverview of BLOOM architecture is shown in Fig 23.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 11, 'page_label': '12'}, page_content='Fig. 23: An overview of BLOOM architecture. Courtesy of\\n[93].\\nGLM: In [94], Zeng et al. introduced GLM-130B, a\\nbilingual (English and Chinese) pre-trained language model\\nwith 130 billion parameters. It was an attempt to open-source\\na 100B-scale model at least as good as GPT-3 (davinci) and\\nunveil how models of such a scale can be successfully pre-\\ntrained.\\nPythia: In [95], Biderman et al. introduced Pythia, a suite\\nof 16 LLMs all trained on public data seen in the exact same\\norder and ranging in size from 70M to 12B parameters. We\\nprovide public access to 154 checkpoints for each one of the\\n16 models, alongside tools to download and reconstruct their\\nexact training dataloaders for further study.\\nOrca: In [96], Mukherjee et al. develop Orca, a 13-billion\\nparameter model that learns to imitate the reasoning process\\nof large foundation models. Orca learns from rich signals\\nfrom GPT-4 including explanation traces; step-by-step thought\\nprocesses; and other complex instructions, guided by teacher\\nassistance from ChatGPT.\\nStarCoder: In [97], Li et al. introduced StarCoder and\\nStarCoderBase. They are 15.5B parameter models with 8K\\ncontext length, infilling capabilities and fast large-batch in-\\nference enabled by multi-query attention. StarCoderBase is\\ntrained on one trillion tokens sourced from The Stack, a\\nlarge collection of permissively licensed GitHub repositories\\nwith inspection tools and an opt-out process. They fine-tuned\\nStarCoderBase on 35B Python tokens, resulting in the creation\\nof StarCoder. They performed the most comprehensive evalu-\\nation of Code LLMs to date and showed that StarCoderBase\\noutperforms every open Code LLM that supports multiple pro-\\ngramming languages and matches or outperforms the OpenAI\\ncode-cushman-001 model.\\nKOSMOS: In [98], Huang et al. introduced KOSMOS-1,\\na Multimodal Large Language Model (MLLM) that can per-\\nceive general modalities, learn in context (i.e., few-shot), and\\nfollow instructions (i.e. zero-shot). Specifically, they trained\\nKOSMOS-1 from scratch on web-scale multi-modal corpora,\\nincluding arbitrarily interleaved text and images, image-caption\\npairs, and text data. Experimental results show that KOSMOS-\\n1 achieves impressive performance on (i) language understand-\\ning, generation, and even OCR-free NLP (directly fed with\\ndocument images), (ii) perception-language tasks, including\\nmultimodal dialogue, image captioning, visual question an-\\nswering, and (iii) vision tasks, such as image recognition with\\ndescriptions (specifying classification via text instructions).\\nGemini: In [99], Gemini team introduced a new family of\\nmultimodal models, that exhibit promising capabilities across\\nimage, audio, video, and text understanding. Gemini family\\nincludes three versions: Ultra for highly-complex tasks, Pro\\nfor enhanced performance and deployability at scale, and Nano\\nfor on-device applications. Gemini architecture is built on top\\nof Transformer decoders, and is trained to support 32k context\\nlength (via using efficient attention mechanisms).\\nSome of the other popular LLM frameworks (or techniques\\nused for efficient developments of LLMs) includes Inner-\\nMonologue [100], Megatron-Turing NLG [101], LongFormer\\n[102], OPT-IML [103], MeTaLM [104], Dromedary [105],\\nPalmyra [106], Camel [107], Yalm [108], MPT [109], ORCA-\\n2 [110], Gorilla [67], PAL [111], Claude [112], CodeGen 2\\n[113], Zephyr [114], Grok [115], Qwen [116], Mamba [30],\\nMixtral-8x7B [117], DocLLM [118], DeepSeek-Coder [119],\\nFuseLLM-7B [120], TinyLlama-1.1B [121], LLaMA-Pro-8B\\n[122].\\nFig 24 provides an overview of some of the most repre-\\nsentative LLM frameworks, and the relevant works that have\\ncontributed to the success of LLMs and helped to push the\\nlimits of LLMs.\\nIII. H OW LLM S ARE BUILT\\nIn this section, we first review the popular architectures\\nused for LLMs, and then discuss data and modeling techniques\\nranging from data preparation, tokenization, to pre-training,\\ninstruction tuning, and alignment.\\nOnce the model architecture is chosen, the major steps\\ninvolved in training an LLM includes: data preparation (col-\\nlection, cleaning, deduping, etc.), tokenization, model pre-\\ntraining (in a self-supervised learning fashion), instruction\\ntuning, and alignment. We will explain each of them in a\\nseparate subsection below. These steps are also illustrated in\\nFig 25.\\nA. Dominant LLM Architectures\\nThe most widely used LLM architectures are encoder-only,\\ndecoder-only, and encoder-decoder. Most of them are based on\\nTransformer (as the building block). Therefore we also review\\nthe Transformer architecture here.\\n1) Transformer: in a ground-breaking work [44], Vaswani\\net al. proposed the Transformer framework, which was orig-\\ninally designed for effective parallel computing using GPUs.\\nThe heart of Transformer is the (self-)attention mechanism,\\nwhich can capture long-term contextual information much\\nmore effectively using GPUs than the recurrence and convo-\\nlution mechanisms. Fig 26 provides a high-level overview of\\ntransformer work. In this section we provide an overview of the\\nmain elements and variants, see [44], [123] for more details.\\nThe Transformer language model architecture, originally\\nproposed for machine translation, consists of an encoder and\\na decoder. The encoder is composed of a stack of N = 6\\nidentical Transformer layers. Each layer has two sub-layers.\\nThe first one is a multi-head self-attention layer, and the other\\none is a simple position-wise fully connected feed-forward\\nnetwork. The decoder is composed of a stack of 6 identical\\nlayers. In addition to the two sub-layers in each encoder layer,'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 12, 'page_label': '13'}, page_content='Fig. 24: Timeline of some of the most representative LLM frameworks (so far). In addition to large language models with our\\n#parameters threshold, we included a few representative works, which pushed the limits of language models, and paved the way\\nfor their success (e.g. vanilla Transformer, BERT, GPT-1), as well as some small language models. ♣ shows entities that serve\\nnot only as models but also as approaches. ♦ shows only approaches.\\nthe decoder has a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. The attention\\nfunction can be described as mapping a query and a set of key-\\nvalue pairs to an output, where the query, keys, values, and\\noutput are all vectors. The output is computed as a weighted\\nsum of the values, where the weight assigned to each value\\nis computed by a compatibility function of the query with the\\ncorresponding key. Instead of performing a single attention\\nfunction with dmodel dimensional keys, values and queries,\\nit is found to be beneficial to linearly project the queries,\\nkeys and values h with different, learned linear projections to\\ndk, dk and dv dimensions, respectively. Positional encoding is\\nincorporated to fuse information about the relative or absolute\\nposition of the tokens in the sequence.\\n2) Encoder-Only: For this family, at each stage, the atten-\\ntion layers can access all the words in the initial sentence.\\nThe pre-training of these models usually consist of some-\\nhow corrupting a given sentence (for instance, by masking\\nrandom words in it) and tasking the model with finding or\\nreconstructing the initial sentence. Encoder models are great\\nfor tasks requiring an understanding of the full sequence,\\nsuch as sentence classification, named entity recognition, and\\nextractive question answering. One prominent encoder only\\nmodel is BERT (Bidirectional Encoder Representations from\\nTransformers), proposed in [24].\\n3) Decoder-Only: For these models, at each stage, for any\\nword, the attention layers can only access the words positioned\\nbefore that in the sentence. These models are also sometimes\\ncalled auto-regressive models. The pretraining of these models\\nis usually formulated as predicting the next word (or token)\\nin the sequence. The decoder-only models are best suited for\\ntasks involving text generation. GPT models are prominent\\nexample of this model category.\\n4) Encoder-Decoder: These models use both encoder and\\ndecoder, and are sometimes called sequence-to-sequence mod-\\nels. At each stage, the attention layers of the encoder can access\\nall the words in the initial sentence, whereas the attention\\nlayers of the decoder only accesses the words positioned before\\na given word in the input. These models are usually pre-\\ntrained using the objectives of encoder or decoder models, but\\nusually involve something a bit more complex. For instance,\\nsome models are pretrained by replacing random spans of text\\n(that can contain several words) with a single mask special\\nword, and the objective is then to predict the text that this\\nmask word replaces. Encoder-decoder models are best suited\\nfor tasks about generating new sentences conditioned on a\\ngiven input, such as summarization, translation, or generative\\nquestion answering.\\nB. Data Cleaning\\nData quality is crucial to the performance of language\\nmodels trained on them. Data cleaning techniques such as\\nfiltering, deduplication, are shown to have a big impact on\\nthe model performance.\\nAs an example, in Falcon40B [124], Penedo et al. showed\\nthat properly filtered and deduplicated web data alone can lead\\nto powerful models; even significantly outperforming models\\nfrom the state-of-the-art trained on The Pile. Despite extensive\\nfiltering, they were able to obtain five trillion tokens from'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 13, 'page_label': '14'}, page_content='How LLMs Are Built?\\nData Cleaning\\nTokenizations\\nBytePairEncoding\\nWordPieceEncoding\\nSentencePieceEncoding\\nPositional Encoding\\nAbsolute Positional Embeddings\\nRelative Positional Embeddings\\nRotary Position Embeddings\\nRelative Positional Bias\\nModel Pre-training\\nMasked Language Modeling\\nCausal Language Modeling\\nNext Sentence Prediction\\nMixture of Experts\\nFine-tuning and Instruction Tuning\\nAlignment\\nSupervised learning\\nReinforcement Learning from Human Feedback\\nDirect Preference Optimization\\nKahneman-Tversky Optimization\\nDecoding Strategies\\nGreedy Search\\nBeam Search\\nTop-k Sampling\\nTop-p Sampling\\nCost-Effective Training/Inference,\\nAdaptation & Compression\\nOptimized Training\\nZero Redundancy Optimizer\\nReceptance Weighted Key Value\\nLow-Rank Adaption\\nKnowledge Distillation\\nQuantization\\nData Filtering\\nRemoving Noise\\nHandling Outliers\\nAddressing Imbalances\\nText Preprocessing\\nDeduplication\\nLLM Architectures\\nEncoder-Only\\nDecoder-Only\\nEncoder-Decoder\\n...\\nSupervised Fine-tuning\\nGeneral Fine-tuning\\nMulti-turn Instructions\\nInstruction Following\\nFig. 25: This figure shows different components of LLMs.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 14, 'page_label': '15'}, page_content='Fig. 26: High-level overview of transformer work. Courtesy of\\n[44].\\nCommonCrawl. They also released an extract of 600 billion\\ntokens from our REFINEDWEB dataset, and 1.3/7.5B param-\\neters language models trained on it. 27 shows the Refinement\\nprocess of CommonCrawl data by this work.\\nFig. 27: Subsequent stages of Macrodata Refinement remove\\nnearly 90% of the documents originally in CommonCrawl.\\nCourtesy of [124].\\n1) Data Filtering: Data filtering aims to enhance the qual-\\nity of training data and the effectiveness of the trained LLMs.\\nCommon data filtering techniques include:\\nRemoving Noise: refers to eliminating irrelevant or noisy\\ndata that might impact the model’s ability to generalize well.\\nAs an example, one can think of removing false information\\nfrom the training data, to lower the chance of model generating\\nfalse responses. Two mainstream approaches for quality filter-\\ning includes: classifier-based, and heuristic-based frameworks.\\nHandling Outliers: Identifying and handling outliers or\\nanomalies in the data to prevent them from disproportionately\\ninfluencing the model.\\nAddressing Imbalances: Balancing the distribution of\\nclasses or categories in the dataset to avoid biases and ensure\\nfair representation. This is specially useful for responsible\\nmodel training and evaluation.\\nText Preprocessing: Cleaning and standardizing text data\\nby removing stop words, punctuation, or other elements that\\nmay not contribute significantly to the model’s learning.\\nDealing with Ambiguities: Resolving or excluding am-\\nbiguous or contradictory data that might confuse the model\\nduring training. This can help the model to provide more\\ndefinite and reliable answers.\\n2) Deduplication: De-duplication refers to the process of\\nremoving duplicate instances or repeated occurrences of the\\nsame data in a dataset. Duplicate data points can introduce\\nbiases in the model training process and reduce the diversity, as\\nthe model may learn from the same examples multiple times,\\npotentially leading to overfitting on those particular instances.\\nSome works [125] have shown that de-duplication improves\\nmodels’ ability to generalize to new, unseen data.\\nThe de-duplication process is particularly important when\\ndealing with large datasets, as duplicates can unintentionally\\ninflate the importance of certain patterns or characteristics.\\nThis is especially relevant in NLP tasks, where diverse and\\nrepresentative training data is crucial for building robust lan-\\nguage models.\\nThe specific de-duplication method can vary based on\\nthe nature of the data and the requirements of the particular\\nlanguage model being trained. It may involve comparing entire\\ndata points or specific features to identify and eliminate du-\\nplicates. At the document level, existing works mainly rely on\\nthe overlap ratio of high-level features (e.g. n-grams overlap)\\nbetween documents to detect duplicate samples.\\nC. Tokenizations\\nTokenization referes to the process of converting a se-\\nquence of text into smaller parts, known as tokens. While\\nthe simplest tokenization tool simply chops text into tokens\\nbased on white space, most tokenization tools rely on a word\\ndictionary. However, out-of-vocabulary (OOV) is a problem\\nin this case because the tokenizer only knows words in its\\ndictionary. To increase the coverage of dictionaries, popular\\ntokenizers used for LLMs are based on sub-words, which can\\nbe combined to form a large number of words, including the\\nwords unseen in training data or words in different languages.\\nIn what follows, we describe three popular tokenizers.\\n1) BytePairEncoding: BytePairEncoding is originally a\\ntype of data compression algorithm that uses frequent patterns\\nat byte level to compress the data. By definition, this algorithm\\nmainly tries to keep the frequent words in their original form\\nand break down ones that are not common. This simple\\nparadigm keeps the vocabulary not very large, but also good\\nenough to represent common words at the same time. Also\\nmorphological forms of the frequent words can be represented\\nvery well if suffix or prefix is also commonly presented in the\\ntraining data of the algorithm.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 15, 'page_label': '16'}, page_content='2) WordPieceEncoding: This algorithm is mainly used for\\nvery well-known models such as BERT and Electra. At the\\nbeginning of training, the algorithm takes all the alphabet from\\nthe training data to make sure that nothing will be left as UNK\\nor unknown from the training dataset. This case happens when\\nthe model is given an input that can not be tokenized by the\\ntokenizer. It mostly happens in cases where some characters are\\nnot tokenizable by it. Similar to BytePairEncoding, it tries to\\nmaximize the likelihood of putting all the tokens in vocabulary\\nbased on their frequency.\\n3) SentencePieceEncoding: Although both tokenizers de-\\nscribed before are strong and have many advantages compared\\nto white-space tokenization, they still take assumption of\\nwords being always separated by white-space as granted. This\\nassumption is not always true, in fact in some languages, words\\ncan be corrupted by many noisy elements such as unwanted\\nspaces or even invented words. SentencePieceEncoding tries\\nto address this issue.\\nD. Positional Encoding\\n1) Absolute Positional Embeddings: (APE) [44] has been\\nused in the original Transformer model to preserve the infor-\\nmation of sequence order. Therefore, the positional information\\nof words is added to the input embeddings at the bottom of\\nboth the encoder and decoder stacks. There are various options\\nfor positional encodings, either learned or fixed. In the vanilla\\nTransformer, sine and cosine functions are employed for this\\npurpose. The main drawback of using APE in Transformers\\nis the restriction to a certain number of tokens. Additionally,\\nAPE fails to account for the relative distances between tokens.\\n2) Relative Positional Embeddings: (RPE) [126] involves\\nextending self-attention to take into account the pairwise links\\nbetween input elements. RPE is added to the model at two\\nlevels: first as an additional component to the keys, and\\nsubsequently as a sub-component of the values matrix. This\\napproach looks at the input as a fully-connected graph with\\nlabels and directed edges. In the case of linear sequences, edges\\ncan capture information about the relative position differences\\nbetween input elements. A clipping distance, represented as k\\n2 ≤ k ≤ n − 4, specifies the maximum limit on relative lo-\\ncations. This allows the model to make reasonable predictions\\nfor sequence lengths that are not part of the training data.\\n3) Rotary Position Embeddings: Rotary Positional Em-\\nbedding (RoPE) [127] tackles problems with existing ap-\\nproaches. Learned absolute positional encodings can lack gen-\\neralizability and meaningfulness, particularly when sentences\\nare short. Moreover, current methods like T5’s positional\\nembedding face challenges with constructing a full attention\\nmatrix between positions. RoPE uses a rotation matrix to\\nencode the absolute position of words and simultaneously in-\\ncludes explicit relative position details in self-attention. RoPE\\nbrings useful features like flexibility with sentence lengths, a\\ndecrease in word dependency as relative distances increase,\\nand the ability to improve linear self-attention with relative\\nposition encoding. GPT-NeoX-20B, PaLM, CODEGEN, and\\nLLaMA are among models that take advantage of RoPE in\\ntheir architectures.\\n4) Relative Positional Bias: The concept behind this type\\nof positional embedding is to facilitate extrapolation during\\ninference for sequences longer than those encountered in train-\\ning. In [128] Press et al. proposed Attention with Linear Biases\\n(ALiBi). Instead of simply adding positional embeddings to\\nword embeddings, they introduced a bias to the attention scores\\nof query-key pairs, imposing a penalty proportional to their\\ndistance. In the BLOOM model, ALiBi is leveraged.\\nE. Model Pre-training\\nPre-training is the very first step in large language model\\ntraining pipeline, and it helps LLMs to acquire fundamental\\nlanguage understanding capabilities, which can be useful in a\\nwide range of language related tasks. During pre-training, the\\nLLM is trained on a massive amount of (usually) unlabeled\\ntexts, usually in a self-supervised manner. There are different\\napproaches used for pre-training like next sentence prediction\\n[24], two most common ones include, next token prediction\\n(autoregressive language modeling), and masked language\\nmodeling.\\nIn Autoregressive Language Modeling framework, given\\na sequence of n tokens x1, ..., xn, the model tries to predict\\nnext token xn+1 (and sometimes next sequence of tokens) in\\nan auto-regressive fashion. One popular loss function in this\\ncase is the log-likelihood of predicted tokens as shown in Eq\\n2\\nLALM (x) =\\nNX\\ni=1\\np(xi+n|xi, ..., xi+n−1) (1)\\nGiven the auto-regressive nature of this framework, the\\ndecoder-only models are naturally better suited to learn how\\nto accomplish these task.\\nIn Masked Language Modeling , some words are masked\\nin a sequence and the model is trained to predict the masked\\nwords based on the surrounding context. Sometimes people\\nrefer to this approach as denoising autoencoding, too. If we\\ndenote the masked/corrupted samples in the sequence x, as ˜x,\\nthen the training objective of this approach can be written as:\\nLMLM (x) =\\nNX\\ni=1\\np(˜x|x\\\\˜x) (2)\\nAnd more recently, Mixture of Experts (MoE) [130],\\n[131] have become very popular in LLM space too. MoEs\\nenable models to be pre-trained with much less compute,\\nwhich means one can dramatically scale up the model or\\ndataset size with the same compute budget as a dense model.\\nMoE consists of two main elements: Sparse MoE layers ,\\nwhich are used instead of dense feed-forward network (FFN)\\nlayers, and have a certain number of “experts” (e.g. 8), in\\nwhich each expert is a neural network. In practice, the experts\\nare FFNs, but they can also be more complex networks. A gate\\nnetwork or router , that determines which tokens are sent to\\nwhich expert. It is worth noting that, one can send a token\\nto more than one expert. How to route a token to an expert\\nis one of the big decisions when working with MoEs - the\\nrouter is composed of learned parameters and is pretrained at\\nthe same time as the rest of the network. Fig 29 provides an\\nillustration of a Switch Transformer encoder block, which are\\nused in MoE.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 16, 'page_label': '17'}, page_content='(a) Absolute Positional Embeddings [129]\\n (b) Relative Positional Embeddings\\n(c) Rotary Positional Embedding [127]\\n (d) Relative Positional Bias [128]\\nFig. 28: Various positional encodings are employed in LLMs.\\nFig. 29: : Illustration of a Switch Transformer encoder block.\\nThey replaced the dense feed forward network (FFN) layer\\npresent in the Transformer with a sparse Switch FFN layer\\n(light blue). . Courtesy of [131].\\nF . Fine-tuning and Instruction Tuning\\nEarly language models such as BERT trained using self-\\nsupervision as explained in section III-E were not able to\\nperform specific tasks. In order for the foundation model to be\\nuseful it needed to be fine-tuned to a specific task with labeled\\ndata (so-called supervised fine-tuning or SFT for short). For\\nexample, in the original BERT paper [24], the model was fine-\\ntuned to 11 different tasks. While more recent LLMs no longer\\nrequire fine-tuning to be used, they can still benefit from task\\nor data-specific fine-tuning. For example, OpenAI reports that\\nthe much smaller GPT-3.5 Turbo model can outperform GPT-4\\nwhen fine-tuned with task specific data 2.\\nFine-tuning does not need to be performed to a single\\ntask though, and there are different approaches to multi-task\\nfine-tuning (see e.g. Mahabi et al. [132]). Fine-tuning to one\\nor more tasks is known to improve results and reduce the\\ncomplexity of prompt engineering, and it can serve as an\\nalternative to retrieval augmented generation. Furthermore,\\nthere are other reasons why it might be advisable to fine-tune.\\nFor example, one might want to fine-tune to expose the model\\nto new or proprietary data that it has not been exposed to\\nduring pre-training.\\nAn important reason to fine-tune LLMs is to align the\\nresponses to the expectations humans will have when providing\\ninstructions through prompts. This is the so-called instruction\\ntuning [133]. We dive into the details of how to design\\nand engineer prompts in section IV-B, but in the context\\nof instruction tuning, it is important to understand that the\\ninstruction is a prompt that specifies the task that the LLM\\nshould accomplish. Instruction tuning datasets such as Natural\\nInstructions [134] include not only the task definition but other\\ncomponents such as positive/negative examples or things to\\navoid.\\nThe specific approach and instruction datasets used to\\ninstruction-tune an LLM varies, but, generally speaking, in-\\nstruction tuned models outperform their original foundation\\nmodels they are based on. For example, InstructGPT [59]\\noutperforms GPT-3 on most benchmarks. The same is true\\nfor Alpaca [62] when compared to LLaMA.\\nSelf-Instruct [135], proposed by Wang et al. is also a\\n2https://platform.openai.com/docs/guides/fine-tuning'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 17, 'page_label': '18'}, page_content='popular approach along this line, in which they introduced a\\nframework for improving the instruction-following capabilities\\nof pre-trained language models by bootstrapping their own\\ngenerations. Their pipeline generates instructions, input, and\\noutput samples from a language model, then filters invalid or\\nsimilar ones before using them to fine tune the original model.\\nG. Alignment\\nAI Alignment is the process of steering AI systems towards\\nhuman goals, preferences, and principles. LLMs, pre-trained\\nfor word prediction, often exhibit unintended behaviors. For\\nexample, they might generate contents that are toxic, harmful,\\nmisleading and biased.\\nInstruction tuning, discussed above, gets LLMs a step\\ncloser to being aligned. However, in many cases, it is important\\nto include further steps to improve the alignment of the model\\nand avoid unintended behaviors 3. We review the most popular\\napproaches to alignment in this subsection.\\nRLHF (reinforcement learning from human feedback) and\\nRLAIF (reinforcement learning from AI feedback) are two\\npopular approaches. RLHF uses a reward model to learn\\nalignment from human feedback. This reward model, after\\nbeing tuned, is able to rate different outputs and score them\\naccording to their alignment preferences given by humans. The\\nreward model gives feedback to the original LLM and this\\nfeedback is used to tune the LLM further [137]. Reinforcement\\nlearning from AI feedback on the other hand, directly connects\\na pretrained and well-aligned model to the LLM and helps it\\nto learn from larger and more aligned models [138].\\nIn another recent work (known as DPO) [139], Rafailov\\net al. discussed that RLHF is a complex and often unstable\\nprocedure, and tried to address this with a new approach. They\\nleveraged a mapping between reward functions and optimal\\npolicies to show that this constrained reward maximization\\nproblem can be optimized exactly with a single stage of policy\\ntraining, essentially solving a classification problem on the\\nhuman preference data. The resulting algorithm, which they\\ncalled Direct Preference Optimization (DPO), is stable, per-\\nformant, and computationally lightweight, eliminating the need\\nfor fitting a reward model, sampling from the LM during fine-\\ntuning, or performing significant hyperparameter tuning. They\\nobserved that fine-tuning with DPO exceeds RLHF’s ability to\\ncontrol sentiment of generations and improves response quality\\nin summarization. Fig 30 shows the high-level comparison\\nbetween DPO vs RLHF.\\nEven more recently Ethayarajh et al. proposed a new align-\\nment approach called the Kahneman-Tversky Optimization\\n(KTO) [136]. Unlike existing state-of-the-art approaches, KTO\\ndoes not require paired preference data ( x, yw, yl), and it\\nonly needs (x,y) and knowledge of whether y is desirable or\\nundesirable. KTO-aligned models are shown to be good or\\nbetter than DPO-aligned models at scales from 1B to 30B,\\ndespite not using paired preferences. KTO is also far easier to\\nuse in the real world than preference optimization methods, as\\nthe kind of data it needs is far more abundant. As an example,\\n3According to very recent research by Ethayarajh et al. [136], further\\nalignment besides SFT mainly improves models of at least 7B parameters.\\nFor smaller models, SFT is sufficient.\\nFig. 30: DPO optimizes for human preferences while avoiding\\nreinforcement learning. Existing methods for fine-tuning lan-\\nguage models with human feedback first fit a reward model\\nto a dataset of prompts and human preferences over pairs of\\nresponses, and then use RL to find a policy that maximizes\\nthe learned reward. In contrast, DPO directly optimizes for\\nthe policy best satisfying the preferences with a simple classi-\\nfication objective, without an explicit reward function or RL.\\nCourtesy of [139].\\nevery retail company has a lot of customer interaction data and\\nwhether that interaction was successful (e.g., purchase made)\\nor unsuccessful (e.g., no purchase made). However, they have\\nlittle to no counterfactual data (i.e., what would have made\\nan unsuccessful customer interaction yl into a successful one\\nyw). Fig 31 shows a high-level comparison between KTO and\\nother alignment approaches discussed above.\\nFig. 31: LLM alignment involves supervised finetuning fol-\\nlowed by optimizing a human-centered loss (HALO). How-\\never, the paired preferences that existing approaches need are\\nhard-to-obtain. In contrast, KTO uses a far more abundant\\nkind of data, making it much easier to use in the real world.\\nCourtesy of [136].\\nH. Decoding Strategies\\nDecoding refers to the process of text generation using pre-\\ntrained LLMs. Given an input prompt, the tokenizer translates\\neach token in the input text into a corresponding token ID.\\nThen, the language model uses these token IDs as input and\\npredicts the next most likely token (or a sequence of tokens).\\nFinally, the model generates logits, which are converted to\\nprobabilities using a softmax function. Different decoding\\nstrategies have been proposed. Some of the most popular ones\\nare greedy search, beam search, as well as different sample\\ntechniques such as top-K, top-P (Nucleus sampling).\\n1) Greedy Search: Greedy search takes the most probable\\ntoken at each step as the next token in the sequence, discarding\\nall other potential options. As you can imagine, this is a simple\\napproach and can loose a lot of temporal consistency and\\ncoherency. It only considers the most probable token at each'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 18, 'page_label': '19'}, page_content='step, without considering the overall effect on the sequence.\\nThis property makes it fast, but it also means that it can miss\\nout on better sequences that might have appeared with slightly\\nless probable next tokens.\\n2) Beam Search: Unlike greedy search that only considers\\nthe next most probable token, beam search takes into account\\nthe N most likely tokens, where N denotes the number of\\nbeams. This procedure is repeated until a predefined maxi-\\nmum sequence length is reached or an end-of-sequence token\\nappears. At this point, the sequence of tokens (AKA “beam”)\\nwith the highest overall score is chosen as the output. For\\nexample for beam size of 2 and maximum length of 5,\\nthe beam search needs to keep track of 25 = 32 possible\\nsequences. So it is more computationally intensive than greedy\\nsearch.\\n3) Top-k Sampling: Top-k sampling is a technique that\\nuses the probability distribution generated by the language\\nmodel to select a token randomly from the k most likely\\noptions.\\nSuppose we have 6 tokens (A, B, C, D, E, F) and k=2,\\nand P(A)= 30%, and P(B)= 20%, P(C)= P(D)= P(E)= P(F)=\\n12.5%. In top-k sampling, tokens C, D, E, F are disregarded,\\nand the model outputs A 60% of the time, and B, 40% of\\nthe time. This approach ensures that we prioritize the most\\nprobable tokens while introducing an element of randomness\\nin the selection process.\\nThe randomness is usually introduced via the concept of\\ntemperature. The temperature T is a parameter that ranges from\\n0 to 1, which affects the probabilities generated by the softmax\\nfunction, making the most likely tokens more influential. In\\npractice, it simply consists of dividing the input logits by\\ntemperature value:\\nsoftmax(xi) = exi/T\\nP\\nj exj /T (3)\\nA low temperature setting significantly alters the proba-\\nbility distribution (and is commonly used in text generation\\nto control the level of “creativity” in the generated output),\\nwhile a large temperature prioritizes the tokens with higher\\nprobabilities. Top-k is a creative way of sampling, and can be\\nused along with beam search. The sequence chosen by top-\\nk sampling may not be the sequence with highest probability\\nin beam search. But it’s important to remember that highest\\nscores do not always lead to more realistic or meaningful\\nsequences.\\n4) Top-p Sampling: Top-p sampling, also known as Nu-\\ncleus sampling, takes a slightly different approach from top-k\\nsampling. Instead of selecting the top k most probable tokens,\\nnucleus sampling chooses a cutoff value p such that the sum of\\nthe probabilities of the selected tokens exceeds p. This forms\\na “nucleus” of tokens from which to randomly choose the next\\ntoken. In other words, in top-p sampling the language model\\nexamines the most probable tokens in descending order and\\nkeeps adding them to the list until the sum of probabilities\\nsurpasses the threshold p. As you can imagine, this could be\\nbetter specially for scenarios in which top-k tokens do not have\\na large probability mass. Unlike top-k sampling, the number\\nof tokens included in the nucleus sampling is not fixed. This\\nvariability often results in a more diverse and creative output,\\nmaking nucleus sampling popular for text generation related\\ntasks.\\nI. Cost-Effective Training/Inference/Adaptation/Compression\\nIn this part, we review some of the popular approaches\\nused for more cost-friendly (and compute-friendly) training\\nand usage of LLMs.\\n1) Optimized Training: There are many frameworks de-\\nveloped for optimized training of LLMs, here we introduce\\nsome of the prominent ones.\\nZeRO: In [140], Rajbhandari et al. developed a novel\\nsolution, Zero Redundancy Optimizer (ZeRO), to optimize\\nmemory, vastly improving training speed of LLMs while\\nincreasing the model size that can be efficiently trained. ZeRO\\neliminates memory redundancies in data- and model-parallel\\ntraining while retaining low communication volume and high\\ncomputational granularity, allowing one to scale the model\\nsize proportional to the number of devices with sustained high\\nefficiency.\\nRWKV: In [141], Peng et al. proposed a novel model\\narchitecture, Receptance Weighted Key Value (RWKV), that\\ncombines the efficient parallelizable training of Transformers\\nwith the efficient inference of RNNs. Their approach leverages\\na linear attention mechanism and allows them to formulate the\\nmodel as either a Transformer or an RNN, which parallelizes\\ncomputations during training and maintains constant compu-\\ntational and memory complexity during inference, leading to\\nthe first non-transformer architecture to be scaled to tens of\\nbillions of parameters. RWKV architecture is shown in Fig\\n32. The Time Complexity comparison of RWKV with different\\nFig. 32: RWKV architecture. Courtesy of [141].\\nTransformers are provided in Fig 33.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 19, 'page_label': '20'}, page_content='Fig. 33: Time Complexity comparison of RWKV with different\\nTransformers. Here T denotes the sequence length, d the\\nfeature dimension, and c is MEGA’s chunk size of quadratic\\nattention. Courtesy of [141].\\n2) Low-Rank Adaption (LoRA): Low-Rank Adaptation is\\na popular and lightweight training technique that significantly\\nreduces the number of trainable parameters, and is based\\non a crucial insight that the difference between the fine-\\ntuned weights for a specialized task and the initial pre-trained\\nweights often exhibits “low intrinsic rank” - meaning that\\nit can be approximated well by a low rank matrix [142].\\nTraining with LoRA is much faster, memory-efficient, and\\nproduces smaller model weights (a few hundred MBs), that are\\neasier to store and share. One property of low-rank matrices\\nis that they can be represented as the product of two smaller\\nmatrices. This realization leads to the hypothesis that this delta\\nbetween fine-tuned weights and initial pre-trained weights can\\nbe represented as the matrix product of two much smaller\\nmatrices. By focusing on updating these two smaller matrices\\nrather than the entire original weight matrix, computational\\nefficiency can be substantially improved.\\nSpecifically, for a pre-trained weight matrix W0 ∈ Rd×k,\\nLoRA constrains its update by representing the latter with\\na low-rank decomposition W0 + ∆W = W0 + BA, where\\nB ∈ Rd×r , A ∈ Rr×k, and the rank r ≪ min(d, k). During\\ntraining, W0 is frozen and does not receive gradient updates,\\nwhile A and B contain trainable parameters. It is worth\\nmentioning that both W0 and ∆W = BA are multiplied with\\nthe same input, and their respective output vectors are summed\\ncoordinate-wise. For h = W0x, their modified forward pass\\nyields: h = W0x + ∆W x= W0x + BAx. Usually a random\\nGaussian initialization is used for A, and zero initialization\\nfor B, so ∆W = BA is zero at the beginning of training.\\nThey then scale ∆W xby αr, where α is a constant in r. This\\nreparametrization is illustrated in Figure 34\\nIt is worth mentioning that LoRA can be applied to a subset\\nof weight matrices in a neural network to reduce the number\\nof trainable parameters. In the Transformer architecture, there\\nare four weight matrices in the self-attention module ( Wq ,\\nWk, Wv , Wo), and two in the MLP module. Most of the\\ntime, LoRA is focused on adapting the attention weights only\\nfor downstream tasks, and freezes the MLP modules, so they\\nare not trained in downstream tasks both for simplicity and\\nparameter-efficiency.\\n3) Knowledge Distillation: Knowledge distillation is the\\nprocess of learning from a larger model [143]. Earlier days of\\nbest-performing models release have proven that this approach\\nis very useful even if it is used in an API distillation approach.\\nFig. 34: An illustration of LoRA reparametrizan. Only A and\\nB trained during this process. Courtesy of [142].\\nIt is also referred to as an approach to distill the knowledge of\\nnot a single model but in fact multiple models into a smaller\\none. Creating smaller models by this approach yields smaller\\nmodel sizes that can be used even on edge devices. Knowledge\\ndistillation as shown in Fig 35, illustrates a general setup of\\nthis training scheme.\\nFig. 35: A generic knowledge distillation framework with\\nstudent and teacher (Courtesy of [144]).\\nKnowledge can be transferred by different forms of learn-\\ning: response distillation, feature distillation, and API distilla-\\ntion. Response distillation is concerned only with the outputs\\nof the teacher model and tries to teach the student model\\nhow to exactly or at least similarly perform (in the sense of\\nprediction) as the teacher. Feature distillation not only uses\\nthe last layer but also intermediate layers as well to create a\\nbetter inner representation for the student model. This helps the\\nsmaller model to have a similar representation as the teacher\\nmodel.\\nAPI distillation is the process of using an API (typically\\nfrom an LLM provider such as OpenAI) to train smaller\\nmodels. In the case of LLMs, it is used to train the model\\nfrom the direct output of the larger model which makes it very\\nsimilar to response distillation. Many concerns are raised by\\nthis type of distillation because in cases where the model itself\\nis not openly available, a (usually) paid API is exposed for end\\nusers. On the other hand, while users pay for each call, how to\\nuse the predictions is limited, for example, OpenAI prohibits\\nusage of its API to create LLMs that later will be used to\\ncompete with it. The main value in such case is training data.\\n4) Quantization: deep learning in its core, is a set of\\nmathematical functions applied to matrices, with a specific'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 20, 'page_label': '21'}, page_content='precision for model weights. Reducing the precision of the\\nweights can be used to reduce the size of the model and also\\nmake it faster. As an example, Float-32 operations compared\\nto Int-8 operations are slower. This process, which is called\\nquantization, can be applied in different phases. Main ap-\\nproaches for model quantization can be categorized as: post\\ntraining quantization and quantization-aware training. Post-\\ntraining quantization is concerned with quantized trained mod-\\nels in two well-known methods: dynamic and static. Dynamic\\npost-training quantization computes the range of quantization\\non the runtime and is slower compared to static. Quantization-\\naware training adds quantization criteria into training, and\\na quantized model is trained and optimized during training\\nprocess. This approach ensures that the end model will have\\ngood performance and also does not need to be quantized after\\ntraining.\\nIV. H OW LLM S ARE USED AND AUGMENTED\\nOnce the LLMs are trained, we can use them to generate\\ndesired outputs for a variety of tasks. LLMs can be used\\ndirectly through basic prompting. However, in order to exploit\\ntheir full potential or to address some of the shortcomings,\\nwe need to augment the models through some external means.\\nIn this section we first provide a brief overview of the main\\nshortcoming of LLMs, with a deeper look at the issue of\\nhallucination. We then describe how prompting and some aug-\\nmentation approaches can not only address those limitations\\nbut also be used to augment the capabilities of LLMs going\\nas far as turning an LLM into a full-blown AI agent with the\\nability to interface with the external world.\\nA. LLM limitations\\nIt is important to remember that LLMs are trained to predict\\na token. While fine-tuning and alignment improves their per-\\nformance and adds different dimensions to their abilities, there\\nare still some important limitations that come up, particularly\\nif they are used naively. Some of them include the following:\\n• They don’t have state/memory. LLMs on their own\\ncannot remember even what was sent to them in the\\nprevious prompt. That is an important limitation for\\nmany of the use cases that require some form of state.\\n• They are stochastic/probabilistic. If you send the same\\nprompt to an LLM several times, you are likely to get\\ndifferent responses. While there are parameters, and\\nin particular the temperature, to limit the variability\\nin the response, this is an inherent property of their\\ntraining that can create issues.\\n• They have stale information and, on their own, don’t\\nhave access to external data. An LLM on its own does\\nnot even know about the current time or day and does\\nnot have access to any information that was not present\\nin its training set.\\n• They are generally very large. This means that many\\ncostly GPU machines are needed for training and\\nserving. In some cases, largest models have poor\\nSLAs, particularly in terms of latency.\\n• They hallucinate. LLMs do not have a notion of\\n”truth” and they have usually been trained on a mix\\nof good and bad content. They can produce very\\nplausible but untruthful answers.\\nWhile the previous limitations can all become important\\nfor some applications, it is worth for us to dive a bit into the\\nlast one, hallucinations, since it has gathered a lot of interest\\nover the past few months and it has also sparked many of the\\nprompt approaches and LLM augmentation methods we will\\nlater describe.\\nHallucination: In the realm of Large Language Models\\n(LLMs), the phenomenon of ”hallucinations” has garnered\\nsignificant attention. Defined in the literature, notably in the\\n”Survey of Hallucination in Natural Language Generation”\\npaper [145], hallucination in an LLM is characterized as\\n”the generation of content that is nonsensical or unfaithful\\nto the provided source.” This terminology, although rooted in\\npsychological parlance, has been appropriated within the field\\nof artificial intelligence.\\nHallucinations in LLMs can be broadly categorized into\\ntwo types:\\n1) Intrinsic Hallucinations: These directly conflict with\\nthe source material, introducing factual inaccuracies\\nor logical inconsistencies.\\n2) Extrinsic Hallucinations : These, while not contra-\\ndicting, are unverifiable against the source, encom-\\npassing speculative or unconfirmable elements.\\nThe definition of ’source’ in LLM contexts varies with the\\ntask. In dialogue-based tasks, it refers to ’world knowledge’,\\nwhereas in text summarization, it pertains to the input text\\nitself. This distinction plays a crucial role in evaluating and\\ninterpreting hallucinations. The impact of hallucinations is also\\nhighly context-dependent. For instance, in creative endeavors\\nlike poem writing, hallucinations might be deemed acceptable\\nor even beneficial.\\nLLMs, trained on diverse datasets including the internet,\\nbooks, and Wikipedia, generate text based on probabilistic\\nmodels without an inherent understanding of truth or falsity.\\nRecent advancements like instruct tuning and Reinforcement\\nLearning from Human Feedback (RLHF) have attempted to\\nsteer LLMs towards more factual outputs, but the fundamental\\nprobabilistic nature and its inherent limitations remain. A\\nrecent study, “Sources of Hallucination by Large Language\\nModels on Inference Tasks” [146], highlights two key aspects\\ncontributing to hallucinations in LLMs: the veracity prior and\\nthe relative frequency heuristic, underscoring the complexities\\ninherent in LLM training and output generation.\\nEffective automated measurement of hallucinations in\\nLLMs requires a combination of statistical and model-based\\nmetrics.\\nStatistical Metrics:\\n• Metrics like ROUGE [147] and BLEU [148] are com-\\nmon for assessing text similarity, focusing on intrinsic\\nhallucinations.\\n• Advanced metrics such as PARENT [149], PARENT-\\nT [150], and Knowledge F1 [151] are utilized when\\nstructured knowledge sources are available. These'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 21, 'page_label': '22'}, page_content='B) Augmenting LLMs through\\nexternal knowledge - RAG\\nHow LLMs Are Used and Augmented\\nC) Using External Tools\\nD) LLM Agents\\nFunctionality of an LLM-based agent\\nTool Access and Utilization\\nDecision Making\\nPrompt engineering techniques for agents\\nReasoning without Observation\\nReason and Act\\nDialog-Enabled Resolving Agents\\na) RAG-aware prompting techniques\\na) Tool-aware prompting techniques\\nA) LLM limitations\\nHallucination\\nHallucination Quantification\\nAutomated metrics\\nHuman judgment\\nStatistical Metrics\\nModel-Based Metrics\\nScoring\\nComparative Analysis\\nIE-Based Metrics\\nQA-Based Metrics\\nNLI-Based Metrics\\nB) Using LLMs\\n Prompt Design and Engineering\\n1) Chain of Thought\\nZero-Shot CoT\\nManual CoT\\n5) Expert Prompting\\n6) Chains\\n2) Tree of Thought 7) Rails\\nTopical Rails\\nFact-Checking Rails\\nJailbreaking Rails\\n8) Automatic Prompt Engineering\\nPrompt Generation\\nPrompt Scoring\\nRefinement and Iteration\\n3) Self-Consistency\\n4) Reflection\\nComponents of a RAG\\nRetrieval \\nGeneration \\nAugmentation\\nRAG Tools\\nLangChain \\nLlamaIndex\\nHayStack\\nMeltano\\nCohere Coral\\nFlowise AI\\nFig. 36: How LLMs Are Used and Augmented.\\nmetrics, while effective, have limitations in capturing\\nsyntactic and semantic nuances.\\nModel-Based Metrics:\\n• IE-Based Metrics : Utilize Information Extraction\\nmodels to simplify knowledge into relational tuples,\\nthen compare these with the source.\\n• QA-Based Metrics: Assess the overlap between gen-\\nerated content and the source through a question-\\nanswering framework (see [152]).\\n• NLI-Based Metrics: Use Natural Language Inference\\ndatasets to evaluate the truthfulness of a generated\\nhypothesis based on a given premise (see [153]).\\n• Faithfulness Classification Metrics : Offer a refined\\nassessment by creating task-specific datasets for a\\nnuanced evaluation (see [154]).\\nDespite advances in automated metrics, human judgment\\nremains a vital piece. It typically involves two methodologies:\\n1) Scoring: Human evaluators rate the level of halluci-\\nnation within a predefined scale.\\n2) Comparative Analysis : Evaluators compare gener-\\nated content against baseline or ground-truth refer-\\nences, adding an essential layer of subjective assess-\\nment.\\nFactScore [155] is a recent example of a metric that can be\\nused both for human and model-based evaluation. The metric\\nbreaks an LLM generation into “atomic facts”. The final score\\nis computed as the sum of the accuracy of each atomic fact,\\ngiving each of them equal weight. Accuracy is a binary number\\nthat simply states whether the atomic fact is supported by the\\nsource. The authors implement different automation strategies\\nthat use LLMs to estimate this metric.\\nFinally, mitigating hallucinations in LLMs is a multifaceted\\nchallenge, requiring tailored strategies to suit various applica-\\ntions. Those include:\\n• Product Design and User Interaction Strategies such\\nas use case design, structuring the input/output, or\\nproviding mechanisms for user feedback.\\n• Data Management and Continuous Improvement.\\nMaintaining and analyzing a tracking set of hallucina-\\ntions is essential for ongoing model improvement.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 22, 'page_label': '23'}, page_content='• Prompt Engineering and Metaprompt Design. Many\\nof the advanced prompt techniques described in IV-B\\nsuch as Retrieval Augmented Generation directly ad-\\ndress hallucination risks.\\n• Model Selection and Configuration for Hallucination\\nMitigation. For exemple, larger models with lower\\ntemperature settings usually perform better. Also,\\ntechniques such as RLHF or domain-sepcific fine-\\ntuning can mitigate hallucination risks.\\nB. Using LLMs: Prompt Design and Engineering\\nA prompt in generative AI models is the textual input\\nprovided by users to guide the model’s output. This could\\nrange from simple questions to detailed descriptions or specific\\ntasks. Prompts generally consist of instructions, questions,\\ninput data, and examples. In practice, to elicit a desired\\nresponse from an AI model, a prompt must contain either\\ninstructions or questions, with other elements being optional.\\nAdvanced prompts involve more complex structures, such as\\n”chain of thought” prompting, where the model is guided to\\nfollow a logical reasoning process to arrive at an answer.\\nPrompt engineering is a rapidly evolving discipline that\\nshapes the interactions and outputs of LLMs and other gen-\\nerative AI models. The essence of prompt engineering lies in\\ncrafting the optimal prompt to achieve a specific goal with\\na generative model. This process is not only about instructing\\nthe model but also involves some understanding of the model’s\\ncapabilities and limitations, and the context within which it\\noperates.\\nPrompt engineering transcends the mere construction of\\nprompts; it requires a blend of domain knowledge, understand-\\ning of the AI model, and a methodical approach to tailor\\nprompts for different contexts. This might involve creating\\ntemplates that can be programmatically modified based on a\\ngiven dataset or context. For example, generating personalized\\nresponses based on user data might use a template that is\\ndynamically filled with relevant user information.\\nFurthermore, prompt engineering is an iterative and ex-\\nploratory process, akin to traditional machine learning prac-\\ntices such as model evaluation or hyperparameter tuning. The\\nrapid growth of this field suggests its potential to revolutionize\\ncertain aspects of machine learning, moving beyond traditional\\nmethods like feature or architecture engineering. On the other\\nhand, traditional engineering practices such as version con-\\ntrol and regression testing need to be adapted to this new\\nparadigm just like they were adapted to other machine learning\\napproaches [156].\\nIn the following paragraphs we detail some of the most\\ninteresting and popular prompt engineering approaches.\\n1) Chain of Thought (CoT): The Chain of Thought (CoT)\\ntechnique, initially described in the paper “Chain-of-Thought\\nPrompting Elicits Reasoning in Large Language Models”[34]\\nby Google researchers, represents a pivotal advancement in\\nprompt engineering for Large Language Models (LLMs).\\nThis approach hinges on the understanding that LLMs, while\\nproficient in token prediction, are not inherently designed for\\nexplicit reasoning. CoT addresses this by guiding the model\\nthrough essential reasoning steps.\\nCoT is based on making the implicit reasoning process of\\nLLMs explicit. By outlining the steps required for reasoning,\\nthe model is directed closer to a logical and reasoned output,\\nespecially in scenarios demanding more than simple informa-\\ntion retrieval or pattern recognition.\\nCoT prompting manifests in two primary forms:\\n1) Zero-Shot CoT: This form involves instructing the\\nLLM to “think step by step”, prompting it to de-\\nconstruct the problem and articulate each stage of\\nreasoning.\\n2) Manual CoT: A more complex variant, it requires\\nproviding step-by-step reasoning examples as tem-\\nplates for the model. While yielding more effective\\nresults, it poses challenges in scalability and mainte-\\nnance.\\nManual CoT is more effective than zero-shot. However,\\nthe effectiveness of this example-based CoT depends on the\\nchoice of diverse examples, and constructing prompts with\\nsuch examples of step by step reasoning by hand is hard and\\nerror prone. That is where automatic CoT [157] comes into\\nplay.\\n2) Tree of Thought (ToT): The Tree of Thought (ToT)\\n[158] prompting technique is inspired by the concept of\\nconsidering various alternative solutions or thought processes\\nbefore converging on the most plausible one. ToT is based\\non the idea of branching out into multiple ”thought trees”\\nwhere each branch represents a different line of reasoning.\\nThis method allows the LLM to explore various possibilities\\nand hypotheses, much like human cognitive processes where\\nmultiple scenarios are considered before determining the most\\nlikely one.\\nA critical aspect of ToT is the evaluation of these reasoning\\npaths. As the LLM generates different branches of thought,\\neach is assessed for its validity and relevance to the query.\\nThis process involves real-time analysis and comparison of\\nthe branches, leading to a selection of the most coherent and\\nlogical outcome.\\nToT is particularly useful in complex problem-solving\\nscenarios where a single line of reasoning might not suffice.\\nIt allows LLMs to mimic a more human-like problem-solving\\napproach, considering a range of possibilities before arriving\\nat a conclusion. This technique enhances the model’s ability\\nto handle ambiguity, complexity, and nuanced tasks, making it\\na valuable tool in advanced AI applications.\\n3) Self-Consistency: Self-Consistency [159] utilizes an\\nensemble-based method, where the LLM is prompted to gen-\\nerate multiple responses to the same query. The consistency\\namong these responses serves as an indicator of their accuracy\\nand reliability.\\nThe Self-Consistency approach is grounded in the principle\\nthat if an LLM generates multiple, similar responses to the\\nsame prompt, it is more likely that the response is accurate.\\nThis method involves asking the LLM to tackle a query mul-\\ntiple times, each time analyzing the response for consistency.\\nThis technique is especially useful in scenarios where factual\\naccuracy and precision are paramount.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 23, 'page_label': '24'}, page_content='The consistency of responses can be measured using vari-\\nous methods. One common approach is to analyze the overlap\\nin the content of the responses. Other methods may include\\ncomparing the semantic similarity of responses or employing\\nmore sophisticated techniques like BERT-scores or n-gram\\noverlaps. These measures help in quantifying the level of\\nagreement among the responses generated by the LLM.\\nSelf-Consistency has significant applications in fields\\nwhere the veracity of information is critical. It is particularly\\nrelevant in scenarios like fact-checking, where ensuring the\\naccuracy of information provided by AI models is essential.\\nBy employing this technique, prompt engineers can enhance\\nthe trustworthiness of LLMs, making them more reliable for\\ntasks that require high levels of factual accuracy.\\n4) Reflection: Reflection [160] involves prompting LLMs\\nto assess and potentially revise their own outputs based on\\nreasoning about the correctness and coherence of their re-\\nsponses. The concept of Reflection centers on the ability of\\nLLMs to engage in a form of self-evaluation. After generating\\nan initial response, the model is prompted to reflect on its\\nown output, considering factors like factual accuracy, logical\\nconsistency, and relevance. This introspective process can lead\\nto the generation of revised or improved responses.\\nA key aspect of Reflection is the LLM’s capacity for\\nself-editing. By evaluating its initial response, the model can\\nidentify potential errors or areas of improvement. This iterative\\nprocess of generation, reflection, and revision enables the LLM\\nto refine its output, enhancing the overall quality and reliability\\nof its responses.\\n5) Expert Prompting: Expert Prompting [161] enhances the\\ncapabilities of Large Language Models (LLMs) by simulating\\nthe responses of experts in various fields. This method involves\\nprompting the LLMs to assume the role of an expert and re-\\nspond accordingly, providing high-quality, informed answers.\\nA key strategy within Expert Prompting is the multi-expert\\napproach. The LLM is prompted to consider responses from\\nmultiple expert perspectives, which are then synthesized to\\nform a comprehensive and well-rounded answer. This tech-\\nnique not only enhances the depth of the response but also\\nincorporates a range of viewpoints, reflecting a more holistic\\nunderstanding of the subject matter.\\n6) Chains: Chains refer to the method of linking multiple\\ncomponents in a sequence to handle complex tasks with Large\\nLanguage Models (LLMs). This approach involves creating a\\nseries of interconnected steps or processes, each contributing\\nto the final outcome. The concept of Chains is based on\\nthe idea of constructing a workflow where different stages\\nor components are sequentially arranged. Each component in\\na Chain performs a specific function, and the output of one\\nserves as the input for the next. This end-to-end arrangement\\nallows for more complex and nuanced processing, as each\\nstage can be tailored to handle a specific aspect of the task.\\nChains can vary in complexity and structure, depending on\\nthe requirements. In “PromptChainer: Chaining Large Lan-\\nguage Model Prompts through Visual Programming” [162],\\nthe authors not only describe the main challenges in designing\\nchains, but also describe a visual tool to support those tasks.\\n7) Rails: Rails in advanced prompt engineering refer to\\na method of guiding and controlling the output of Large\\nLanguage Models (LLMs) through predefined rules or tem-\\nplates. This approach is designed to ensure that the model’s\\nresponses adhere to certain standards or criteria, enhancing the\\nrelevance, safety, and accuracy of the output. The concept of\\nRails involves setting up a framework or a set of guidelines\\nthat the LLM must follow while generating responses. These\\nguidelines are typically defined using a modeling language or\\ntemplates known as Canonical Forms, which standardize the\\nway natural language sentences are structured and delivered.\\nRails can be designed for various purposes, depending on\\nthe specific needs of the application:\\n• Topical Rails: Ensure that the LLM sticks to a\\nparticular topic or domain.\\n• Fact-Checking Rails: Aimed at minimizing the gen-\\neration of false or misleading information.\\n• Jailbreaking Rails: Prevent the LLM from generating\\nresponses that attempt to bypass its own operational\\nconstraints or guidelines.\\n8) Automatic Prompt Engineering (APE): Automatic\\nPrompt Engineering (APE) [163] focuses on automating the\\nprocess of prompt creation for Large Language Models\\n(LLMs). APE seeks to streamline and optimize the prompt\\ndesign process, leveraging the capabilities of LLMs themselves\\nto generate and evaluate prompts. APE involves using LLMs\\nin a self-referential manner where the model is employed\\nto generate, score, and refine prompts. This recursive use of\\nLLMs enables the creation of high-quality prompts that are\\nmore likely to elicit the desired response or outcome.\\nThe methodology of APE can be broken down into several\\nkey steps:\\n• Prompt Generation: The LLM generates a range of\\npotential prompts based on a given task or objective.\\n• Prompt Scoring: Each generated prompt is then\\nevaluated for its effectiveness, often using criteria\\nlike clarity, specificity, and likelihood of eliciting the\\ndesired response.\\n• Refinement and Iteration: Based on these evalua-\\ntions, prompts can be refined and iterated upon, further\\nenhancing their quality and effectiveness.\\nC. Augmenting LLMs through external knowledge - RAG\\nOne of the main limitations of pre-trained LLMs is their\\nlack of up-to-date knowledge or access to private or use-\\ncase-specific information. This is where retrieval augmented\\ngeneration (RAG) comes into the picture [164]. RAG, illus-\\ntrated in figure 37, involves extracting a query from the input\\nprompt and using that query to retrieve relevant information\\nfrom an external knowledge source (e.g. a search engine or a\\nknowledge graph, see figure 38 ). The relevant information is\\nthen added to the original prompt and fed to the LLM in order\\nfor the model to generate the final response. A RAG system\\nincludes three important components: Retrieval, Generation,\\nAugmentation [165].'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 24, 'page_label': '25'}, page_content='Fig. 37: An example of synthesizing RAG with LLMs for question answering application [166].\\nFig. 38: This is one example of synthesizing the KG as a\\nretriever with LLMs [167].\\na) RAG-aware prompting techniques: Because of the\\nimportance of RAG to build advanced LLM systems, several\\nRAG-aware prompting techniques have been developed re-\\ncently. One such technique is Forward-looking Active Retrieval\\nAugmented Generation (FLARE)\\nForward-looking Active Retrieval Augmented Generation\\n(FLARE) [168] enhances the capabilities of Large Language\\nModels (LLMs) by iteratively combining prediction and in-\\nformation retrieval. FLARE represents an evolution in the\\nuse of retrieval-augmented generation, aimed at improving the\\naccuracy and relevance of LLM responses.\\nFLARE involves an iterative process where the LLM\\nactively predicts upcoming content and uses these predictions\\nas queries to retrieve relevant information. This method con-\\ntrasts with traditional retrieval-augmented models that typically\\nretrieve information once and then proceed with generation. In\\nFLARE, this process is dynamic and ongoing throughout the\\ngeneration phase. In FLARE, each sentence or segment gener-\\nated by the LLM is evaluated for confidence. If the confidence\\nlevel is below a certain threshold, the model uses the generated\\ncontent as a query to retrieve relevant information, which is\\nthen used to regenerate or refine the sentence. This iterative\\nprocess ensures that each part of the response is informed by\\nthe most relevant and current information available.\\nFor more details on RAG framework and its relevant works,\\nwe refer the readers to this survey of retrieval augmented\\ngenerations [165].\\nD. Using External Tools\\nRetrieving information from an external knowledge source\\nas described above is only one of the potential ways to augment\\nan LLM. More generally, an LLM can access any number\\nof external tools (e.g. an API to a service) to augment its\\nfunctionality. In that regards, RAG can be seen as a specific\\ninstance of the broader category of the so called ”tools”.\\nTools in this context are external functions or services that\\nLLMs can utilize. These tools extend the range of tasks an\\nLLM can perform, from basic information retrieval to complex\\ninteractions with external databases or APIs.\\nIn the paper ”Toolformer: Language Models Can Teach\\nThemselves to Use Tools” [169], the authors go beyond simple\\ntool usage by training an LLM to decide what tool to use\\nwhen, and even what parameters the API needs. Tools include\\ntwo different search engines, or a calculator. In the following\\nexamples, the LLM decides to call an external Q&A tool,\\na calculator, and a Wikipedia Search Engine More recently,\\nresearchers at Berkeley have trained a new LLM called Gorilla\\n[67] that beats GPT-4 at the use of APIs, a specific but quite\\ngeneral tool.\\na) Tool-aware prompting techniques: Similarly to what\\nwas described with RAG, several tool-aware prompting ap-\\nproaches have been developed to make usage of tools more\\nscalable. A popular technique is the so called Automatic Multi-\\nstep Reasoning and Tool-use (ART).\\nAutomatic Multi-step Reasoning and Tool-use (ART) [170]\\nis a prompt engineering technique that combines automated\\nchain of thought prompting with the use of external tools.\\nART represents a convergence of multiple prompt engineering\\nstrategies, enhancing the ability of Large Language Models'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 25, 'page_label': '26'}, page_content='(LLMs) to handle complex tasks that require both reasoning\\nand interaction with external data sources or tools.\\nART involves a systematic approach where, given a task\\nand input, the system first identifies similar tasks from a task\\nlibrary. These tasks are then used as examples in the prompt,\\nguiding the LLM on how to approach and execute the current\\ntask. This method is particularly effective when tasks require a\\ncombination of internal reasoning and external data processing\\nor retrieval.\\nE. LLM Agents\\nThe idea of AI agents has been well-explored in the history\\nof AI. An agent is typically an autonomous entity that can\\nperceive the environment using its sensors, make a judgment\\nbased on the state it currently is, and accordingly act based on\\nthe actions that are available to it.\\nIn the context of LLMs, an agent refers to a system based\\non a specialized instantiation of an (augmented) LLM that\\nis capable of performing specific tasks autonomously. These\\nagents are designed to interact with users and environment to\\nmake decisions based on the input and the intended goal of\\nthe interaction. Agents are based on LLMs equipped with the\\nability to access and use tools, and to make decisions based on\\nthe given input. They are designed to handle tasks that require\\na degree of autonomy and decision-making, typically beyond\\nsimple response generation.\\nThe functionalities of a generic LLM-based agent include:\\n• Tool Access and Utilization: Agents have the capabil-\\nity to access external tools and services, and to utilize\\nthese resources effectively to accomplish tasks.\\n• Decision Making: They can make decisions based on\\nthe input, context, and the tools available to them,\\noften employing complex reasoning processes.\\nAs an example, an LLM that has access to a function (or\\nan API) such as weather API, can answer any question related\\nto the weather of the specific place. In other words, it can use\\nAPIs to solve problems. Furthermore, if that LLM has access\\nto an API that allows to make purchases, a purchasing agent\\ncan be built to not only have capabilities to read information\\nfrom the external world, but also act on it [171].\\nFig. 40 shows another example of LLM-based agents for\\nconversational information seeking [36], where an LLM is\\naugmented with a set of plug-and-play modules, including\\na working memory that tracks the dialog state, a policy that\\nmakes an execution plan for the task and selects next system\\naction, an action executor that performs an action selected by\\nthe policy (consolidating evidence from external knowledge,\\nor prompting the LLM to generate responses), and a utility\\nthat accesses the alignment of the LLM’s responses with user\\nexpectations or specific business requirements, and generate\\nfeedback to improve agent performance.\\nFor more details on LLM-based AI agents see recent survey\\n[172], [173], [174].\\na) Prompt engineering techniques for agents: Like\\nRAG and Tools, prompt engineering techniques that specif-\\nically address the needs of LLM-based agents have been\\ndeveloped. Three such examples are Reasoning without Ob-\\nservation (ReWOO), Reason and Act (ReAct), and Dialog-\\nEnabled Resolving Agents (DERA).\\nReasoning without Observation (ReWOO) [175] aims to\\ndecouple reasoning from direct observations. ReWOO operates\\nby enabling LLMs to formulate comprehensive reasoning plans\\nor meta-plans without immediate reliance on external data\\nor tools. This approach allows the agent to create a struc-\\ntured framework for reasoning that can be executed once the\\nnecessary data or observations are available. In ReWOO, the\\nLLM initially develops a plan (a series of steps) that outlines\\nhow to approach and solve a given problem. This meta-\\nplanning phase is crucial as it sets the stage for the agent to\\nprocess information once it becomes available. The execution\\nphase then involves integrating actual data or observations into\\nthe pre-specified plan, leading to coherent and contextually\\nrelevant responses. ReWOO offers significant advantages in\\nterms of token efficiency and robustness to tool failure. It\\nenables LLMs to handle tasks where immediate access to\\nexternal data is not available, relying instead on a well-\\nstructured reasoning framework. This method is particularly\\nadvantageous in scenarios where data retrieval is costly, slow,\\nor uncertain, allowing the LLM-based agent to maintain a high\\nlevel of performance and reliability.\\nReason and Act (ReAct)[176] prompts LLMs to generate\\nnot only verbal reasoning but also actionable steps, thus\\nenhancing the model’s dynamic problem-solving capabilities.\\nReAct is grounded in the principle of integrating reasoning\\nwith action. In this approach, the LLM is prompted to alternate\\nbetween generating reasoning traces (explanations) and taking\\nactions (steps or commands) in an interleaved manner. This\\napproach allows the model to dynamically reason about a prob-\\nlem, and propose and take concrete actions simultaneously.\\nDialog-Enabled Resolving Agents (DERA) [177] are spe-\\ncialized AI agents that can engage in dialogue, resolve queries,\\nand make decisions based on interactive exchanges. DERA\\nis developed based on the idea of utilizing multiple agents\\nwithin a dialog context, each with specific roles and functions.\\nThese agents can include Researchers, who gather and analyze\\ninformation, and Deciders, who make final judgments based\\non the information provided. This division of roles allows for\\na well-organized and efficient approach to problem-solving\\nand decision-making. DERA is particularly advantageous in\\nscenarios requiring complex decision-making and problem-\\nsolving, such as those in medical diagnostics or customer ser-\\nvice. The collaborative and interactive nature of DERA agents\\nallows them to handle intricate queries with a level of depth\\nand nuance that single-agent systems might struggle with.\\nMoreover, this approach aligns well with human decision-\\nmaking processes, making AI reasoning more relatable and\\ntrustworthy.\\nV. P OPULAR DATASETS FOR LLM S\\nLarge language models exhibit promising accomplish-\\nments, but the main question that arises is how effectively\\nthey function and how their performance can be assessed in\\nspecific tasks or applications.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 26, 'page_label': '27'}, page_content='Fig. 39: HuggingGPT: An agent-based approach to use tools and planning [image courtesy of [171]]\\nFig. 40: A LLM-based agent for conversational information\\nseeking. Courtesy of [36].\\nThe evaluation of LLMs poses particular challenges due\\nto the evolving landscape of their applications. The original\\nintent behind developing LLMs was to boost the performance\\nof NLP tasks such as translation, summarization, question-\\nanswering, and so on [178]. However, it is evident today\\nthat these models are finding utility across diverse domains\\nincluding code generation and finance. Moreover, the eval-\\nuation of LLMs encompasses several critical considerations\\nsuch as fairness and bias, fact-checking, and reasoning. In\\nthis section, we outline the commonly used benchmarks for\\nassessing LLMs. These benchmarks are categorized based on\\ntraining or evaluating the LLM Capabilities.\\nA. Datasets for Basic Tasks: language model-\\ning/understanding/generation\\nThis section provides an overview of the benchmarks and\\ndatasets suited to evaluate the basic abilities of LLMs.\\n• Natural Questions [179] is a QA dataset that consists\\nof real anonymized, aggregated queries submitted to\\nthe Google search engine as questions. An annotator\\nis presented with a question along with a Wikipedia\\npage from the top 5 search results, and annotates a\\nlong answer (typically a paragraph) and a short answer\\n(one or more entities) if present on the page, or marks\\nnull if no long/short answer is present.\\n• MMLU [180] is intended to evaluate the knowl-\\nedge gained in zero-shot and few-shot scenarios. That\\nmeans that MMLU assesses both the general knowl-\\nedge and problem-solving ability of a model. It covers\\n57 subjects in STEM, humanities, social sciences,\\nand other areas. The benchmark varies in complexity,\\nranging from elementary to advanced professional.\\nIt is worth mentioning that the main contribution of\\nthis dataset is for multi-task language understanding,\\nquestion answering, and arithmetic reasoning.\\n• MBPP [181] stands for “Mostly Basic Python Prob-\\nlems” and provides a benchmark for evaluating the\\nperformance of models designed for code generation.\\nThe benchmark encompasses 974 short Python pro-\\ngrams including a wide range of topics, including\\nfundamental programming concepts and standard li-\\nbrary usage, and more. Each challenge comprises a\\ntask description, a code solution, and three automated\\ntest cases.\\n• HumanEval [182] is a dataset for code generation\\ntask. This dataset consists of 164 hand-crafted pro-\\ngramming challenges. Each challenge is accompanied\\nby a function signature, docstring, code body, and mul-\\ntiple unit tests. The main intuition behind developing\\nthis dataset is to guarantee the exclusion of its contents\\nfrom training datasets for code generation models.\\n• APPS [183] is designed for code generation task\\nfocusing on the Python programming language. The\\nAPPS dataset contains a collection of 232, 444 Python\\nprograms. Each program in the dataset has an average\\nof 18 lines of Python code. Additionally, APPS offers\\naccess to a repository of 10, 000 unique programming'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 27, 'page_label': '28'}, page_content='Fig. 41: Dataset applications.\\nexercises, each with text-based problem descriptions.\\nThe final aspect to highlight is that the it includes test\\ncases.\\n• WikiSQL [184] is crafted for code generation task and\\nit has 87,726 carefully labeled pairs of SQL queries\\nand corresponding natural language questions from\\nWikipedia tables. The SQL queries comprise three\\nsubsets: test sets ( 17, 284 examples), development\\n(9, 145 examples), and training ( 61, 297 examples).\\n• TriviaQA [185] is designed for QA task. This\\ndataset comprises more than 650, 000 question-\\nanswer-evidence triples. There are 95, 000 question-\\nanswer pairs in this dataset, each authored by trivia en-\\nthusiasts and supported by an average of six indepen-\\ndently sourced evidence documents. These documents\\nare automatically acquired from Wikipedia or broader\\nweb search results. The dataset is categorized into\\ntwo segments, including those with authentic answers\\nfrom Wikipedia and web domains, and verified sets\\nembody the accurately answered questions along with\\ntheir associated documents from both Wikipedia and\\nonline.\\n• RACE [186] suits for reading comprehension task.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 28, 'page_label': '29'}, page_content='Fig. 42: Datasets licensed under different licenses.\\nThis dataset is based on English tests completed by\\nChinese students from middle school and high school,\\naged 12 to 18, and it contains roughly 28, 000 texts\\nand 100, 000 questions rigorously prepared by human\\nspecialists, primarily English instructors. This dataset\\ncontains a wide range of subjects that were purpose-\\nfully chosen to assess students’ comprehension and\\nreasoning abilities. This dataset is available in three\\nsubgroups: RACE-M, RACE-H, and RACE. RACE-\\nM refers to the middle school examinations, whereas\\nRACE-H denotes the high school tests. Finally, RACE\\nis the synthesis of RACE-M and RACE-H.\\n• SQuAD [187] stands for “Stanford Question Answer-\\ning Dataset” and is a crowdsourced reading compre-\\nhension dataset based on Wikipedia articles. It has\\napproximately 100, 000 question-answer pairs con-\\nnected to more than 500 articles. The answers to\\nthese questions are typically text fragments or spans\\ntaken from the corresponding reading passages. The\\nquestions may be unanswerable in some cases. The\\ndataset is divided into three sets: an 80% training set,\\na 10% development set, and a 10% hidden test set.\\n• BoolQ [188] is a yes/no question-answering dataset\\nwhere the goal is reading comprehension task. BoolQ\\nincludes 15, 942 examples. Each example is a triplet\\nthat includes a question, a relevant paragraph, and\\nthe solution. Although the main intuition behind\\nthis dataset is for reading comprehension, it can be\\nused for reasoning, natural language inference, and\\nquestion-answering tasks.\\n• MultiRC [189] is another dataset that fits reading\\ncomprehension task. MultiRC contains brief para-\\ngraphs as well as multi-sentence questions that can\\nbe answered using the information in the paragraph.\\nThe paragraphs in this dataset come from a variety\\nof sources, including news, fiction, historical texts,\\nWikipedia articles, discussions on society and law,\\nelementary school science textbooks, and 9/11 re-\\nports. Each question has many response choices, with\\none or more of them being correct. Answering the\\nquestions requires reasoning across several sentences.\\nMultiRC dataset encompasses around 6, 000 multi-\\nsentence questions gathered from over 800 paragraphs.\\nOn average, each question offers about two valid\\nanswer alternatives out of a total of five.\\nB. Datasets for Emergent: ICL, reasoning (CoT), instruction\\nfollowing\\nThis section centers on the benchmarks and datasets em-\\nployed to evaluate the emergent abilities of LLMs.\\n• GSM8K [190] is designed to evaluate the model’s\\nability for multi-step mathematical reasoning. GSM8K\\nincludes 8.5K linguistically diverse grade school math\\nword problems written by humans. The dataset is split\\ninto two sets: a training set with 7.5K problems,\\nand a test set with 1K problems. These problems\\nneed 2 to 8 steps to be solved. Solutions mainly'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 29, 'page_label': '30'}, page_content='are a series of elementary calculations using basic\\narithmetic operations.\\n• MATH [191] enables to assess how well models can\\nsolve math problems. MATH dataset hast 12, 500\\nproblems from high school math competitions. Each\\nproblem in the dataset has a step-by-step solution and\\na final answer enclosed in a box. The problems cover\\na wide range of topics and have different levels of\\ncomplexity. There are seven subjects in total. Further-\\nmore, the difficulty of each problem is rated based\\non the AoPS standards on a scale from ′1′ to ′5′. A\\n′1′ shows the easiest problems in a subject, while ′5′\\nrepresents the most difficult. In terms of formatting,\\nall problems and solutions are presented using LATEX\\nand the Asymptote vector graphics language.\\n• HellaSwag [192] is designed to assess commonsense\\nreasoning in LLMs. This benchmark includes 70, 000\\nmultiple-choice questions. Each question is derived\\nfrom one of two domains: ActivityNet or WikiHow,\\nand presents four answer choices regarding what\\nmight happen in the following situation. The correct\\nanswer provides an actual statement describing the\\nupcoming event, but the three wrong answers are\\ncreated to confuse machines.\\n• AI2 Reasoning Challenge (ARC) [193] is used\\nfor commonsense reasoning. This benchmark encom-\\npasses 7, 787 science examination questions. These\\nquestions are in English, and most of them are set\\nup in a multiple-choice format. The questions have\\nbeen divided into two groups: a Challenge Set with\\n2, 590 difficult questions and an Easy Set with 5,197\\nquestions. Each collection has also been pre-divided\\ninto Train, Development, and Test subsets.\\n• PIQA [194] is intended to evaluate the language\\nrepresentations on their knowledge of physical com-\\nmonsense. In this dataset, the focus is on everyday\\nsituations with a preference for uncommon solutions.\\nThe central task is a multiple-choice question answer-\\ning, where a question (q) is provided along with two\\npotential solutions (s1, s2). Then, the best solution is\\nchosen by whether a model or a human. For each\\nquestion, only one of the solutions is the correct\\nanswer.\\n• SIQA [195] provides a framework for evaluating mod-\\nels’ ability for commonsense reasoning about social\\nsituations. SIQA dataset has 38, 000 multiple-choice\\nquestions designed to assess emotional and social\\nintelligence in everyday circumstances. This dataset\\ncovers a wide variety of social scenarios. In SIQA,\\nthe potential answers is a mixture of human-selected\\nresponses and machine-generated ones that have been\\nfiltered through adversarial processes.\\n• OpenBookQA (OBQA) [196] is a new kind of\\nquestion-answering dataset where answering its ques-\\ntions requires additional common and commonsense\\nknowledge not contained in the book and rich text\\ncomprehension. This dataset includes around 6,000\\nmultiple-choice questions. Each question is linked to\\none core fact, as well as an additional collection\\nof over 6000 facts. The questions were developed\\nusing a multi-stage crowdsourcing and expert filter-\\ning procedure. OpenBookQA questions are difficult\\nbecause they need multi-hop reasoning with limited\\nbackground.\\n• TruthfulQA [197] is designed specifically to eval-\\nuate the truthfulness of language models in gen-\\nerating answers to questions. This dataset includes\\n817 questions, written by authors, from 38 different\\ncategories, including health, law, finance, and politics.\\nThese questions are purposefully designed to chal-\\nlenge human responders, as they may contain common\\nmisunderstandings that lead to incorrect answers.\\n• OPT-IML Bench [103] is a comprehensive bench-\\nmark for Instruction Meta-Learning. It covers 2000\\nNLP tasks from 8 existing benchmarks. The OPT-IML\\nBench consists of a training set with 17.9 M examples,\\na dev set with 145K samples, and a test set with 321K\\nsamples.\\nC. Datasets for Augmented: using external knowledge/tools\\nThis section focuses on datasets designed for the aug-\\nmented abilities of LLMs.\\n• HotpotQA [198] is designed to cover a diverse and\\nexplainable question-answering dataset that necessi-\\ntates multi-hop reasoning. This dataset is derived from\\nthe English Wikipedia. It consists of roughly 113, 000\\nquestions. Each question in the dataset comes with\\ntwo paragraphs, called gold paragraphs, from two\\nWikipedia articles. Also, there is a list of sentences\\nin those paragraphs that crowdworkers have picked as\\nimportant for answering the question.\\n• ToolQA [199] is a question answering benchmark\\nto evaluate LLMs’ ability to use external tools for\\nanswering questions.\\n• GPT4Tools serves as an instructional dataset, gener-\\nated by instructing advanced teachers (such as Chat-\\nGPT), with instructions conditioned on visual content\\nand tool descriptions. This process results in the\\ngeneration of instructions related to the use of tools.\\nThere are three versions of this dataset. The first\\nversion comprises 71,000 instruction-following data\\npoints utilized to fine-tune the GPT4Tools model. The\\nnext version consists of manually cleaned instruction\\ndata used for validation, covering instructions related\\nto the tools from the first version. The last version is\\ncleaned instruction data used for testing and includes\\ninstructions related to some tools that are not present\\nin the first version.\\nVI. P ROMINENT LLM S’ PERFORMANCE ON\\nBENCHMARKS\\nIn this section we first provide an overview of some of\\npopular metrics used for evaluating the performance of LLMs\\nunder different scenarios. We then look at the performance\\nof prominent large language models on some of the popular\\ndatasets and benchmarks.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 30, 'page_label': '31'}, page_content='TABLE II: LLM Datasets Overview.\\nBenchmark Name Evaluation Metric Leaderboard Source paperswithcode\\nHumanEval PASS@k Link Link Link\\nMBPP PASS@k, Accuracy - Link Link\\nAPPS PASS@k, Accuracy - Link Link\\nWikiSQL Accuracy - Link Link\\nCoNaLa BLEU Link Link\\nCodeParrot PASS@k - Link -\\nHellaSwag Accuracy Link Link Link\\nAI2 Reasoning\\nChallenge (ARC) Accuracy Link Link Link\\nBoolQ Accuracy - Link Link\\nMultiRC F1-score, Accuracy - Link Link\\nCNN/Daily Mail [200] Accuracy - Link -\\nSQuAD F1-score, EM Link Link Link\\nRACE Accuracy - Link Link\\nCNN/Daily Mail [201] ROUGE - Link Link\\nDrop F1-score, EM Link Link Link\\nQuAC F1-score, HEQ-Q, HEQ-D Link Link Link\\nTriviaQA EM, F1-score, Accuracy Link Link Link\\nNatural Questions EM, F1-score, Accuracy Link Link Link\\nStrategyQA Accuracy, Recall@10, SARI Link Link Link\\nCoQA F1-score Link Link Link\\nXSum ROUGE - Link Link\\nSAMSum ROUGE - - Link\\nWikiSum ROUGE - Link -\\nDialogSum ROUGE - Link Link\\nTruthfulQA MC1 , MC2, % true, % info, BLEURT Link Link Link\\nMMLU Accuracy Link Link Link\\nGSM8K Accuracy Link Link Link\\nPIQA Accuracy Link Link Link\\nSIQA Accuracy Link Link Link\\nOpenBookQA (OBQA) Accuracy Link Link Link\\nHotpotQA EM, F1-score, Joint EM, Joint F1-score, Link Link Link\\nMATH Accuracy - Link Link\\nCommonsenseQA Accuracy Link Link Link\\nNatural Instructions ROUGE-L, Human Link Link Link\\nBIG-bench Accuracy, Average - Link Link\\nToolTalk Success rate, Precision, Recall, Incorrect\\naction rate, Percent of failing error types - Link Link\\nMetaTool Accuracy, Precision, Recall, F1-score - Link Link\\nGPT4Tools\\nSuccessful Rate of Thought, Successful\\nRate of Action, Successful Rate of Ar-\\nguments, Success Rate\\n- Link Link\\nAPI-Bank\\nCorrectness, ROUGE, Error(API Hallu-\\ncination, Has Exception, Invalid Input\\nParameters, False API Call Format, API\\nCall, Miss Input Parameters)\\n- Link Link\\nAlpaca-CoT - - Link Link\\nA. Popular Metrics for Evaluating LLMs\\nEvaluating the performance of generative language models\\ndepends on the underlying task they are going to be used for.\\nTasks that are mostly about selecting a choice out of given\\nones (such as sentiment analysis), can be seen as simple as\\nclassification and their performance can be evaluated using\\nclassification metrics. Metrics such as accuracy, precision,\\nrecall, F1, etc are applicable in this case. It is also important to\\nnote that the answers generated by the model for specific tasks\\nsuch as multi-choice question answering are always either True\\nor False. If the answer is not in a set of options, it can be seen\\nas False as well.\\nHowever, some tasks that are purely open-ended text gener-\\nation cannot be evaluated in the same way as for categorization.\\nDifferent metrics are required for the specific purpose of the\\nevaluation. Code generation is a very different case in open-\\nended generative evaluations. The generated code must pass\\nthe test suite but on the other hand, it is also important\\nto understand if a model is capable of generating different\\nsolutions as a code, what is the probability of selecting the\\ncorrect one among them. Pass@k is a very good metric in this\\ncase. It works in this manner that given a problem, different\\nsolutions as code are generated. They are tested for correctness\\nusing different functionality tests. Afterward, from generated\\nn solutions, and the respective c number of them being correct\\nequation 4 provides the final value.\\npass@k := E\\nProblems\\n\"\\n1 −\\n\\x00n−c\\nk\\n\\x01\\n\\x00n\\nk\\n\\x01\\n#\\n(4)\\nExact match (EM) is another metric that is mostly con-\\ncerned with exact matches from (pre-defined) answers. It\\ncounts a prediction as correct if it exactly matches one of\\nmore than one desired reference text token by token. In some\\ncases, it can be the same as accuracy and the equation 5 shows\\nthe mathematical definition. Here M is total number of correct\\nanswers and N is the total number of questions [202].\\nEM = M\\nN (5)'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 31, 'page_label': '32'}, page_content='Human equivalence score (HEQ) on the other hand, is an\\nalternative to F1 score [203]. HEQ-Q represents the precision\\nof individual questions, wherein an answer is deemed correct\\nif the model’s F1 score surpasses the average human F1 score.\\nLikewise, HEQ-D denotes the precision of each dialogue; it is\\ndeemed accurate when all questions within the dialogue meet\\nthe criteria of HEQ [182].\\nEvaluation of other generative tasks such as machine trans-\\nlation are based on metrics such as Rouge and BLEU. These\\nscores work well when there is a reference text as ground\\ntruth (such as translation) and a hypothesis that is generated\\nby the generative model, in our case the LLM. These scores\\nare mostly used for cases where the goal is to detect the\\nsimilarity of the answer and ground truth in a computation\\nmanner. In a computation manner, it meant that nothing more\\nthan N-Grams would be used. However, metrics such as BERT-\\nScore are also good for these cases but they are also heavily\\nerroneous because another model is used to judge. Still, even\\ntoday, evaluating purely generated content is very hard and\\nno completely fitting metric is not found, metrics are either\\nlooking for simplistic features such as N-Gram, SkipGram,\\netc, or they are models with unknown accuracy and preciseness\\n[204].\\nGenerative evaluation metrics are also another type of eval-\\nuation metric for LLMs that use another LLM for evaluating\\nthe answer. However, depending on the task itself, evaluation\\ncan be possible in this way or not. Another dependency\\nthat makes generative evaluation error-prone is reliance on\\nthe prompt itself. RAGAS is one of the good examples that\\nincorporate the usage of generative evaluation.\\nVarious benchmarks and leaderboards have been proposed\\nto address the most challenging question in the world of\\nlarge language models: Which one is better? However not\\na simple answer can address this question. The answer de-\\npends on various aspects of large language models. Section V\\nshows the categorical presentation of different tasks and the\\nmost important datasets in each category. We will follow the\\nsame categorization and provide a comparison based on each\\ncategory. After providing comparison for each category, we\\nwill provide a broad overview of aggregated performance by\\naveraging the reported performance metric on different tasks.\\nEvaluating different LLMs can be seen also from different\\nperspectives. For example, a LLM with a drastically fewer\\nnumber of parameters is not completely comparable to one\\nwith a larger number of parameters. From this perspective, we\\nwill categorize LLMs in four categories as well: small (less\\nthan or equal to 1 billion parameters), medium (between 1 and\\n10 billion), large (between 10 and 100 billion), and very large\\n(more than 100 billion). Another classification for the LLMs\\nwe use is their primary use case. We consider each LLM to\\nbe either: Foundation model (pretrained language model with\\nno instruction fine-tuning and chat fine-tuning), Instruction\\nmodel (pretrained language model with only instruction fine-\\ntuning), and Chat model (pretrained language model with\\ninstruction and chat fine-tuning). Apart from all the catego-\\nrization described, another category is required to distinguish\\nbetween original models and tuned ones. Original models are\\nthose that have been released as a foundation model or a fine-\\ntuned one. Tuned models are those that grasped the original\\nmodel and tuned it with different datasets or even different\\ntraining approaches. It is also good to note that original models\\nare usually foundation models that have been fine-tuned on\\nspecific datasets or even different approaches. Availability of\\nthe model weights regardless of the license is another category\\nin our classification. Models that have their weights publicly\\navailable (even through request) are noted as Public models\\nwhile others are noted as Private. Table III shows all of these\\ndefinitions and abbreviations used in the rest of the article.\\nFigure 43 illustrate these visually.\\nAccording to the provided categorizations, we can catego-\\nrize and label each notable LLM as shown in table IV. As can\\nbe seen from this table, models categorized as very large are\\nalso unavailable as well.\\nB. LLMs’ Performance on Different Tasks\\nCommonsense reasoning is one of the important capabili-\\nties each model can obtain. This capability denotes the ability\\nof the model to use prior knowledge in combination with\\nreasoning skills. In the case of HellaSwag for example, finding\\nthe continuation of text is challenging because the given text\\ncontains a partial part of the story while the given choices\\nas continuation are tricky to select, and without having prior\\nknowledge about the world it is not possible. This specific kind\\nof reasoning deserves high attention because it is related to\\nutilizing previous knowledge with open text-described scenes\\nor facts. As can be seen from table V not just Unavailable\\nmodels but also Public ones can achieve good results on\\nvarious tests.\\nTABLE V: Commonsense reasoning comparison.\\nModel OBQA HellaSwag\\nDavinci-003 51 83.4\\nFalcon 7B 44.4 76.3\\nAlpaca 43.4 73.9\\nPythia 7B 37.2 64\\nPythia 12B 43.2 68.1\\nLLAMA 7B 42.4 73\\nDolly 6B 41.2 67.6\\nDolly 12B 40.4 71\\nAlpaca 7B 43.4 73.9\\nAlpaca Lora 7B 42.6 74\\nGPT-J 6.7B 38.2 66.2\\nLLama 7B 42.4 73\\nLLama 13B 42.2 76.2\\nPythia 6.7B 37.2 64\\nPythia 12B 38 67.3\\nStableLM Tuned 33.4 53.6\\nKoala 13B 42.8 72.6\\nMosaic mpt-7B 42.6 76.3\\nLLAMA 2 70B - 87.33\\nLLAMA 65B - 86.09\\nFalcon 40B - 85.3\\nFalcon 180B - 88.86\\nMPT Instruct 30B - 84.31\\nMPT Instruct 7B - 77.91\\nYi 6B - 76.42\\nYi 34B - 85.69\\nGPT-4 - 95.3\\nGemini Ultra - 87.8\\nFrom the results presented in Table V it is clear that GPT-4\\nachieves best results for HellaSwag while Davinci-003 is best\\nmodel for OBQA. It is also good to note that results for OBQA\\nare not reported for all of the models and possibly davinci-003\\nis not the best model achieving highest results on OBQA.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 32, 'page_label': '33'}, page_content='TABLE III: LLM categories and respective definitions.\\nClassification Category Description\\nSize\\nSmall Number of parameters ≤ 1B\\nMedium 1B < Number of parameters ≤ 10B\\nLarge 10B < Number of parameters ≤ 100B\\nVery Large 100B < Number of parameters\\nType\\nFoundation model Pretrained language model\\nInstruction model Pretrained and instruction fine-tuned language model\\nChat model Pretrained, instruction fine-tuned, and chat fine-tuned language model\\nOrigin Original model An original model released with either Foundation, Instruction, or Chat model\\nTuned model Fine-tuned version of an original model\\nAvailability Publicly available Model and weights are available due to request to without request\\nPublicly unavailable Model and weights are not publicly available\\nTABLE IV: Different LLM categorization.\\nModel Size #Params (B) Type Availability Origin\\nDavinci-002 Very Large 175 Instruction Unavailable Tuned\\nDavinci-003 Very Large 175 Instruction Unavailable Tuned\\nGPT 3.5-turbo Large 20 Chat Unavailable Tuned\\nFalcon 7B Medium 7 Foundation Public Original\\nAlpaca Large 13 Chat Public Tuned\\nPythia 7B Medium 7 Foundation Public Original\\nPythia 12B Large 12 Foundation Public Original\\nLLAMA 7B Medium 7 Chat Public Original\\nLLAMA 2 7B Medium 7 Chat Public Tuned\\nLLAMA 2 7B Medium 7 Foundation Public Original\\nVicuna 13B Large 13 Foundation Public Tuned\\nVicuna 7B Medium 7 Foundation Public Tuned\\nClaude Large 93 Chat Unavailable Original\\nClaude 2 Very Large 137 Chat Unavailable Original\\nNot all models report their performance on all datasets, and\\nbecause of that, the number of models for which performance\\nis reported in different tables varies.\\nTABLE VI: Symbolic reasoning comparison.\\nModel Cobjects Penguins\\nGPT-NeoX 26 33.56\\nOPT 66B 31.2 28.08\\nBloomberg GPT 34.8 37.67\\nBLOOM 176B 36.8 40.41\\nPaLM 540B 38 44.5\\nGopher-280B 49.2 40.6\\nChinchilla-70B 59.7 48.7\\nPaLM 2 61.2 65.8\\nWorld knowledge is mostly about general knowledge ques-\\ntions, for example, in Wikifact dataset questions such as ”Who\\nis the author of a specific well-known book” can be found and\\nreferences are also provided. Table VII shows the results.\\nTABLE VII: World knowledge comparison.\\nModel TriviaQA NaturalQ WebQ ARC\\nBLOOM - - - 32.9\\nBLOOM 176B - - - 50.85\\nBloomberg GPT - - - 48.63\\nChinchilla - 35.5 - -\\nCodex + REPLUG 76.8 44.7 - -\\nGAL 120B - - - 67.9\\nGLaM 62B/64E 75.8 32.5 15.5 50.3\\nGopher - 28.2 - -\\nGPT-3 175B 71.2 29.9 41.5 85.2\\nGPT-4 - - - 96.4\\nGPT-NeoX - - - 45.39\\nLLaMA 13B - - - 52.7\\nLLaMA 2 70B 85 33 - -\\nLLaMA 33B - 24.9 - 57.8\\nLLaMA 65B 72.6 39.9 - -\\nLLaMA 7B - - - 47.6\\nMistral 7B 69.9 28.8 - 55.5\\nNeo-6B - 13.7 - -\\nOPT - - - 31.1\\nOPT 66B - - - 44.54\\nOPT-175B - - - 43.94\\nOPT-175B - - - 25.6\\nPaLM 2-L 86.1 37.5 28.2 95.1\\nPaLM 2-M 81.7 32 26.9 64.9\\nPaLM 2-S 75.2 25.3 21.8 59.6\\nPaLM-540B 81.4 39.6 43.5 87.1\\nphi-1.5-web 1.3B - - - 44.9\\nSparseGPT - - - 38.99\\nSparseGPT - - - 39.85\\nSparseGPT - - - 41.3\\nFor some specific use-case models, it is highly demanded to\\nhave coding and code-generation capability. Table VIII shows\\nthe results of different models on coding capability.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 33, 'page_label': '34'}, page_content='Large\\nLanguage\\nModels\\nParameters\\nAvailability\\nOriginality\\nType\\nSmall LM\\n# of params <1B\\nMedium LM\\n1B < # of params <10B\\nLarge LM\\n10B < # of params <100B\\nVery Large LM\\n100B < # of params\\nTuned\\nFine tuning\\nOriginal\\nPublicPrivate\\nFoundation\\nInstruction\\nChat\\nFine tuned models that are originally\\nbased on original models.\\nExample: Alpaca (based on LLaMA)\\nOriginal models that are not fine\\ntuned or based on any other\\npretrained model.\\nExample: LLaMA\\nModel weights are publicly released\\nand is available.\\nExample: LLaMA\\nModel weights are NOT publicly\\nreleased and is NOT available.\\nExample: GPT-4\\nPretrained model with no instruction\\nor chat fine-tuning.\\nExample: MPT-7B\\nPretrained model that is\\nalso fine-tuned on\\ninstruction following.\\nExample: MPT-7B-instruct\\nPretrained model that is\\nalso fine-tuned on chat.\\nExample: MPT-7B-chat\\nFig. 43: LLM categorizations.\\nTABLE VIII: Coding capability comparison.\\nModel HumanEval\\nGemini Ultra 74.4\\nGemini Pro 67.7\\nGPT-4 67\\nWizardCoder 15B 57.3\\nphi-1 1.3B 50.6\\nCode Llama 48.8\\nGPT-3.5 48.1\\nOctoCoder 46.2\\nphi-1-small 45\\nPaLM 2-S 37.6\\nInstructCodeT5+ 16B 35\\nMistral 7B 30.5\\nLLaMA 2 29.9\\nphi-1-base 29\\nCodex-12B 28.81\\nPaLM 540B 26.2\\nCodeT5+ 2B 24.2\\nLLaMA 65B 23.7\\nLLaMA 33B 21.7\\nPaLM 62B 15.9\\nLLaMA 13B 15.8\\nLaMDA 137B 14\\nMIM-350M 13.7\\nLLaMA 7B 10.5\\nPaLM 8B 3.6\\nArithmetic reasoning is another challenging reasoning ca-\\npability to achieve. GSM8K for example contains grade school\\nmathematical questions with respect to their answers. Table IX\\nprovides an insight for different model comparisons.\\nTABLE IX: Arithmetic reasoning comparison.\\nModel GSM8k MATH\\nGemini Ultra 94.4 53.2\\nGPT-4 87.1 42.5\\nGemini Pro 86.5 32.6\\nToRA 70B 84.3 49.7\\nMathCoder-L-70B 83.9 -\\nMetaMath 70B 82.3 26\\nMuggleMATH 70B 82.3 -\\nMathCoder-CL-34B 81.7 45.2\\nToRA-Code 34B 80.7 50.8\\nMetaMath-Mistral-7B 77.7 -\\nArithmo2-Mistral-7B 76.4 -\\nToRA-Code 13B 75.8 48.1\\nArithmo-Mistral-7B 74.7 -\\nMathCoder-CL-13B 74.1 35.9\\nMuggleMATH 13B 74 -\\nCodeT5+ 73.8 -\\nKwaiYiiMath 13B 73.3 -\\nToRA-Code 7B 72.6 44.6\\nMathCoder-L-13B 72.6 29.9\\nMetaMath 13B 71 22.5\\nLLaMA 65B 69.7 10.6\\nMuggleMATH 7B 68.4 -\\nMathCoder-CL-7B 67.8 23.3\\nMetaMath 7B 66.4 19.4\\nRFT 70B 64.8 -\\nMathCoder-L-7B 64.2 -\\nOrca 2-13B 59.14 -\\nU-PaLM 58.5 -\\nPaLM-540B 58.1 8.8\\nLLaMA 2 70B 56.8 -\\nRFT 13B 55.3 -\\nLLaMA 33B 53.1 7.1\\nMistral 7B 52.2 13.1\\nRFT 7B 51.2 -\\nLLaMA 65B 50.9 20.5\\nOrca 2-7B 47.23 -\\nText-davinci-002 40.7 19.1\\nLLaMA 33B 35.6 3.9\\nGPT-Neo-2.7B 19.5 -\\nLLaMA 7B 18.1 2.9\\nPaLM 540B 17.9 8.8\\nLLaMA 13B 17.8 3.9\\nLLaMA 7B 11 2.9\\nGPT-Neo-125M 7.5 -\\nPaLM 8B 4.1 1.5\\nGPT-2 - 5.4\\nGPT-3 175B - 5.2\\nPaLM 62B - 4.4\\nGPT-3-13B - 3\\nLLaMA 7B 11 2.9\\nPaLM 8B - 1.5'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 34, 'page_label': '35'}, page_content='Large language models in some cases are hallucinating an-\\nswers simply because they are next-token prediction machines.\\nHallucination is one of the important factors in measuring\\nhow much a large language model is trustworthy and reliable.\\nMeasuring hallucination on the other hand is also not easy as it\\nseems because each fact can be written in different styles and\\neven the smallest changes in writing make it hard to detect.\\nIt is fair to assume if any particular LLM is more capable\\nto detect hallucination of false information in text, it is also\\nmore trustworthy. HaluEval is one of the datasets that aims to\\nmeasure hallucination in this field [205]. Evaluation can also be\\nperformed by another model judging the response with regard\\nto the actual answer [206]. Table X shows the evaluation of\\ndifferent models based on these datasets.\\nVII. C HALLENGES AND FUTURE DIRECTIONS\\nAs we have seen in the previous sections, large language\\nmodels have achieved impressive results in the past 1-2 years.\\nAt the same time this is still a new and extremely active\\nresearch area where the pace of innovation is increasing rather\\nthan slowing down. As in any other evolving area though, there\\nare still numerous challenges ahead. Here we briefly mention\\nsome of the challenges and main active areas which are known\\nso far. It is worth noting that LLM challenges are discussed\\nin details in a work by Kaddour et al. [207].\\nA. Smaller and more efficient Language Models\\nThis is a survey on large language models, and there\\nhas been an initial push towards ”larger is better” that has\\nclearly been rewarded with ever larger models like GPT-\\n4 getting better accuracy and performance in benchmarks.\\nHowever, those large models are costly and inefficient in\\nseveral dimensions (e.g. high latency). In response to all of\\nthis, there is a current research trend to come up with Small\\nLanguage Models (SLMs) as a cost-effective alternative to\\nLLMs, particularly when used on specific tasks that might not\\nrequire the full generality of larger models. Prominent works\\nin this direction include Phi-1 [208], Phi-1.5 [209], and Phi-2\\nfrom Microsoft.\\nMore generally, we should expect many research efforts in\\nthis area of how to train smaller and more efficient models.\\nTechniques such as parameter-efficient fine-tuning (PEFT),\\nteacher/student, and other forms of distillation – see section\\nIII-I – will continue to be used to build a smaller model out\\nof larger ones.\\nB. New Post-attention Architectural Paradigms\\nTransformer blocks have been a crucial and constant part of\\nmost of current LLM frameworks, and it’s a big question mark\\nhow much longer this architecture will be in vogue, and what\\nwill be the next big architectural break-through in the field of\\ndeep learning (and NLP). Since AlexNet in 2012, we have seen\\nmany architectures go in and out of fashion, including LSTM,\\nGRU, seq2seq, but Transformers have been the dominant\\napproach since its inception. As described earlier, attention is\\nthe main mechanism driving transformers. More recently, there\\nhas been promising research in alternative approaches that are\\nbeing labelled as post-attention.\\nAn important class of such class of post-attention models\\nare the so called State Space Models (SSMs). While the notion\\nof State Space Models has a long history in machine learning,\\nit should be noted that in the context of language models, SSM\\nis usually used in reference to the newer Structure State Space\\nModel architecture or S4 for short (see Gu et al. [29]). Some\\nrecent models in this category are Mamba [30], Hyena [210],\\nand Striped Hyena [211].\\nWhile all of those models are very competitive in terms of\\nperformance in leaderboards and efficiency, they also address\\nan important challenge in more traditional attention-based\\narchitectures: the lack of support for larger context windows .\\nHaving a good answer to many prompts requires context.\\nFor example, the response to ”Recommend some good movies\\nfor me” requires a lot of context about ”me” as well as what\\nmovies are available and which ones I have not watched.\\nContext length is especially important for RAG, where large\\nportions of text might be retrieved and injected into the prompt\\nfor generation (see section IV-C.\\nThe longer the context length, the more tokens we can\\nsqueeze into the context. The more information the model has\\naccess to, the better its response will be. But on the other\\nhand, with very long context, it would be hard for the model\\nto remember everything and efficiently process all the informa-\\ntion. Attention-based models are highly inefficient for longer\\ncontexts and that is why we should expect more research in\\ndifferent mechanisms that enable processing longer contexts\\nand generally come up with more efficient architectures.\\nThat being said, new architectures might not only propose\\nalternatives for the attention mechanism but rather rethink the\\nwhole Transformer architecture. As an early example of this,\\nMonarch Mixer [212] proposes a new architecture that uses\\nthe same sub-quadratic primitive that achieves high hardware\\nefficiency on GPUs – Monarch matrices – along both sequence\\nlength and model dimension.\\nOn the other end of the spectrum, it is worth mentioning\\nthat there are some attention-compatible architectural mecha-\\nnisms that have been recently gaining steam and proving their\\nvalue in creating better and more powerful LLMs. Probably\\nthe best example of such mechanism is Mixture of Experts\\n(MoE). MoEs have been around in machine learning for years,\\neven before the Deep Learning Era [213], but they have been\\ngaining popularity since then, and particularly in the context\\nof Transformer models and LLMs.\\nIn LLMs, MoEs allow to train an extremely large model\\nthan is then only partially instantiated during inference\\nwhen some of the experts are turned off wherever the gat-\\ning/weighting function has a low weight assigned to them. As\\nan example, the GLaM model has 1.2 trillion parameters, but\\nduring inference only 2 out of the 64 experts are used [84].\\nMoEs are nowadays an important component of the so-\\ncalled frontier LLMs (i.e. the most advanced and capable\\nmodels). GPT-4 itself is rumored to be based on a MoE\\narchitecture, and some of the best performing LLMs such as\\nMixtral [117], are basically an MoE version of pre-existing\\nLLMs.\\nFinally, it is important to note that MoEs can be used as a\\ncomponent of any architecture regardless of whether it is based'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 35, 'page_label': '36'}, page_content='TABLE X: Hallucination evaluation\\nModel HHEM HaluEval QA HaluEval Dialogue HaluEval Sum. HaluEval General\\nGPT 4 97 - - - -\\nGPT 4 Turbo 97 - - - -\\nGPT 3.5 Turbo 96.5 62.59 72.4 58.53 79.44\\nDavinci002 - 60.05 60.81 47.77 80.42\\nDavinci003 - 49.65 68.37 48.07 80.4\\nGPT-3 - 49.21 50.02 51.23 72.72\\nGoogle Gemini Pro 95.2 - - - -\\nLlama 2 70B 94.9 - - - -\\nLlama 2 7B 94.4 49.6 43.99 49.55 20.46\\nLlama 2 13B 94.1 - - - -\\nCohere-Chat 92.5 - - - -\\nCohere 91.5 - - - -\\nClaude 2 91.5 69.78 64.73 57.75 75\\nClaude 1 67.6 64.83 53.76 73.88\\nMicrosoft Phi 2 91.5 - - - -\\nGoogle Palm 2 (beta) 91.4 - - - -\\nMixtral 8x7B 90.7 - - - -\\nAmazon Titan Express 90.6 - - - -\\nMistral 7B 90.6 - - - -\\nGoogle Palm 2 Chat (beta) 90 - - - -\\nGoogle Palm 2 87.9 - - - -\\nGoogle Palm 2 Chat 72.8 - - - -\\nChatGLM - 47.93 44.41 48.57 30.92\\nFalcon - 39.66 29.08 42.71 18.98\\nVicuna - 60.34 46.35 45.62 19.48\\nAlpaca - 6.68 17.55 20.63 9.54\\non attention or not. In fact, MoEs have also been applied to\\nSSM-based LLMs like Mamba citepioro2024moemamba. We\\nshould continue to see MoE-driven improvements in the future\\nregardless of the underlying architecture.\\nC. Multi-modal Models\\nFuture LLMs are expected to be multi-modal and handle\\na variety of data types, such as text, images, and videos,\\naudio, in a unified manner. This opens up possibilities for\\nmore diverse applications in fields like question answering,\\ncontent generation, creative arts, and healthcare, robotics, and\\nbeyond. There are already several prominent multi-modal\\nLLMs out there, including: LLA V A [214], LLA V A-Plus [215],\\nGPT-4 [33], Qwen-vl [116], Next-GPT [216], but the trend is\\nexpected to be continued. Evaluation of these models also is a\\nnew research topic, especially conversational generative vision\\nmodels [217]. Multi-modal LLMs can unlock huge potentials\\nin a variety of tasks, and there has already been a descent\\nprogress in this direction, which needs a dedicated paper to\\ndiscuss all its details.\\nD. Improved LLM Usage and Augmentation techniques\\nAs we described in sectionIV, many of the shortcomings\\nand limitations of LLMs such as hallucination can be ad-\\ndressed through advanced prompt engineering, use of tools,\\nor other augmentation techniques. We should expect not only\\ncontinued, but accelerated research in this area. It is worth\\nmentioning that, in the specific case of software engineering,\\nsome works ([218]) tried to automatically eliminate this issue\\nfrom the overall software engineering workflow\\nLLM-based systems are already starting to replace ma-\\nchine learning systems that were until recently using other\\napproaches. As a clear example of this, LLMs are now being\\ndeployed to better understand people preference and interests,\\nand provide more personalized interactions, whether in cus-\\ntomer service, content recommendation, or other applications.\\nThis involves better understanding of user preferences, and\\nanalyzing their past interactions and using them as the context.\\nWe will continue to see research in the application and usage\\nof LLMs for not only personalization and recommendations ,\\nbut many other application areas using other machine learning\\ntechniques.\\nFinally, another important area of research we expect to\\ngather increased attention is that of LLM-based agents and\\nmulti-agent systems [172], [173], [174]. The development of\\nLLM systems with access to external tools and decision-\\nmaking capabilities is both exciting and challenging. We will\\nsee continued research and progress in this important area that\\nsome argue could lead to Artificial General Intelligence (AGI).\\nE. Security and Ethical/Responsible AI\\nEnsuring the robustness and security of LLMs against\\nadversarial attacks and other vulnerabilities is a critical area\\nof research [219]. As LLMs are increasingly deployed in real-\\nworld applications, they need to be protected from potential\\nthreats, to prevent them being used to manipulate people or\\nspread mis-information. Improving the reasoning capabilities\\nof these model [220], would help them to better detect potential\\nadversarial attacks.\\nAddressing ethical concerns and biases in LLMs is another\\nactive area of research. Efforts are being made to ensure that\\nLLMs are fair, unbiased, and capable of handling sensitive\\ninformation responsibly. As LLMs are being used more and\\nmore by a large number of people on a daily basis, making\\nsure they are unbiased and behave responsibly is crucial.\\nVIII. C ONCLUSION\\nThis paper present a survey of LLMs developed in the\\npast few years. We first provide an overview of early pre-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 36, 'page_label': '37'}, page_content='trained language models (e.g., as BERT), then review three\\npopular LLM families (GPT, LLaMA, PaLM), and other\\nrepresentative LLMs. We then survey methods and techniques\\nof building, augmenting, and using LLMs. We review popular\\nLLM datasets and benchmarks, and compare performance of\\na set of prominent models on public benchmarks. Finally, we\\npresent open challenges and future research directions.\\nREFERENCES\\n[1] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\\nfor neural language models,” arXiv preprint arXiv:2001.08361 , 2020.\\n[2] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\\nE. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark\\net al. , “Training compute-optimal large language models,” arXiv\\npreprint arXiv:2203.15556, 2022.\\n[3] C. E. Shannon, “Prediction and entropy of printed english,” Bell system\\ntechnical journal, vol. 30, no. 1, pp. 50–64, 1951.\\n[4] F. Jelinek, Statistical methods for speech recognition . MIT press,\\n1998.\\n[5] C. Manning and H. Schutze, Foundations of statistical natural lan-\\nguage processing. MIT press, 1999.\\n[6] C. D. Manning, An introduction to information retrieval . Cambridge\\nuniversity press, 2009.\\n[7] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y . Hou, Y . Min,\\nB. Zhang, J. Zhang, Z. Dong et al. , “A survey of large language\\nmodels,” arXiv preprint arXiv:2303.18223 , 2023.\\n[8] C. Zhou, Q. Li, C. Li, J. Yu, Y . Liu, G. Wang, K. Zhang, C. Ji, Q. Yan,\\nL. He et al., “A comprehensive survey on pretrained foundation mod-\\nels: A history from bert to chatgpt,” arXiv preprint arXiv:2302.09419,\\n2023.\\n[9] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-\\ntrain, prompt, and predict: A systematic survey of prompting methods\\nin natural language processing,” ACM Computing Surveys , vol. 55,\\nno. 9, pp. 1–35, 2023.\\n[10] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\\nJ. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint\\narXiv:2301.00234, 2022.\\n[11] J. Huang and K. C.-C. Chang, “Towards reasoning in large language\\nmodels: A survey,” arXiv preprint arXiv:2212.10403 , 2022.\\n[12] S. F. Chen and J. Goodman, “An empirical study of smoothing\\ntechniques for language modeling,” Computer Speech & Language ,\\nvol. 13, no. 4, pp. 359–394, 1999.\\n[13] Y . Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic\\nlanguage model,” Advances in neural information processing systems ,\\nvol. 13, 2000.\\n[14] H. Schwenk, D. D ´echelotte, and J.-L. Gauvain, “Continuous space\\nlanguage models for statistical machine translation,” in Proceedings\\nof the COLING/ACL 2006 Main Conference Poster Sessions , 2006,\\npp. 723–730.\\n[15] T. Mikolov, M. Karafi ´at, L. Burget, J. Cernock `y, and S. Khudanpur,\\n“Recurrent neural network based language model.” in Interspeech,\\nvol. 2, no. 3. Makuhari, 2010, pp. 1045–1048.\\n[16] A. Graves, “Generating sequences with recurrent neural networks,”\\narXiv preprint arXiv:1308.0850 , 2013.\\n[17] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, “Learning\\ndeep structured semantic models for web search using clickthrough\\ndata,” in Proceedings of the 22nd ACM international conference on\\nInformation & Knowledge Management , 2013, pp. 2333–2338.\\n[18] J. Gao, C. Xiong, P. Bennett, and N. Craswell, Neural Approaches to\\nConversational Information Retrieval. Springer Nature, 2023, vol. 44.\\n[19] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning\\nwith neural networks,” Advances in neural information processing\\nsystems, vol. 27, 2014.\\n[20] K. Cho, B. Van Merri ¨enboer, D. Bahdanau, and Y . Bengio, “On\\nthe properties of neural machine translation: Encoder-decoder ap-\\nproaches,” arXiv preprint arXiv:1409.1259 , 2014.\\n[21] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Doll ´ar,\\nJ. Gao, X. He, M. Mitchell, J. C. Platt et al. , “From captions to\\nvisual concepts and back,” in Proceedings of the IEEE conference\\non computer vision and pattern recognition , 2015, pp. 1473–1482.\\n[22] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell:\\nA neural image caption generator,” in Proceedings of the IEEE\\nconference on computer vision and pattern recognition , 2015, pp.\\n3156–3164.\\n[23] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\\nand L. Zettlemoyer, “Deep contextualized word representations. corr\\nabs/1802.05365 (2018),” arXiv preprint arXiv:1802.05365 , 2018.\\n[24] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\\nof deep bidirectional transformers for language understanding,” arXiv\\npreprint arXiv:1810.04805, 2018.\\n[25] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\\npretraining approach,” arXiv preprint arXiv:1907.11692 , 2019.\\n[26] P. He, X. Liu, J. Gao, and W. Chen, “Deberta: Decoding-enhanced bert\\nwith disentangled attention,” arXiv preprint arXiv:2006.03654 , 2020.\\n[27] X. Han, Z. Zhang, N. Ding, Y . Gu, X. Liu, Y . Huo, J. Qiu, Y . Yao,\\nA. Zhang, L. Zhang et al. , “Pre-trained models: Past, present and\\nfuture,” AI Open, vol. 2, pp. 225–250, 2021.\\n[28] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, “Pre-trained\\nmodels for natural language processing: A survey,” Science China\\nTechnological Sciences, vol. 63, no. 10, pp. 1872–1897, 2020.\\n[29] A. Gu, K. Goel, and C. R ´e, “Efficiently modeling long sequences with\\nstructured state spaces,” 2022.\\n[30] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with\\nselective state spaces,” arXiv preprint arXiv:2312.00752 , 2023.\\n[31] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al.,\\n“Palm: Scaling language modeling with pathways,” arXiv preprint\\narXiv:2204.02311, 2022.\\n[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., “Llama:\\nOpen and efficient foundation language models,” arXiv preprint\\narXiv:2302.13971, 2023.\\n[33] OpenAI, “GPT-4 Technical Report,” https://arxiv.org/pdf/2303.\\n08774v3.pdf, 2023.\\n[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter,\\nF. Xia, E. Chi, Q. V . Le, and D. Zhou, “Chain-of-thought\\nprompting elicits reasoning in large language models,” in\\nAdvances in Neural Information Processing Systems , S. Koyejo,\\nS. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\\nEds., vol. 35. Curran Associates, Inc., 2022, pp. 24 824–24 837.\\n[Online]. Available: https://proceedings.neurips.cc/paper files/paper/\\n2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\\n[35] G. Mialon, R. Dess `ı, M. Lomeli, C. Nalmpantis, R. Pasunuru,\\nR. Raileanu, B. Rozi `ere, T. Schick, J. Dwivedi-Yu, A. Celikyil-\\nmaz et al. , “Augmented language models: a survey,” arXiv preprint\\narXiv:2302.07842, 2023.\\n[36] B. Peng, M. Galley, P. He, H. Cheng, Y . Xie, Y . Hu, Q. Huang,\\nL. Liden, Z. Yu, W. Chen, and J. Gao, “Check your facts and try\\nagain: Improving large language models with external knowledge and\\nautomated feedback,” arXiv preprint arXiv:2302.12813 , 2023.\\n[37] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\\n“React: Synergizing reasoning and acting in language models,” arXiv\\npreprint arXiv:2210.03629, 2022.\\n[38] D. E. Rumelhart, G. E. Hinton, R. J. Williams et al., “Learning internal\\nrepresentations by error propagation,” 1985.\\n[39] J. L. Elman, “Finding structure in time,” Cognitive science , vol. 14,\\nno. 2, pp. 179–211, 1990.\\n[40] M. V . Mahoney, “Fast text compression with neural networks.” in\\nFLAIRS conference, 2000, pp. 230–234.\\n[41] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. ˇCernock`y, “Strate-\\ngies for training large scale neural network language models,” in 2011\\nIEEE Workshop on Automatic Speech Recognition & Understanding .\\nIEEE, 2011, pp. 196–201.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 37, 'page_label': '38'}, page_content='[42] tmikolov. rnnlm. [Online]. Available: https://www.fit.vutbr.cz/\\n∼imikolov/rnnlm/\\n[43] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu,\\nand J. Gao, “Deep learning–based text classification: a comprehensive\\nreview,” ACM computing surveys (CSUR) , vol. 54, no. 3, pp. 1–40,\\n2021.\\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\\nAdvances in neural information processing systems , vol. 30, 2017.\\n[45] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\\n“Albert: A lite bert for self-supervised learning of language represen-\\ntations,” arXiv preprint arXiv:1909.11942 , 2019.\\n[46] K. Clark, M.-T. Luong, Q. V . Le, and C. D. Manning, “Electra: Pre-\\ntraining text encoders as discriminators rather than generators,” arXiv\\npreprint arXiv:2003.10555, 2020.\\n[47] G. Lample and A. Conneau, “Cross-lingual language model pretrain-\\ning,” arXiv preprint arXiv:1901.07291 , 2019.\\n[48] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and\\nQ. V . Le, “Xlnet: Generalized autoregressive pretraining for language\\nunderstanding,” Advances in neural information processing systems ,\\nvol. 32, 2019.\\n[49] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao,\\nM. Zhou, and H.-W. Hon, “Unified language model pre-training for\\nnatural language understanding and generation,” Advances in neural\\ninformation processing systems , vol. 32, 2019.\\n[50] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., “Improv-\\ning language understanding by generative pre-training,” 2018.\\n[51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\\n“Language models are unsupervised multitask learners,” OpenAI blog,\\nvol. 1, no. 8, p. 9, 2019.\\n[52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\\nwith a unified text-to-text transformer,” The Journal of Machine\\nLearning Research, vol. 21, no. 1, pp. 5485–5551, 2020.\\n[53] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant,\\nA. Barua, and C. Raffel, “mt5: A massively multilingual pre-trained\\ntext-to-text transformer,” arXiv preprint arXiv:2010.11934 , 2020.\\n[54] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y . Liu, “Mass: Masked\\nsequence to sequence pre-training for language generation,” arXiv\\npreprint arXiv:1905.02450, 2019.\\n[55] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy,\\nV . Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-\\nsequence pre-training for natural language generation, translation, and\\ncomprehension,” arXiv preprint arXiv:1910.13461 , 2019.\\n[56] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\\nA. Neelakantan, P. Shyam, G. Sastry, A. Askellet al., “Language mod-\\nels are few-shot learners,” Advances in neural information processing\\nsystems, vol. 33, pp. 1877–1901, 2020.\\n[57] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Ka-\\nplan, H. Edwards, Y . Burda, N. Joseph, G. Brockman et al. ,\\n“Evaluating large language models trained on code,” arXiv preprint\\narXiv:2107.03374, 2021.\\n[58] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332, 2021.\\n[59] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language\\nmodels to follow instructions with human feedback,” Advances in\\nNeural Information Processing Systems , vol. 35, pp. 27 730–27 744,\\n2022.\\n[60] OpenAI. (2022) Introducing chatgpt. [Online]. Available: https:\\n//openai.com/blog/chatgpt\\n[61] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei,\\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al. , “Llama\\n2: Open foundation and fine-tuned chat models,” arXiv preprint\\narXiv:2307.09288, 2023.\\n[62] R. Taori, I. Gulrajani, T. Zhang, Y . Dubois, X. Li, C. Guestrin, P. Liang,\\nand T. B. Hashimoto, “Alpaca: A strong, replicable instruction-\\nfollowing model,” Stanford Center for Research on Foundation Mod-\\nels. https://crfm. stanford. edu/2023/03/13/alpaca. html , vol. 3, no. 6,\\np. 7, 2023.\\n[63] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Ef-\\nficient finetuning of quantized llms,” arXiv preprint arXiv:2305.14314,\\n2023.\\n[64] X. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine,\\nand D. Song, “Koala: A dialogue model for academic research,” Blog\\npost, April, vol. 1, 2023.\\n[65] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,\\nD. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier et al.,\\n“Mistral 7b,” arXiv preprint arXiv:2310.06825 , 2023.\\n[66] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y . Adi,\\nJ. Liu, T. Remez, J. Rapin et al., “Code llama: Open foundation models\\nfor code,” arXiv preprint arXiv:2308.12950 , 2023.\\n[67] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Large\\nlanguage model connected with massive apis,” 2023.\\n[68] A. Pal, D. Karkhanis, M. Roberts, S. Dooley, A. Sundararajan, and\\nS. Naidu, “Giraffe: Adventures in expanding context lengths in llms,”\\narXiv preprint arXiv:2308.10882 , 2023.\\n[69] B. Huang, “Vigogne: French instruction-following and chat models,”\\nhttps://github.com/bofenghuang/vigogne, 2023.\\n[70] Y . Wang, H. Ivison, P. Dasigi, J. Hessel, T. Khot, K. R. Chandu,\\nD. Wadden, K. MacMillan, N. A. Smith, I. Beltagy et al., “How far can\\ncamels go? exploring the state of instruction tuning on open resources,”\\narXiv preprint arXiv:2306.04751 , 2023.\\n[71] S. Tworkowski, K. Staniszewski, M. Pacek, Y . Wu, H. Michalewski,\\nand P. Miło ´s, “Focused transformer: Contrastive training for context\\nscaling,” arXiv preprint arXiv:2307.03170 , 2023.\\n[72] D. Mahan, R. Carlow, L. Castricato, N. Cooper,\\nand C. Laforte, “Stable beluga models.” [Online].\\nAvailable: [https://huggingface.co/stabilityai/StableBeluga2](https://\\nhuggingface.co/stabilityai/StableBeluga2)\\n[73] Y . Tay, J. Wei, H. W. Chung, V . Q. Tran, D. R. So, S. Shakeri, X. Gar-\\ncia, H. S. Zheng, J. Rao, A. Chowdhery et al., “Transcending scaling\\nlaws with 0.1% extra compute,” arXiv preprint arXiv:2210.11399 ,\\n2022.\\n[74] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus,\\nY . Li, X. Wang, M. Dehghani, S. Brahma et al., “Scaling instruction-\\nfinetuned language models,” arXiv preprint arXiv:2210.11416 , 2022.\\n[75] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,\\nS. Shakeri, E. Taropa, P. Bailey, Z. Chen et al. , “Palm 2 technical\\nreport,” arXiv preprint arXiv:2305.10403 , 2023.\\n[76] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung,\\nN. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl et al., “Large language\\nmodels encode clinical knowledge,” arXiv preprint arXiv:2212.13138,\\n2022.\\n[77] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou,\\nK. Clark, S. Pfohl, H. Cole-Lewis, D. Neal et al. , “Towards expert-\\nlevel medical question answering with large language models,” arXiv\\npreprint arXiv:2305.09617, 2023.\\n[78] J. Wei, M. Bosma, V . Y . Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,\\nA. M. Dai, and Q. V . Le, “Finetuned language models are zero-shot\\nlearners,” arXiv preprint arXiv:2109.01652 , 2021.\\n[79] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song,\\nJ. Aslanides, S. Henderson, R. Ring, S. Younget al., “Scaling language\\nmodels: Methods, analysis & insights from training gopher,” arXiv\\npreprint arXiv:2112.11446, 2021.\\n[80] V . Sanh, A. Webson, C. Raffel, S. H. Bach, L. Sutawika, Z. Alyafeai,\\nA. Chaffin, A. Stiegler, T. L. Scao, A. Raja et al. , “Multi-\\ntask prompted training enables zero-shot task generalization,” arXiv\\npreprint arXiv:2110.08207, 2021.\\n[81] Y . Sun, S. Wang, S. Feng, S. Ding, C. Pang, J. Shang, J. Liu, X. Chen,\\nY . Zhao, Y . Luet al., “Ernie 3.0: Large-scale knowledge enhanced pre-\\ntraining for language understanding and generation,” arXiv preprint\\narXiv:2107.02137, 2021.\\n[82] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Mil-\\nlican, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clark\\net al. , “Improving language models by retrieving from trillions of\\ntokens,” in International conference on machine learning . PMLR,\\n2022, pp. 2206–2240.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 38, 'page_label': '39'}, page_content='[83] O. Lieber, O. Sharir, B. Lenz, and Y . Shoham, “Jurassic-1: Technical\\ndetails and evaluation,” White Paper. AI21 Labs, vol. 1, p. 9, 2021.\\n[84] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,\\nY . Zhou, A. W. Yu, O. Firat et al. , “Glam: Efficient scaling of\\nlanguage models with mixture-of-experts,” in International Conference\\non Machine Learning . PMLR, 2022, pp. 5547–5569.\\n[85] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-\\nT. Cheng, A. Jin, T. Bos, L. Baker, Y . Du et al. , “Lamda: Language\\nmodels for dialog applications,” arXiv preprint arXiv:2201.08239 ,\\n2022.\\n[86] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\\nC. Dewan, M. Diab, X. Li, X. V . Lin et al. , “Opt: Open pre-trained\\ntransformer language models,” arXiv preprint arXiv:2205.01068, 2022.\\n[87] R. Taylor, M. Kardas, G. Cucurull, T. Scialom, A. Hartshorn, E. Sar-\\navia, A. Poulton, V . Kerkez, and R. Stojnic, “Galactica: A large\\nlanguage model for science,” arXiv preprint arXiv:2211.09085 , 2022.\\n[88] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y . Zhou,\\nS. Savarese, and C. Xiong, “Codegen: An open large language\\nmodel for code with multi-turn program synthesis,” arXiv preprint\\narXiv:2203.13474, 2022.\\n[89] S. Soltan, S. Ananthakrishnan, J. FitzGerald, R. Gupta, W. Hamza,\\nH. Khan, C. Peris, S. Rawls, A. Rosenbaum, A. Rumshisky et al. ,\\n“Alexatm 20b: Few-shot learning using a large-scale multilingual\\nseq2seq model,” arXiv preprint arXiv:2208.01448 , 2022.\\n[90] A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V . Firoiu,\\nT. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker et al. ,\\n“Improving alignment of dialogue agents via targeted human judge-\\nments,” arXiv preprint arXiv:2209.14375 , 2022.\\n[91] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski,\\nV . Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo et al. ,\\n“Solving quantitative reasoning problems with language models,”\\nAdvances in Neural Information Processing Systems , vol. 35, pp.\\n3843–3857, 2022.\\n[92] Y . Tay, M. Dehghani, V . Q. Tran, X. Garcia, D. Bahri, T. Schuster,\\nH. S. Zheng, N. Houlsby, and D. Metzler, “Unifying language learning\\nparadigms,” arXiv preprint arXiv:2205.05131 , 2022.\\n[93] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow,\\nR. Castagn´e, A. S. Luccioni, F. Yvon, M. Gall´e et al., “Bloom: A 176b-\\nparameter open-access multilingual language model,” arXiv preprint\\narXiv:2211.05100, 2022.\\n[94] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu,\\nW. Zheng, X. Xia et al. , “Glm-130b: An open bilingual pre-trained\\nmodel,” arXiv preprint arXiv:2210.02414 , 2022.\\n[95] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O’Brien,\\nE. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff et al.,\\n“Pythia: A suite for analyzing large language models across train-\\ning and scaling,” in International Conference on Machine Learning .\\nPMLR, 2023, pp. 2397–2430.\\n[96] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and\\nA. Awadallah, “Orca: Progressive learning from complex explanation\\ntraces of gpt-4,” arXiv preprint arXiv:2306.02707 , 2023.\\n[97] R. Li, L. B. Allal, Y . Zi, N. Muennighoff, D. Kocetkov, C. Mou,\\nM. Marone, C. Akiki, J. Li, J. Chim et al., “Starcoder: may the source\\nbe with you!” arXiv preprint arXiv:2305.06161 , 2023.\\n[98] S. Huang, L. Dong, W. Wang, Y . Hao, S. Singhal, S. Ma, T. Lv,\\nL. Cui, O. K. Mohammed, Q. Liu et al. , “Language is not all you\\nneed: Aligning perception with language models,” arXiv preprint\\narXiv:2302.14045, 2023.\\n[99] G. Team, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Yu, R. Soricut,\\nJ. Schalkwyk, A. M. Dai, A. Hauth et al., “Gemini: a family of highly\\ncapable multimodal models,” arXiv preprint arXiv:2312.11805 , 2023.\\n[100] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\\nJ. Tompson, I. Mordatch, Y . Chebotar et al. , “Inner monologue:\\nEmbodied reasoning through planning with language models,” arXiv\\npreprint arXiv:2207.05608, 2022.\\n[101] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari,\\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti\\net al. , “Using deepspeed and megatron to train megatron-turing\\nnlg 530b, a large-scale generative language model,” arXiv preprint\\narXiv:2201.11990, 2022.\\n[102] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\\ndocument transformer,” arXiv preprint arXiv:2004.05150 , 2020.\\n[103] S. Iyer, X. V . Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shus-\\nter, T. Wang, Q. Liu, P. S. Koura et al. , “Opt-iml: Scaling language\\nmodel instruction meta learning through the lens of generalization,”\\narXiv preprint arXiv:2212.12017 , 2022.\\n[104] Y . Hao, H. Song, L. Dong, S. Huang, Z. Chi, W. Wang, S. Ma,\\nand F. Wei, “Language models are general-purpose interfaces,” arXiv\\npreprint arXiv:2206.06336, 2022.\\n[105] Z. Sun, Y . Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y . Yang,\\nand C. Gan, “Principle-driven self-alignment of language mod-\\nels from scratch with minimal human supervision,” arXiv preprint\\narXiv:2305.03047, 2023.\\n[106] W. E. team, “Palmyra-base Parameter Autoregressive Language\\nModel,” https://dev.writer.com, 2023.\\n[107] ——, “Camel-5b instructgpt,” https://dev.writer.com, 2023.\\n[108] Yandex. Yalm. [Online]. Available: https://github.com/yandex/\\nYaLM-100B\\n[109] M. Team et al., “Introducing mpt-7b: a new standard for open-source,\\ncommercially usable llms,” 2023.\\n[110] A. Mitra, L. D. Corro, S. Mahajan, A. Codas, C. Simoes, S. Agarwal,\\nX. Chen, A. Razdaibiedina, E. Jones, K. Aggarwal, H. Palangi,\\nG. Zheng, C. Rosset, H. Khanpour, and A. Awadallah, “Orca 2:\\nTeaching small language models how to reason,” 2023.\\n[111] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y . Yang, J. Callan, and\\nG. Neubig, “Pal: Program-aided language models,” in International\\nConference on Machine Learning . PMLR, 2023, pp. 10 764–10 799.\\n[112] Anthropic. claude. [Online]. Available: https://www.anthropic.com/\\nnews/introducing-claude\\n[113] E. Nijkamp, H. Hayashi, C. Xiong, S. Savarese, and Y . Zhou,\\n“Codegen2: Lessons for training llms on programming and natural\\nlanguages,” arXiv preprint arXiv:2305.02309 , 2023.\\n[114] L. Tunstall, E. Beeching, N. Lambert, N. Rajani, K. Rasul, Y . Belkada,\\nS. Huang, L. von Werra, C. Fourrier, N. Habib et al., “Zephyr: Direct\\ndistillation of lm alignment,” arXiv preprint arXiv:2310.16944 , 2023.\\n[115] X. team. Grok. [Online]. Available: https://grok.x.ai/\\n[116] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou,\\nand J. Zhou, “Qwen-vl: A frontier large vision-language model with\\nversatile abilities,” arXiv preprint arXiv:2308.12966 , 2023.\\n[117] mixtral. mixtral. [Online]. Available: https://mistral.ai/news/\\nmixtral-of-experts/\\n[118] D. Wang, N. Raman, M. Sibue, Z. Ma, P. Babkin, S. Kaur, Y . Pei,\\nA. Nourbakhsh, and X. Liu, “Docllm: A layout-aware generative\\nlanguage model for multimodal document understanding,” 2023.\\n[119] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi,\\nY . Wu, Y . K. Li, F. Luo, Y . Xiong, and W. Liang, “Deepseek-coder:\\nWhen the large language model meets programming – the rise of code\\nintelligence,” 2024.\\n[120] F. Wan, X. Huang, D. Cai, X. Quan, W. Bi, and S. Shi, “Knowledge\\nfusion of large language models,” 2024.\\n[121] P. Zhang, G. Zeng, T. Wang, and W. Lu, “Tinyllama: An open-source\\nsmall language model,” 2024.\\n[122] C. Wu, Y . Gan, Y . Ge, Z. Lu, J. Wang, Y . Feng, P. Luo, and Y . Shan,\\n“Llama pro: Progressive llama with block expansion,” 2024.\\n[123] X. Amatriain, A. Sankar, J. Bing, P. K. Bodigutla, T. J. Hazen, and\\nM. Kazi, “Transformer models: an introduction and catalog,” 2023.\\n[124] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\\nH. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The refined-\\nweb dataset for falcon llm: outperforming curated corpora with web\\ndata, and web data only,” arXiv preprint arXiv:2306.01116 , 2023.\\n[125] D. Hernandez, T. Brown, T. Conerly, N. DasSarma, D. Drain, S. El-\\nShowk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Hume et al. ,\\n“Scaling laws and interpretability of learning from repeated data,”\\narXiv preprint arXiv:2205.10487 , 2022.\\n[126] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative\\nposition representations,” arXiv preprint arXiv:1803.02155 , 2018.\\n[127] J. Su, Y . Lu, S. Pan, B. Wen, and Y . Liu, “Roformer: En-'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 39, 'page_label': '40'}, page_content='hanced transformer with rotary position embedding,” arXiv preprint\\narXiv:2104.09864, 2021.\\n[128] O. Press, N. A. Smith, and M. Lewis, “Train short, test long: Attention\\nwith linear biases enables input length extrapolation,” arXiv preprint\\narXiv:2108.12409, 2021.\\n[129] G. Ke, D. He, and T.-Y . Liu, “Rethinking positional encoding in\\nlanguage pre-training,” arXiv preprint arXiv:2006.15595 , 2020.\\n[130] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\\nand J. Dean, “Outrageously large neural networks: The sparsely-gated\\nmixture-of-experts layer,” arXiv preprint arXiv:1701.06538 , 2017.\\n[131] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling\\nto trillion parameter models with simple and efficient sparsity,” The\\nJournal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270,\\n2022.\\n[132] R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson,\\n“Parameter-efficient multi-task fine-tuning for transformers via shared\\nhypernetworks,” 2021.\\n[133] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu,\\nT. Zhang, F. Wu, and G. Wang, “Instruction tuning for large language\\nmodels: A survey,” 2023.\\n[134] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi, “Cross-task\\ngeneralization via natural language crowdsourcing instructions,” arXiv\\npreprint arXiv:2104.08773, 2021.\\n[135] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi,\\nand H. Hajishirzi, “Self-instruct: Aligning language model with self\\ngenerated instructions,” arXiv preprint arXiv:2212.10560 , 2022.\\n[136] K. Ethayarajh, W. Xu, D. Jurafsky, and D. Kiela. Kto. [Online].\\nAvailable: https://github.com/ContextualAI/HALOs/blob/main/assets/\\nreport.pdf\\n[137] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and\\nD. Amodei, “Deep reinforcement learning from human preferences,”\\nAdvances in neural information processing systems , vol. 30, 2017.\\n[138] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V . Car-\\nbune, and A. Rastogi, “Rlaif: Scaling reinforcement learning from\\nhuman feedback with ai feedback,” arXiv preprint arXiv:2309.00267 ,\\n2023.\\n[139] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and\\nC. Finn, “Direct preference optimization: Your language model is\\nsecretly a reward model,” arXiv preprint arXiv:2305.18290 , 2023.\\n[140] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He, “Zero: Memory\\noptimizations toward training trillion parameter models,” in SC20: In-\\nternational Conference for High Performance Computing, Networking,\\nStorage and Analysis . IEEE, 2020, pp. 1–16.\\n[141] B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao,\\nX. Cheng, M. Chung, M. Grella, K. K. GV et al., “Rwkv: Reinventing\\nrnns for the transformer era,” arXiv preprint arXiv:2305.13048, 2023.\\n[142] E. J. Hu, Y . Shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang,\\nand W. Chen, “Lora: Low-rank adaptation of large language models,”\\narXiv preprint arXiv:2106.09685 , 2021.\\n[143] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\\nneural network,” arXiv preprint arXiv:1503.02531 , 2015.\\n[144] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation:\\nA survey,” International Journal of Computer Vision , vol. 129, pp.\\n1789–1819, 2021.\\n[145] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . J.\\nBang, A. Madotto, and P. Fung, “Survey of hallucination in natural\\nlanguage generation,” ACM Comput. Surv., vol. 55, no. 12, mar 2023.\\n[Online]. Available: https://doi.org/10.1145/3571730\\n[146] N. McKenna, T. Li, L. Cheng, M. J. Hosseini, M. Johnson, and\\nM. Steedman, “Sources of hallucination by large language models on\\ninference tasks,” 2023.\\n[147] C.-Y . Lin, “ROUGE: A package for automatic evaluation of\\nsummaries,” in Text Summarization Branches Out. Barcelona, Spain:\\nAssociation for Computational Linguistics, Jul. 2004, pp. 74–81.\\n[Online]. Available: https://aclanthology.org/W04-1013\\n[148] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for\\nautomatic evaluation of machine translation,” in Proceedings of the\\n40th Annual Meeting of the Association for Computational Linguistics,\\nP. Isabelle, E. Charniak, and D. Lin, Eds. Philadelphia, Pennsylvania,\\nUSA: Association for Computational Linguistics, Jul. 2002, pp. 311–\\n318. [Online]. Available: https://aclanthology.org/P02-1040\\n[149] B. Dhingra, M. Faruqui, A. Parikh, M.-W. Chang, D. Das, and\\nW. Cohen, “Handling divergent reference texts when evaluating\\ntable-to-text generation,” in Proceedings of the 57th Annual Meeting\\nof the Association for Computational Linguistics , A. Korhonen,\\nD. Traum, and L. M `arquez, Eds. Florence, Italy: Association\\nfor Computational Linguistics, Jul. 2019, pp. 4884–4895. [Online].\\nAvailable: https://aclanthology.org/P19-1483\\n[150] Z. Wang, X. Wang, B. An, D. Yu, and C. Chen, “Towards faithful\\nneural table-to-text generation with content-matching constraints,”\\nin Proceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics , D. Jurafsky, J. Chai, N. Schluter,\\nand J. Tetreault, Eds. Online: Association for Computational\\nLinguistics, Jul. 2020, pp. 1072–1086. [Online]. Available: https:\\n//aclanthology.org/2020.acl-main.101\\n[151] H. Song, W.-N. Zhang, J. Hu, and T. Liu, “Generating persona consis-\\ntent dialogues by exploiting natural language inference,” Proceedings\\nof the AAAI Conference on Artificial Intelligence , vol. 34, no. 05, pp.\\n8878–8885, Apr. 2020.\\n[152] O. Honovich, L. Choshen, R. Aharoni, E. Neeman, I. Szpektor,\\nand O. Abend, “ q2: Evaluating factual consistency in knowledge-\\ngrounded dialogues via question generation and question answering,”\\nin Proceedings of the 2021 Conference on Empirical Methods in\\nNatural Language Processing , M.-F. Moens, X. Huang, L. Specia,\\nand S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic:\\nAssociation for Computational Linguistics, Nov. 2021, pp. 7856–7870.\\n[Online]. Available: https://aclanthology.org/2021.emnlp-main.619\\n[153] N. Dziri, H. Rashkin, T. Linzen, and D. Reitter, “Evaluating attribution\\nin dialogue systems: The BEGIN benchmark,” Transactions of the\\nAssociation for Computational Linguistics , vol. 10, pp. 1066–1083,\\n2022. [Online]. Available: https://aclanthology.org/2022.tacl-1.62\\n[154] S. Santhanam, B. Hedayatnia, S. Gella, A. Padmakumar, S. Kim,\\nY . Liu, and D. Z. Hakkani-T¨ur, “Rome was built in 1776: A case study\\non factual correctness in knowledge-grounded response generation,”\\nArXiv, vol. abs/2110.05456, 2021.\\n[155] S. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer,\\nL. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomic\\nevaluation of factual precision in long form text generation,” 2023.\\n[156] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner,\\nV . Chaudhary, and M. Young, “Machine learning: The high interest\\ncredit card of technical debt,” in SE4ML: Software Engineering for\\nMachine Learning (NIPS 2014 Workshop) , 2014.\\n[157] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought\\nprompting in large language models,” 2022.\\n[158] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and\\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving with\\nlarge language models,” 2023.\\n[159] P. Manakul, A. Liusie, and M. J. F. Gales, “Selfcheckgpt: Zero-\\nresource black-box hallucination detection for generative large lan-\\nguage models,” 2023.\\n[160] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan,\\nand S. Yao, “Reflexion: Language agents with verbal reinforcement\\nlearning,” 2023.\\n[161] S. J. Zhang, S. Florin, A. N. Lee, E. Niknafs, A. Marginean, A. Wang,\\nK. Tyser, Z. Chin, Y . Hicke, N. Singh, M. Udell, Y . Kim, T. Buonassisi,\\nA. Solar-Lezama, and I. Drori, “Exploring the mit mathematics and\\neecs curriculum using large language models,” 2023.\\n[162] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J.\\nCai, “Promptchainer: Chaining large language model prompts through\\nvisual programming,” 2022.\\n[163] Y . Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and\\nJ. Ba, “Large language models are human-level prompt engineers,”\\n2023.\\n[164] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin,\\nN. Goyal, H. K¨uttler, M. Lewis, W. Yih, T. Rockt¨aschel, S. Riedel, and\\nD. Kiela, “Retrieval-augmented generation for knowledge-intensive\\nNLP tasks,” CoRR, vol. abs/2005.11401, 2020. [Online]. Available:\\nhttps://arxiv.org/abs/2005.11401\\n[165] Y . Gao, Y . Xiong, X. Gao, K. Jia, J. Pan, Y . Bi, Y . Dai, J. Sun, and'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 40, 'page_label': '41'}, page_content='H. Wang, “Retrieval-augmented generation for large language models:\\nA survey,” arXiv preprint arXiv:2312.10997 , 2023.\\n[166] A. W. Services. (Year of publication, e.g., 2023) Question answering\\nusing retrieval augmented generation with foundation models in\\namazon sagemaker jumpstart. Accessed: Date of access, e.g.,\\nDecember 5, 2023. [Online]. Available: https://shorturl.at/dSV47\\n[167] S. Pan, L. Luo, Y . Wang, C. Chen, J. Wang, and X. Wu, “Unifying large\\nlanguage models and knowledge graphs: A roadmap,” arXiv preprint\\narXiv:2306.08302, 2023.\\n[168] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\\n2023.\\n[169] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” 2023.\\n[170] B. Paranjape, S. Lundberg, S. Singh, H. Hajishirzi, L. Zettlemoyer,\\nand M. T. Ribeiro, “Art: Automatic multi-step reasoning and tool-use\\nfor large language models,” 2023.\\n[171] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, and Y . Zhuang, “Hugginggpt:\\nSolving ai tasks with chatgpt and its friends in huggingface,” arXiv\\npreprint arXiv:2303.17580, 2023.\\n[172] Z. Xi, W. Chen, X. Guo, W. He, Y . Ding, B. Hong, M. Zhang, J. Wang,\\nS. Jin, E. Zhou et al., “The rise and potential of large language model\\nbased agents: A survey,” arXiv preprint arXiv:2309.07864 , 2023.\\n[173] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\\nJ. Tang, X. Chen, Y . Lin et al. , “A survey on large language model\\nbased autonomous agents,” arXiv preprint arXiv:2308.11432 , 2023.\\n[174] Z. Durante, Q. Huang, N. Wake, R. Gong, J. S. Park, B. Sarkar,\\nR. Taori, Y . Noda, D. Terzopoulos, Y . Choi, K. Ikeuchi, H. V o, L. Fei-\\nFei, and J. Gao, “Agent ai: Surveying the horizons of multimodal\\ninteraction,” arXiv preprint arXiv:2401.03568 , 2024.\\n[175] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y . Liu, and D. Xu, “Rewoo:\\nDecoupling reasoning from observations for efficient augmented lan-\\nguage models,” 2023.\\n[176] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\\n“React: Synergizing reasoning and acting in language models,” 2023.\\n[177] V . Nair, E. Schumacher, G. Tso, and A. Kannan, “Dera: Enhanc-\\ning large language model completions with dialog-enabled resolving\\nagents,” 2023.\\n[178] Y . Chang, X. Wang, J. Wang, Y . Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\\nC. Wang, Y . Wang, W. Ye, Y . Zhang, Y . Chang, P. S. Yu, Q. Yang,\\nand X. Xie, “A survey on evaluation of large language models,” 2023.\\n[179] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova,\\nL. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit,\\nQ. Le, and S. Petrov, “Natural questions: A benchmark for\\nquestion answering research,” Transactions of the Association for\\nComputational Linguistics , vol. 7, pp. 452–466, 2019. [Online].\\nAvailable: https://aclanthology.org/Q19-1026\\n[180] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\n2021.\\n[181] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\\nE. Jiang, C. Cai, M. Terry, Q. Le et al., “Program synthesis with large\\nlanguage models,” arXiv preprint arXiv:2108.07732 , 2021.\\n[182] E. Choi, H. He, M. Iyyer, M. Yatskar, W.-t. Yih, Y . Choi, P. Liang,\\nand L. Zettlemoyer, “QuAC: Question answering in context,” in\\nProceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing , E. Riloff, D. Chiang, J. Hockenmaier, and\\nJ. Tsujii, Eds. Brussels, Belgium: Association for Computational\\nLinguistics, Oct.-Nov. 2018, pp. 2174–2184. [Online]. Available:\\nhttps://aclanthology.org/D18-1241\\n[183] D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo,\\nC. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt, “Measuring\\ncoding challenge competence with apps,” NeurIPS, 2021.\\n[184] V . Zhong, C. Xiong, and R. Socher, “Seq2sql: Generating structured\\nqueries from natural language using reinforcement learning,” arXiv\\npreprint arXiv:1709.00103, 2017.\\n[185] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA:\\nA large scale distantly supervised challenge dataset for reading\\ncomprehension,” in Proceedings of the 55th Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\nR. Barzilay and M.-Y . Kan, Eds. Vancouver, Canada: Association\\nfor Computational Linguistics, Jul. 2017, pp. 1601–1611. [Online].\\nAvailable: https://aclanthology.org/P17-1147\\n[186] G. Lai, Q. Xie, H. Liu, Y . Yang, and E. Hovy, “RACE: Large-scale\\nReAding comprehension dataset from examinations,” in Proceedings\\nof the 2017 Conference on Empirical Methods in Natural Language\\nProcessing, M. Palmer, R. Hwa, and S. Riedel, Eds. Copenhagen,\\nDenmark: Association for Computational Linguistics, Sep. 2017, pp.\\n785–794. [Online]. Available: https://aclanthology.org/D17-1082\\n[187] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “SQuAD: 100,000+\\nquestions for machine comprehension of text,” in Proceedings of\\nthe 2016 Conference on Empirical Methods in Natural Language\\nProcessing, J. Su, K. Duh, and X. Carreras, Eds. Austin, Texas:\\nAssociation for Computational Linguistics, Nov. 2016, pp. 2383–2392.\\n[Online]. Available: https://aclanthology.org/D16-1264\\n[188] C. Clark, K. Lee, M. Chang, T. Kwiatkowski, M. Collins, and\\nK. Toutanova, “Boolq: Exploring the surprising difficulty of natural\\nyes/no questions,” CoRR, vol. abs/1905.10044, 2019. [Online].\\nAvailable: http://arxiv.org/abs/1905.10044\\n[189] D. Khashabi, S. Chaturvedi, M. Roth, S. Upadhyay, and D. Roth,\\n“Looking beyond the surface:a challenge set for reading compre-\\nhension over multiple sentences,” in Proceedings of North American\\nChapter of the Association for Computational Linguistics (NAACL) ,\\n2018.\\n[190] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and\\nJ. Schulman, “Training verifiers to solve math word problems,”\\nCoRR, vol. abs/2110.14168, 2021. [Online]. Available: https:\\n//arxiv.org/abs/2110.14168\\n[191] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang,\\nD. Song, and J. Steinhardt, “Measuring mathematical problem solving\\nwith the MATH dataset,” CoRR, vol. abs/2103.03874, 2021. [Online].\\nAvailable: https://arxiv.org/abs/2103.03874\\n[192] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hellaswag:\\nCan a machine really finish your sentence?” 2019.\\n[193] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try\\narc, the AI2 reasoning challenge,” CoRR, vol. abs/1803.05457, 2018.\\n[Online]. Available: http://arxiv.org/abs/1803.05457\\n[194] Y . Bisk, R. Zellers, R. L. Bras, J. Gao, and Y . Choi, “PIQA:\\nreasoning about physical commonsense in natural language,” CoRR,\\nvol. abs/1911.11641, 2019. [Online]. Available: http://arxiv.org/abs/\\n1911.11641\\n[195] M. Sap, H. Rashkin, D. Chen, R. L. Bras, and Y . Choi, “Socialiqa:\\nCommonsense reasoning about social interactions,” CoRR, vol.\\nabs/1904.09728, 2019. [Online]. Available: http://arxiv.org/abs/1904.\\n09728\\n[196] T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal, “Can a suit of\\narmor conduct electricity? A new dataset for open book question\\nanswering,” CoRR, vol. abs/1809.02789, 2018. [Online]. Available:\\nhttp://arxiv.org/abs/1809.02789\\n[197] S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models\\nmimic human falsehoods,” arXiv preprint arXiv:2109.07958 , 2021.\\n[198] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. W. Cohen, R. Salakhutdinov,\\nand C. D. Manning, “Hotpotqa: A dataset for diverse, explainable\\nmulti-hop question answering,” CoRR, vol. abs/1809.09600, 2018.\\n[Online]. Available: http://arxiv.org/abs/1809.09600\\n[199] Y . Zhuang, Y . Yu, K. Wang, H. Sun, and C. Zhang, “Toolqa: A\\ndataset for llm question answering with external tools,” arXiv preprint\\narXiv:2306.13304, 2023.\\n[200] D. Chen, J. Bolton, and C. D. Manning, “A thorough examination\\nof the cnn/daily mail reading comprehension task,” in Association for\\nComputational Linguistics (ACL) , 2016.\\n[201] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al., “Abstractive text\\nsummarization using sequence-to-sequence rnns and beyond,” arXiv\\npreprint arXiv:1602.06023, 2016.\\n[202] Y . Bai and D. Z. Wang, “More than reading comprehension: A survey'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 41, 'page_label': '42'}, page_content='on datasets and metrics of textual question answering,” arXiv preprint\\narXiv:2109.12264, 2021.\\n[203] H.-Y . Huang, E. Choi, and W.-t. Yih, “Flowqa: Grasping flow in\\nhistory for conversational machine comprehension,” arXiv preprint\\narXiv:1810.06683, 2018.\\n[204] S. Lee, J. Lee, H. Moon, C. Park, J. Seo, S. Eo, S. Koo, and H. Lim, “A\\nsurvey on evaluation metrics for machine translation,” Mathematics,\\nvol. 11, no. 4, p. 1006, 2023.\\n[205] J. Li, X. Cheng, W. X. Zhao, J.-Y . Nie, and J.-R. Wen, “Halueval:\\nA large-scale hallucination evaluation benchmark for large language\\nmodels,” in Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing , 2023, pp. 6449–6464.\\n[206] Simon Mark Hughes, “Hughes hallucination evaluation model\\n(hhem) leaderboard,” 2024, https://huggingface.co/spaces/vectara/\\nHallucination-evaluation-leaderboard, Last accessed on 2024-01-21.\\n[207] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and\\nR. McHardy, “Challenges and applications of large language models,”\\narXiv preprint arXiv:2307.10169 , 2023.\\n[208] S. Gunasekar, Y . Zhang, J. Aneja, C. C. T. Mendes, A. Del Giorno,\\nS. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi et al.,\\n“Textbooks are all you need,” arXiv preprint arXiv:2306.11644, 2023.\\n[209] Y . Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y . T.\\nLee, “Textbooks are all you need ii: phi-1.5 technical report,” arXiv\\npreprint arXiv:2309.05463, 2023.\\n[210] M. Poli, S. Massaroli, E. Nguyen, D. Y . Fu, T. Dao, S. Baccus,\\nY . Bengio, S. Ermon, and C. R ´e, “Hyena hierarchy: Towards larger\\nconvolutional language models,” 2023.\\n[211] M. Poli, J. Wang, S. Massaroli, J. Quesnelle, E. Nguyen, and\\nA. Thomas, “StripedHyena: Moving Beyond Transformers with\\nHybrid Signal Processing Models,” 12 2023. [Online]. Available:\\nhttps://github.com/togethercomputer/stripedhyena\\n[212] D. Y . Fu, S. Arora, J. Grogan, I. Johnson, S. Eyuboglu, A. W. Thomas,\\nB. Spector, M. Poli, A. Rudra, and C. R ´e, “Monarch mixer: A simple\\nsub-quadratic gemm-based architecture,” 2023.\\n[213] G. J. McLachlan, S. X. Lee, and S. I. Rathnayake, “Finite mixture\\nmodels,” Annual review of statistics and its application , vol. 6, pp.\\n355–378, 2019.\\n[214] H. Liu, C. Li, Q. Wu, and Y . J. Lee, “Visual instruction tuning,” arXiv\\npreprint arXiv:2304.08485, 2023.\\n[215] S. Liu, H. Cheng, H. Liu, H. Zhang, F. Li, T. Ren, X. Zou,\\nJ. Yang, H. Su, J. Zhu, L. Zhang, J. Gao, and C. Li, “Llava-plus:\\nLearning to use tools for creating multimodal agents,” arXiv preprint\\narXiv:2311.05437, 2023.\\n[216] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, “Next-gpt: Any-to-any\\nmultimodal llm,” arXiv preprint arXiv:2309.05519 , 2023.\\n[217] N. N. Khasmakhi, M. Asgari-Chenaghlu, N. Asghar, P. Schaer, and\\nD. Z ¨uhlke, “Convgenvismo: Evaluation of conversational generative\\nvision models,” 2023.\\n[218] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman,\\nI. Harper, A. Marginean, S. Sengupta, and E. Wang, “Automated unit\\ntest improvement using large language models at meta,” arXiv preprint\\narXiv:2402.09171, 2024.\\n[219] L. Sun, Y . Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y . Huang,\\nW. Lyu, Y . Zhang, X. Li et al. , “Trustllm: Trustworthiness in large\\nlanguage models,” arXiv preprint arXiv:2401.05561 , 2024.\\n[220] M. Josifoski, L. Klein, M. Peyrard, Y . Li, S. Geng, J. P. Schnitzler,\\nY . Yao, J. Wei, D. Paul, and R. West, “Flows: Building blocks of\\nreasoning and collaborating ai,” arXiv preprint arXiv:2308.01285 ,\\n2023.\\n[221] Microsoft. Deepspeed. [Online]. Available: https://github.com/\\nmicrosoft/DeepSpeed\\n[222] HuggingFace. Transformers. [Online]. Available: https://github.com/\\nhuggingface/transformers\\n[223] Nvidia. Megatron. [Online]. Available: https://github.com/NVIDIA/\\nMegatron-LM\\n[224] BMTrain. Bmtrain. [Online]. Available: https://github.com/OpenBMB/\\nBMTrain\\n[225] EleutherAI. gpt-neox. [Online]. Available: https://github.com/\\nEleutherAI/gpt-neox\\n[226] microsoft. Lora. [Online]. Available: https://github.com/microsoft/\\nLoRA\\n[227] ColossalAI. Colossalai. [Online]. Available: https://github.com/\\nhpcaitech/ColossalAI\\n[228] FastChat. Fastchat. [Online]. Available: https://github.com/lm-sys/\\nFastChat\\n[229] skypilot. skypilot. [Online]. Available: https://github.com/skypilot-org/\\nskypilot\\n[230] vllm. vllm. [Online]. Available: https://github.com/vllm-project/vllm\\n[231] huggingface. text-generation-inference. [Online]. Available: https:\\n//github.com/huggingface/text-generation-inference\\n[232] langchain. langchain. [Online]. Available: https://github.com/\\nlangchain-ai/langchain\\n[233] bentoml. Openllm. [Online]. Available: https://github.com/bentoml/\\nOpenLLM\\n[234] embedchain. embedchain. [Online]. Available: https://github.com/\\nembedchain/embedchain\\n[235] microsoft. autogen. [Online]. Available: https://github.com/microsoft/\\nautogen\\n[236] babyagi. babyagi. [Online]. Available: https://github.com/\\nyoheinakajima/babyagi\\n[237] guidance. guidance. [Online]. Available: https://github.com/\\nguidance-ai/guidance\\n[238] prompttools. prompttools. [Online]. Available: https://github.com/\\nhegelai/prompttools\\n[239] promptfoo. promptfoo. [Online]. Available: https://github.com/\\npromptfoo/promptfoo\\n[240] facebook. faiss. [Online]. Available: https://github.com/\\nfacebookresearch/faiss\\n[241] milvus. milvus. [Online]. Available: https://github.com/milvus-io/\\nmilvus\\n[242] qdrant. qdrant. [Online]. Available: https://github.com/qdrant/qdrant\\n[243] weaviate. weaviate. [Online]. Available: https://github.com/weaviate/\\nweaviate\\n[244] llama index. llama-index. [Online]. Available: https://github.com/\\nrun-llama/llama index\\nAPPENDIX\\n1. Open Source Toolkits For LLM Development and\\nDeployment\\nThere are various frameworks and libraries developed for\\nLLM training, evaluation, and deployment, and covering every\\nsingle framework is out of this paper’s scope. But we try to\\nprovide a brief introduction of some of the most popular ones,\\ngrouped into different categories.\\nA. LLM Training/Inference Frameworks\\nSome of the popular frameworks which are useful for LLM\\ntraining includes (note that some of them can be used beyond\\nLLM training too):\\nDeepSpeed [221] is a deep learning optimization library\\nthat makes distributed training and inference easy, efficient,\\nand effective. DeepSpeed enables world’s most powerful lan-\\nguage models like MT-530B and BLOOM. It is an easy-\\nto-use deep learning optimization software suite that powers\\nunprecedented scale and speed for both training and inference.\\nWith DeepSpeed you can:\\nTransformers [222] is library by HuggingFace which\\nprovides thousands of pretrained models to perform tasks on\\ndifferent modalities such as text, vision, and audio. Using\\npretrained models one can reduce compute costs, carbon'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 42, 'page_label': '43'}, page_content='footprint, and save the time and resources required to train\\na model from scratch.\\nMegatron-LM [223] is a large, powerful transformer\\ndeveloped by the Applied Deep Learning Research team\\nat NVIDIA. It contains efficient, model-parallel (tensor, se-\\nquence, and pipeline), and multi-node pre-training of trans-\\nformer based models such as GPT, BERT, and T5 using mixed\\nprecision.\\nBMTrain [224] is an efficient large model training toolkit\\nthat can be used to train large models with tens of billions of\\nparameters. It can train models in a distributed manner while\\nkeeping the code as simple as stand-alone training.\\nGPT-NeoX [225] leverages many of the same features and\\ntechnologies as the popular Megatron-DeepSpeed library but\\nwith substantially increased usability and novel optimizations.\\nLoRA [226] library provides the support for Low-Rank\\nAdaptation of Large Language Models. It reduces the number\\nof trainable parameters by learning pairs of rank-decompostion\\nmatrices while freezing the original weights. This vastly\\nreduces the storage requirement for large language models\\nadapted to specific tasks and enables efficient task-switching\\nduring deployment all without introducing inference latency.\\nLoRA also outperforms several other adaptation methods in-\\ncluding adapter, prefix-tuning, and fine-tuning.\\nColossalAI library [227] provides a collection of parallel\\ncomponents. It aims to support developers to write their\\ndistributed deep learning models just like how they write their\\nmodel on their laptop. They provide user-friendly tools to\\nkickstart distributed training and inference in a few lines. In\\nterms of Parallelism strategies, they support: Data Parallelism,\\nPipeline Parallelism, Sequence Parallelism, Zero Redundancy\\nOptimizer (ZeRO) [140], and Auto-Parallelism.\\nB. Deployment Tools\\nWe provide an overview of some of the most popular LLM\\ndeployment tools here.\\nFastChat [228] is an open platform for training, serv-\\ning, and evaluating large language model based chatbots.\\nFastChat’s core features include: The training and evaluation\\ncode for state-of-the-art models (e.g., Vicuna, MT-Bench), and\\na distributed multi-model serving system with web UI and\\nOpenAI-compatible RESTful APIs.\\nSkypilot [229] is a framework for running LLMs, AI,\\nand batch jobs on any cloud, offering maximum cost savings,\\nhighest GPU availability, and managed execution.\\nvLLM [230] is a fast and easy-to-use library for LLM in-\\nference and serving. vLLM seamlessly supports many Hugging\\nFace models, including the following architectures: Aquila,\\nBaichuan, BLOOM, ChatGLM, DeciLM, Falcon, GPT Big-\\nCode, LLaMA, LLaMA 2, Mistral, Mixtral, MPT, OPT, Qwen,\\nYi, and many more.\\ntext-generation-inference [231] is a toolkit for deploying\\nand serving Large Language Models (LLMs). TGI enables\\nhigh-performance text generation for the most popular open-\\nsource LLMs, including Llama, Falcon, StarCoder, BLOOM,\\nGPT-NeoX, and more.\\nLangChain [232] is a framework for developing applica-\\ntions powered by language models. It enables applications that:\\n• Are context-aware: connect a language model to\\nsources of context (prompt instructions, few shot ex-\\namples, content to ground its response in, etc.)\\n• Reason: rely on a language model to reason (about\\nhow to answer based on provided context, what ac-\\ntions to take, etc.)\\nOpenLLM [233] is an open-source platform designed to\\nfacilitate the deployment and operation of large language mod-\\nels (LLMs) in real-world applications. With OpenLLM, you\\ncan run inference on any open-source LLM, deploy them on\\nthe cloud or on-premises, and build powerful AI applications.\\nEmbedchain [234] is an Open Source RAG Framework\\nthat makes it easy to create and deploy AI apps. Embedchain\\nstreamlines the creation of RAG applications, offering a seam-\\nless process for managing various types of unstructured data.\\nIt efficiently segments data into manageable chunks, generates\\nrelevant embeddings, and stores them in a vector database for\\noptimized retrieval.\\nAutogen [235] is a framework that enables the devel-\\nopment of LLM applications using multiple agents that can\\nconverse with each other to solve tasks. AutoGen agents\\nare customizable, conversable, and seamlessly allow human\\nparticipation. They can operate in various modes that employ\\ncombinations of LLMs, human inputs, and tools.\\nBabyAGI [236] is an autonomous Artificial Intelligence\\nagent, that is designed to generate and execute tasks based on\\ngiven objectives. It harnesses cutting-edge technologies from\\nOpenAI, Pinecone, LangChain, and Chroma to automate tasks\\nand achieve specific goals. In this blog post, we will dive\\ninto the unique features of BabyAGI and explore how it can\\nstreamline task automation.\\nC. Prompting Libraries\\nGuidance [237] is a programming paradigm that offers\\nsuperior control and efficiency compared to conventional\\nprompting and chaining. It allows users to constrain generation\\n(e.g. with regex and CFGs) as well as to interleave control\\n(conditional, loops) and generation seamlessly.\\nPromptTools [238] offers a set of open-source, self-\\nhostable tools for experimenting with, testing, and evaluating\\nLLMs, vector databases, and prompts. The core idea is to\\nenable developers to evaluate using familiar interfaces like\\ncode, notebooks, and a local playground.\\nPromptBench [?] is a Pytorch-based Python package for\\nEvaluation of Large Language Models (LLMs). It provides\\nuser-friendly APIs for researchers to conduct evaluation on\\nLLMs.\\nPromptfoo [239] is a tool for testing and evaluating LLM\\noutput quality. It systematically test prompts, models, and\\nRAGs with predefined test cases.'), Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-03-25T01:00:06+00:00', 'author': '', 'keywords': '', 'moddate': '2025-03-25T01:00:06+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Large Language Models- A Survey.pdf', 'total_pages': 44, 'page': 43, 'page_label': '44'}, page_content='D. VectorDB\\nFaiss [240] is a library developed by Facebook AI Re-\\nsearch that provides efficient similarity search and clustering\\nof dense vectors. It is designed for use with large-scale,\\nhigh-dimensional data and supports several index types and\\nalgorithms for various use cases.\\nMilvus [241] is an open-source vector database built to\\npower embedding similarity search and AI applications. Mil-\\nvus makes unstructured data search more accessible, and pro-\\nvides a consistent user experience regardless of the deployment\\nenvironment.\\nQdrant [242] is a vector similarity search engine and\\nvector database. It provides a production-ready service with a\\nconvenient API to store, search, and manage points—vectors\\nwith an additional payload Qdrant is tailored to extended\\nfiltering support. environment.\\nWeaviate [243] is an open-source, GraphQL-based vec-\\ntor search engine that enables similarity search on high-\\ndimensional data. While it is open-source, the commercial ver-\\nsion offers additional features, support, and managed services.\\nSome of the other popular options includes LlamaIndex\\n[244] and Pinecone.')]\n",
      "\n",
      "type:<class 'list'> len:111\n",
      "page_content='Published in Transactions on Machine Learning Research (May/2024)\n",
      "Efficient Large Language Models: A Survey\n",
      "Zhongwei Wan∗ wan.512@osu.edu\n",
      "Xin Wang∗ wang.15980@osu.edu\n",
      "Che Liu† che.liu21@imperial.ac.uk\n",
      "Samiul Alam∗ alam.140@osu.edu\n",
      "Yu Zheng‡ zhengy30@msu.edu\n",
      "Jiachen Liu§ amberljc@umich.edu\n",
      "Zhongnan Qu¶‡‡ znqu@amazon.com\n",
      "Shen Yan∥ shenyan@google.com\n",
      "Yi Zhu†† yi@boson.ai\n",
      "Quanlu Zhang∗∗ quzha@microsoft.com\n",
      "Mosharaf Chowdhury§ mosharaf@umich.edu\n",
      "Mi Zhang∗ mizhang.1@osu.edu\n",
      "∗The Ohio State University †Imperial College London ‡Michigan State University §University of\n",
      "Michigan ¶Amazon AWS AI ∥Google Research ∗∗Microsoft Research Asia ††Boson AI\n",
      "Reviewed on OpenReview:https://openreview.net/forum?id=bsCCJHbO8A\n",
      "Abstract\n",
      "Large Language Models (LLMs) have demonstrated remarkable capabilities in important\n",
      "tasks such as natural language understanding and language generation, and thus have the\n",
      "potential to make a substantial impact on our society. Such capabilities, however, come with\n",
      "the considerable resources they demand, highlighting the strong need to develop effective\n",
      "techniques for addressing their efficiency challenges. In this survey, we provide a systematic\n",
      "and comprehensive review of efficient LLMs research. We organize the literature in a taxon-\n",
      "omy consisting of three main categories, covering distinct yet interconnected efficient LLMs\n",
      "topics from model-centric, data-centric, and framework-centric perspective, respectively. We\n",
      "have also created a GitHub repository where we organize the papers featured in this survey\n",
      "at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain\n",
      "the repository and incorporate new research as it emerges. We hope our survey can serve as\n",
      "a valuable resource to help researchers and practitioners gain a systematic understanding of\n",
      "efficient LLMs research and inspire them to contribute to this important and exciting field.\n",
      "‡‡The work is done outside Amazon.\n",
      "1' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-05-18T01:00:32+00:00', 'author': '', 'keywords': '', 'moddate': '2024-05-18T01:00:32+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/Efficient Large Language Models- A Survey.pdf', 'total_pages': 67, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(docs) \n",
    "print(f\"\\ntype:{type(docs)} len:{len(docs)}\")\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b180bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1163, which is longer than the specified 1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 54\n",
      "page_content='Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:\n",
      "\n",
      "Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty. They have done so during periods of prosperity and tranquility. And they have done so in the midst of war and depression; at moments of great strife and great struggle.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "file_path = data_path / \"state_of_the_union.txt\"\n",
    "with open(file_path) as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=128,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(f\"Chunks: {len(texts)}\")\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b6a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:\n",
      "\n",
      "Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty. They have done so during periods of prosperity and tranquility. And they have done so in the midst of war and depression; at moments of great strife and great struggle.'\n",
      "page_content='It's tempting to look back on these moments and assume that our progress was inevitable, that America was always destined to succeed. But when the Union was turned back at Bull Run and the Allies first landed at Omaha Beach, victory was very much in doubt. When the market crashed on Black Tuesday and civil rights marchers were beaten on Bloody Sunday, the future was anything but certain. These were times that tested the courage of our convictions and the strength of our union. And despite all our divisions and disagreements, our hesitations and our fears, America prevailed because we chose to move forward as one nation and one people.\n",
      "\n",
      "Again, we are tested. And again, we must answer history's call.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load example document\n",
    "file_path = data_path / \"state_of_the_union.txt\"\n",
    "with open(file_path) as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=128,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbe26f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:\\n\\nOur Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty. They have done so during periods of prosperity and tranquility. And they have done so in the midst of war and depression; at moments of great strife and great struggle.',\n",
       " \"It's tempting to look back on these moments and assume that our progress was inevitable, that America was always destined to succeed. But when the Union was turned back at Bull Run and the Allies first landed at Omaha Beach, victory was very much in doubt. When the market crashed on Black Tuesday and civil rights marchers were beaten on Bloody Sunday, the future was anything but certain. These were times that tested the courage of our convictions and the strength of our union. And despite all our divisions and disagreements, our hesitations and our fears, America prevailed because we chose to move forward as one nation and one people.\\n\\nAgain, we are tested. And again, we must answer history's call.\",\n",
       " \"Again, we are tested. And again, we must answer history's call.\\n\\nOne year ago, I took office amid two wars, an economy rocked by severe recession, a financial system on the verge of collapse and a government deeply in debt. Experts from across the political spectrum warned that if we did not act, we might face a second depression. So we acted immediately and aggressively. And one year later, the worst of the storm has passed.\\n\\nBut the devastation remains. One in 10 Americans still cannot find work. Many businesses have shuttered. Home values have declined. Small towns and rural communities have been hit especially hard. For those who had already known poverty, life has become that much harder.\\n\\nThis recession has also compounded the burdens that America's families have been dealing with for decades -- the burden of working harder and longer for less, of being unable to save enough to retire or help kids with college.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.split_text(state_of_the_union)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90507e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb95835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madame Speaker, Vice President Biden, members of Congress, distinguished guests, and fellow Americans:\n",
      "\n",
      "Our Constitution declares that from time to time, the president shall give to Congress information about the state of our union. For 220 years, our leaders have fulfilled this duty.\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.create_documents([state_of_the_union])\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d99a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
