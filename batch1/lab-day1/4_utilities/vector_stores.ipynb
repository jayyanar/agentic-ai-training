{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c79f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain-crash-course/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be81a16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7698a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load PDF\n",
    "data_path = Path(\"../data\")\n",
    "file_path = data_path / \"Reinforcement Learning from Human Feedback.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8792d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=128)\n",
    "chunks = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c769738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement Learning from Human F eedback\n",
      "A short introduction to RLHF and post-training focused on language models.\n",
      "Nathan Lambert\n",
      "2 July 2025\n",
      "Abstract\n",
      "Reinforcement learning from human feedback (RLHF) has become an important\n",
      "technical and storytelling tool to deploy the latest machine learning systems. In this\n",
      "book, we hope to give a gentle introduction to the core methods for people with some\n",
      "level of quantitative background. The book starts with the origins of RLHF ‚Äì both\n",
      "in recent literature and in a convergence of disparate fields of science in economics,\n",
      "philosophy , and optimal control. W e then set the stage with definitions, problem\n",
      "formulation, data collection, and other common math used in the literature. The\n",
      "core of the book details every optimization stage in using RLHF, from starting with\n",
      "instruction tuning to training a reward model and finally all of rejection sampling,\n",
      "reinforcement learning, and direct alignment algorithms. The book concludes with\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f49ee1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create embeddings and store in FAISS\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e8ffc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "persissent_directory = \"./chroma_db\"\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings, collection_metadata={\"hnsw:space\": \"cosine\"}, persist_directory=persissent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b18daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Query and retrieve relevant docs\n",
    "query = \"what is RLHF\"\n",
    "results = vectorstore.similarity_search_with_score(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5099b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Relevant Documents:\n",
      "score: 0.5509 content: details key decisions and basic implementation examples for each step in this process.\n",
      "RLHF has been applied to many domains successfully, with complexity increasing as the\n",
      "techniques have matured. Early breakthrough experiments with RLHF were applied to\n",
      "deep reinforcement learning[1], summarization [2], following instructions[3], parsing web\n",
      "information for question answering[4], and ‚Äúalignment‚Äù[5]. A summary of the early RLHF\n",
      "recipes is shown below in fig.1.\n",
      "Figure 1: A rendition of the early, three stage RLHF process with SFT, a reward model,\n",
      "and then optimization.\n",
      "In modern language model training, RLHF is one component of post-training. Post-training\n",
      "is a more complete set of techniques and best-practices to make language models more useful\n",
      "for downstream tasks [6]. Post-training can be summarized as using three optimization\n",
      "methods:\n",
      "1. Instruction / Supervised Finetuning (IFT/SFT), where we teach formatting and for\n",
      "base of instruction following abilities. This is largely about learningfeatures in lan-\n",
      "---------------------------------\n",
      "score: 0.5409 content: Modern research has established RLHF as a general method to integrate subtle stylistic and\n",
      "related behavioral features into the models. Compared to other techniques for post-training,\n",
      "such as instruction finetuning, RLHF generalizes far better across domains[7] [8] ‚Äì helping\n",
      "create effective general purpose models.\n",
      "Intuitively, this can be seen in how the optimization techniques are applied. Instruction\n",
      "finetuning is training the model to predict the next certain token when the text preceding\n",
      "7\n",
      "---------------------------------\n",
      "score: 0.5017 content: is close to examples it has seen. It is optimizing the model to more regularly output specific\n",
      "features in text. This is a per-token update.\n",
      "RLHF on the other hand tunes the responses on the response level rather than looking at\n",
      "the next token specifically. Additionally, it is telling the model what abetter response looks\n",
      "like, rather than a specific response it should learn. RLHF also shows a model which type of\n",
      "response it should avoid, i.e. negative feedback. The training to achieve this is often called\n",
      "a contrastive loss function and is referenced throughout this book.\n",
      "While this flexibility is a major advantage of RLHF, it comes with implementation challenges.\n",
      "Largely, these center onhow to control the optimization. As we will cover in this book,\n",
      "implementing RLHF often requires training a reward model, of which best practices are not\n",
      "strongly established and depend on the area of application. With this, the optimization\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 3 Relevant Documents:\")\n",
    "for doc, score in results:\n",
    "    # The score from Chroma is cosine distance (lower is better)\n",
    "    # We convert it to cosine similarity (higher is better, range 0 to 1)\n",
    "    similarity_score = 1 - score\n",
    "    print(f\"score: {similarity_score:.4f} content: {doc.page_content}\")\n",
    "    print(f\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73026f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Top 3 Relevant Documents:\n",
      "details key decisions and basic implementation examples for each step in this process.\n",
      "RLHF has been applied to many domains successfully, with complexity increasing as the\n",
      "techniques have matured. Early breakthrough experiments with RLHF were applied to\n",
      "deep reinforcement learning[1], summarization [2], following instructions[3], parsing web\n",
      "information for question answering[4], and ‚Äúalignment‚Äù[5]. A summary of the early RLHF\n",
      "recipes is shown below in fig.1.\n",
      "Figure 1: A rendition of the early, three stage RLHF process with SFT, a reward model,\n",
      "and then optimization.\n",
      "In modern language model training, RLHF is one component of post-training. Post-training\n",
      "is a more complete set of techniques and best-practices to make language models more useful\n",
      "for downstream tasks [6]. Post-training can be summarized as using three optimization\n",
      "methods:\n",
      "1. Instruction / Supervised Finetuning (IFT/SFT), where we teach formatting and for\n",
      "base of instruction following abilities. This is largely about learningfeatures in lan-\n",
      "----------- ----------------------\n",
      "Modern research has established RLHF as a general method to integrate subtle stylistic and\n",
      "related behavioral features into the models. Compared to other techniques for post-training,\n",
      "such as instruction finetuning, RLHF generalizes far better across domains[7] [8] ‚Äì helping\n",
      "create effective general purpose models.\n",
      "Intuitively, this can be seen in how the optimization techniques are applied. Instruction\n",
      "finetuning is training the model to predict the next certain token when the text preceding\n",
      "7\n",
      "----------- ----------------------\n",
      "is close to examples it has seen. It is optimizing the model to more regularly output specific\n",
      "features in text. This is a per-token update.\n",
      "RLHF on the other hand tunes the responses on the response level rather than looking at\n",
      "the next token specifically. Additionally, it is telling the model what abetter response looks\n",
      "like, rather than a specific response it should learn. RLHF also shows a model which type of\n",
      "response it should avoid, i.e. negative feedback. The training to achieve this is often called\n",
      "a contrastive loss function and is referenced throughout this book.\n",
      "While this flexibility is a major advantage of RLHF, it comes with implementation challenges.\n",
      "Largely, these center onhow to control the optimization. As we will cover in this book,\n",
      "implementing RLHF often requires training a reward model, of which best practices are not\n",
      "strongly established and depend on the area of application. With this, the optimization\n",
      "----------- ----------------------\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(\"\\nüîç Top 3 Relevant Documents:\")\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n",
    "    print(\"----------- ----------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0c5cced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ---------------------- Query and Retrieve Documents ----------------------\n",
      "\n",
      "üîç Top 3 Relevant Documents:\n",
      "details key decisions and basic implementation examples for each step in this process.\n",
      "RLHF has been applied to many domains successfully, with complexity increasing as the\n",
      "techniques have matured. Early breakthrough experiments with RLHF were applied to\n",
      "deep reinforcement learning[1], summarization [2], following instructions[3], parsing web\n",
      "information for question answering[4], and ‚Äúalignment‚Äù[5]. A summary of the early RLHF\n",
      "recipes is shown below in fig.1.\n",
      "Figure 1: A rendition of the early, three stage RLHF process with SFT, a reward model,\n",
      "and then optimization.\n",
      "In modern language model training, RLHF is one component of post-training. Post-training\n",
      "is a more complete set of techniques and best-practices to make language models more useful\n",
      "for downstream tasks [6]. Post-training can be summarized as using three optimization\n",
      "methods:\n",
      "1. Instruction / Supervised Finetuning (IFT/SFT), where we teach formatting and for\n",
      "base of instruction following abilities. This is largely about learningfeatures in lan-\n",
      "----------- ----------------------\n",
      "Modern research has established RLHF as a general method to integrate subtle stylistic and\n",
      "related behavioral features into the models. Compared to other techniques for post-training,\n",
      "such as instruction finetuning, RLHF generalizes far better across domains[7] [8] ‚Äì helping\n",
      "create effective general purpose models.\n",
      "Intuitively, this can be seen in how the optimization techniques are applied. Instruction\n",
      "finetuning is training the model to predict the next certain token when the text preceding\n",
      "7\n",
      "----------- ----------------------\n",
      "is close to examples it has seen. It is optimizing the model to more regularly output specific\n",
      "features in text. This is a per-token update.\n",
      "RLHF on the other hand tunes the responses on the response level rather than looking at\n",
      "the next token specifically. Additionally, it is telling the model what abetter response looks\n",
      "like, rather than a specific response it should learn. RLHF also shows a model which type of\n",
      "response it should avoid, i.e. negative feedback. The training to achieve this is often called\n",
      "a contrastive loss function and is referenced throughout this book.\n",
      "While this flexibility is a major advantage of RLHF, it comes with implementation challenges.\n",
      "Largely, these center onhow to control the optimization. As we will cover in this book,\n",
      "implementing RLHF often requires training a reward model, of which best practices are not\n",
      "strongly established and depend on the area of application. With this, the optimization\n",
      "----------- ----------------------\n",
      "\n",
      "# ----------------------  lambda = 0.1----------------------\n",
      "\n",
      "üîç Top 3 Relevant Documents:\n",
      "details key decisions and basic implementation examples for each step in this process.\n",
      "RLHF has been applied to many domains successfully, with complexity increasing as the\n",
      "techniques have matured. Early breakthrough experiments with RLHF were applied to\n",
      "deep reinforcement learning[1], summarization [2], following instructions[3], parsing web\n",
      "information for question answering[4], and ‚Äúalignment‚Äù[5]. A summary of the early RLHF\n",
      "recipes is shown below in fig.1.\n",
      "Figure 1: A rendition of the early, three stage RLHF process with SFT, a reward model,\n",
      "and then optimization.\n",
      "In modern language model training, RLHF is one component of post-training. Post-training\n",
      "is a more complete set of techniques and best-practices to make language models more useful\n",
      "for downstream tasks [6]. Post-training can be summarized as using three optimization\n",
      "methods:\n",
      "1. Instruction / Supervised Finetuning (IFT/SFT), where we teach formatting and for\n",
      "base of instruction following abilities. This is largely about learningfeatures in lan-\n",
      "----------- ----------------------\n",
      "with a suite of methods designed for quantifying preferences.\n",
      "5.1.1 Quantifying preferences\n",
      "The core of RLHF‚Äôs motivation is the ability to optimize a model of human preferences,\n",
      "which therefore needs to be quantified. To do this, RLHF builds on extensive literature\n",
      "with assumptions that human decisions and preferences can be quantified. Early philoso-\n",
      "phers discussed the existence of preferences, such as Aristotle‚Äôs Topics, Book Three, and\n",
      "substantive forms of this reasoning emerged later withThe Port-Royal Logic [73]:\n",
      "To judge what one must do to obtain a good or avoid an evil, it is necessary\n",
      "to consider not only the good and evil in itself, but also the probability that it\n",
      "happens or does not happen.\n",
      "Progression of these ideas continued through Bentham‚ÄôsHedonic Calculus [74] that proposed\n",
      "that all of life‚Äôs considerations can be weighed, and Ramsey‚ÄôsT ruth and Probability[75] that\n",
      "applied a quantitative model to preferences. This direction, drawing on advancements in\n",
      "----------- ----------------------\n",
      "human data allows finer control of the models in real-world product settings or for newer\n",
      "training methods such as character training.\n",
      "The term RLAIF was introduced in Anthropic‚Äôs workConstitutional AI: Harmlessness from\n",
      "AI F eedback[18], which resulted in initial confusion in the AI community over the relation-\n",
      "ship between the methods. Since the release of the Constitutional AI (CAI) paper and the\n",
      "formalization of RLAIF, RLAIF has become a default method within the post-training and\n",
      "RLHF literatures ‚Äì there are far more examples than one can easily enumerate. The rela-\n",
      "tionship should be understood as CAI was the example that kickstarted the broader field of\n",
      "RLAIF.\n",
      "A rule of thumb for the difference between human data and AI feedback data is as follows:\n",
      "1. Human data is high-noise and low-bias,\n",
      "2. Synthetic preference data is low-noise and high-bias,\n",
      "Results in many academic results showing how one can substitute AI preference data in\n",
      "----------- ----------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Query and Retrieve Documents ----------------------\n",
    "print(\"\\n# ---------------------- Query and Retrieve Documents ----------------------\")\n",
    "query = \"what is RLHF?\"\n",
    "# Step 4: Convert vectorstore into a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 3, \"lambda_mult\": 1}\n",
    ")\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(\"\\nüîç Top 3 Relevant Documents:\")\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n",
    "    print(\"----------- ----------------------\")\n",
    "\n",
    "print(\"\\n# ----------------------  lambda = 0.1----------------------\")\n",
    "\n",
    "# Step 4: Convert vectorstore into a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\"k\": 3, \"lambda_mult\": 0.1}\n",
    ")\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "print(\"\\nüîç Top 3 Relevant Documents:\")\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n",
    "    print(\"----------- ----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c9aa4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n",
    "    # base_url=\"...\",\n",
    "    # organization=\"...\",\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de944c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/df4h8c2x1hb4dtfplhjxxcdc0000gn/T/ipykernel_76644/3979646991.py:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  multi_results = multi_retriever.get_relevant_documents(query, kwargs={\"k\": 1})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chunk 1 ---\n",
      "1 Introduction\n",
      "Reinforcement learning from Human Feedback (RLHF) is a technique used to incorporate\n",
      "human information into AI systems. RLHF emerged primarily as a method to solve hard to\n",
      "specify problems. Its early applications were often in control problems and other traditional\n",
      "domains for reinforcement learning (RL). RLHF became most known through the release\n",
      "of ChatGPT and the subsequent rapid development of large language models (LLMs) and\n",
      "other foundation models.\n",
      "The basic pipeline for RLHF involves three steps. First, a language model that can follow\n",
      "user questions must be trained (see Chapter 9). Second, human preference data must be\n",
      "collected for the training of a reward model of human preferences (see Chapter 7). Finally,\n",
      "the language model can be optimized with an RL optimizer of choice, by sampling genera-\n",
      "tions and rating them with respect to the reward model (see Chapter 3 and 11). This book\n",
      "details key decisions and basic implementation examples for each step in this process.\n",
      "\n",
      "--- Chunk 2 ---\n",
      "details key decisions and basic implementation examples for each step in this process.\n",
      "RLHF has been applied to many domains successfully, with complexity increasing as the\n",
      "techniques have matured. Early breakthrough experiments with RLHF were applied to\n",
      "deep reinforcement learning[1], summarization [2], following instructions[3], parsing web\n",
      "information for question answering[4], and ‚Äúalignment‚Äù[5]. A summary of the early RLHF\n",
      "recipes is shown below in fig.1.\n",
      "Figure 1: A rendition of the early, three stage RLHF process with SFT, a reward model,\n",
      "and then optimization.\n",
      "In modern language model training, RLHF is one component of post-training. Post-training\n",
      "is a more complete set of techniques and best-practices to make language models more useful\n",
      "for downstream tasks [6]. Post-training can be summarized as using three optimization\n",
      "methods:\n",
      "1. Instruction / Supervised Finetuning (IFT/SFT), where we teach formatting and for\n",
      "base of instruction following abilities. This is largely about learningfeatures in lan-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 1}),\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "multi_results = multi_retriever.get_relevant_documents(query, kwargs={\"k\": 1})\n",
    "\n",
    "for i, doc in enumerate(multi_results, 1):\n",
    "    print(f\"--- Chunk {i} ---\")\n",
    "    print(doc.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abced6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
