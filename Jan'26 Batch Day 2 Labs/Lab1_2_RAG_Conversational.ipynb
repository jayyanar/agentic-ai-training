{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1.2: Conversational RAG with Groq\n",
    "\n",
    "In this lab, we will extend our RAG system to support **Conversational Memory**. This allows users to ask follow-up questions, and the system will understand the context from previous turns.\n",
    "\n",
    "## Key Concepts\n",
    "1. **History Aware Retriever**: Rephrases the user's latest query using the chat history so it makes sense as a standalone query to the vector store.\n",
    "2. **Chat History**: Maintaining line of conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4a6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "%pip install -qU langchain langchain-groq langchain-community langchain-huggingface chromadb sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2059d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup API Keys\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29be1e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\GenAI Labs\\Jan-2026-day2-labs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# 3. Setup Vector Store (Same as Lab 1.1)\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Load\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed & Store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ec8a5",
   "metadata": {},
   "source": [
    "## 4. History Aware Retriever\n",
    "We need a chain that takes the `chat_history` and the `input` and generates a search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3b57e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    temperature=0,\n",
    "    reasoning_format=\"parsed\"\n",
    ")\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain to rephrase question\n",
    "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f7fda3",
   "metadata": {},
   "source": [
    "## 5. QA Chain with History\n",
    "Now we create the final chain that uses the retrieved documents to answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b519c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "qa_system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def contextualized_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return contextualize_q_chain\n",
    "    else:\n",
    "        return input.get(\"input\")\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=contextualized_question | retriever | format_docs\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcfd4f3",
   "metadata": {},
   "source": [
    "## 6. Testing the Chat\n",
    "We can now manage specific chat sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb85cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What are the main modules in an agent's architecture?\n",
      "AI: The main modules in an agent's architecture are **Planning**, **Memory**, and **Tool Use**. Planning involves task decomposition, self-reflection, and optimizing actions based on beliefs and environment observations. Memory includes short-term (contextual learning) and long-term (external storage/retrieval) components, while Tool Use enables API calls for external information or execution.\n",
      "\n",
      "User: What is the difference between them?\n",
      "AI: The **Planning** module focuses on task decomposition, subgoal creation, and self-reflection to optimize decision-making. The **Memory** module stores and retrieves information (short-term for context, long-term for persistent data) to inform actions and learning. The **Tool Use** module enables interaction with external APIs to access real-time data or execute tasks beyond the agentâ€™s internal knowledge. Each serves distinct roles: Planning strategizes, Memory retains context, and Tool Use extends capabilities through external resources.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "# First Question\n",
    "user_input = \"What are the main modules in an agent's architecture?\"\n",
    "# rag_chain returns string now\n",
    "answer = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
    "print(f\"User: {user_input}\")\n",
    "print(f\"AI: {answer}\")\n",
    "\n",
    "# Update History\n",
    "chat_history.extend([HumanMessage(content=user_input), AIMessage(content=answer)])\n",
    "\n",
    "# Second Question (Follow-up)\n",
    "user_input = \"What is the difference between them?\"\n",
    "answer = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
    "print(f\"\\nUser: {user_input}\")\n",
    "print(f\"AI: {answer}\")\n",
    "\n",
    "chat_history.extend([HumanMessage(content=user_input), AIMessage(content=answer)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
