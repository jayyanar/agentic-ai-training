{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 2.4: LLM Observability with Arize Phoenix\n",
                "\n",
                "In this lab, we will learn how to instrument our LLM applications for observability using **Arize Phoenix**. Observability is crucial for understanding how your LLM applications are performing, debugging issues, and evaluating quality.\n",
                "\n",
                "We will cover:\n",
                "1.  **Setup**: Installing Phoenix and launching the local UI.\n",
                "2.  **Part 1: Simple LangChain Flow**: Instrumenting a basic chain.\n",
                "3.  **Part 2: Advanced LangGraph Application**: Instrumenting a stateful agent workflow.\n",
                "\n",
                "### Prerequisites\n",
                "- Ensure you have your Groq API Key ready.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "312def0f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Note: you may need to restart the kernel to use updated packages.\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "# 1. Install Dependencies\n",
                "%pip install -qU arize-phoenix arize-otel openinference-instrumentation-langchain langchain langchain-groq langgraph\n",
                "%pip install -q \"arize[AutoEmbeddings]\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "5ab4658e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Setup API Keys\n",
                "import getpass\n",
                "import os\n",
                "from arize.otel import register\n",
                "\n",
                "if \"GROQ_API_KEY\" not in os.environ:\n",
                "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
                "\n",
                "if \"ARIZE_SPACE_ID\" not in os.environ:\n",
                "    os.environ[\"ARIZE_SPACE_ID\"] = getpass.getpass(\"Enter your Arize Space ID: \")\n",
                "\n",
                "if \"ARIZE_API_KEY\" not in os.environ:\n",
                "    os.environ[\"ARIZE_API_KEY\"] = getpass.getpass(\"Enter your Arize API Key: \")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "47fd9737",
            "metadata": {},
            "source": [
                "## 3. Launch Arize Phoenix (Optional)\n",
                "You can still launch the Phoenix application locally to view traces locally, or you can view them in the Arize Cloud dashboard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "692d2020",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "OpenTelemetry Tracing Details\n",
                        "|  Arize Project: arize-lab-demo\n",
                        "|  Span Processor: BatchSpanProcessor\n",
                        "|  Collector Endpoint: otlp.arize.com\n",
                        "|  Transport: gRPC\n",
                        "|  Transport Headers: {'authorization': '****', 'api_key': '****', 'arize-space-id': '****', 'space_id': '****', 'arize-interface': '****'}\n",
                        "|  \n",
                        "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
                        "|  \n",
                        "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
                        "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
                        "\n",
                        "ðŸ”­ Arize observability configured successfully!\n",
                        "ðŸ“Š All LangChain operations will now be automatically traced\n"
                    ]
                }
            ],
            "source": [
                "# Import Arize observability components\n",
                "from arize.otel import register\n",
                "from getpass import getpass\n",
                "\n",
                "# Configure Arize tracing\n",
                "tracer_provider = register(\n",
                "    space_id=os.environ[\"ARIZE_SPACE_ID\"],\n",
                "    api_key=os.environ[\"ARIZE_API_KEY\"],\n",
                "    project_name=\"arize-lab-demo\"\n",
                ")\n",
                "\n",
                "# Enable automatic LangChain instrumentation\n",
                "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
                "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
                "\n",
                "print(\"ðŸ”­ Arize observability configured successfully!\")\n",
                "print(\"ðŸ“Š All LangChain operations will now be automatically traced\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e2c3823c",
            "metadata": {},
            "source": [
                "## Part 1: Simple LangChain Flow\n",
                "\n",
                "We will create a simple LangChain application and instrument it using `LangChainInstrumentor`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "73e99769",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "d:\\Projects\\GenAI Labs\\Jan-2026-RAG-LangGraph-labs\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Invoking chain...\n",
                        "Response: **Why did the chicken cross the road?**  \n",
                        "To improve the *observability* of its journeyâ€”now every step is logged, metrics are tracked, and the trace reveals it was part of a larger *flock workflow*! ðŸ”ðŸ“Š  \n",
                        "\n",
                        "*(Bonus punchline: Without observability, weâ€™d still be wondering if it was a flaky egg or a systemic poultry issue!)*  \n",
                        "\n",
                        "---  \n",
                        "*Logs: \"Weâ€™re here!\"*  \n",
                        "*Metrics: \"Weâ€™re here too!\"*  \n",
                        "*Traces: \"And weâ€™re everywhere in between.\"*  \n",
                        "*Together: \"Welcome to your new observable system!\"* ðŸš€\n"
                    ]
                }
            ],
            "source": [
                "from langchain_groq import ChatGroq\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "\n",
                "\n",
                "# Setup a simple chain\n",
                "llm = ChatGroq(\n",
                "    model=\"qwen/qwen3-32b\",\n",
                "    temperature=0,\n",
                "    reasoning_format=\"parsed\"\n",
                ")\n",
                "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
                "chain = prompt | llm | StrOutputParser()\n",
                "\n",
                "# Invoke the chain\n",
                "print(\"Invoking chain...\")\n",
                "response = chain.invoke({\"topic\": \"observability\"})\n",
                "print(f\"Response: {response}\")\n",
                "\n",
                "# Check the Phoenix UI and Arize Cloud to see the trace!"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b58211ff",
            "metadata": {},
            "source": [
                "## Part 2: Advanced LangGraph Application\n",
                "\n",
                "Now, let's look at something more complex: a LangGraph workflow. LangGraph allows for stateful, cyclic interactions, which are harder to debug without traces.\n",
                "\n",
                "We will build a simple agent that can allow a user to chat."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "48a30367",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ðŸš€ Starting Advanced LangGraph Execution...\n",
                        "\n",
                        "--- NODE: RESEARCHER ---\n",
                        "Content: Tracing is a critical component in Large Language Model (LLM) applications, enabling developers to monitor, debug, and optimize complex, distributed systems. Below is a detailed technical explanation ...\n",
                        "\n",
                        "--- NODE: REVIEWER ---\n",
                        "Content: - **Architecture & Performance Optimization**: Tracing maps distributed LLM workflows (UI, APIs, preprocessing, model inference, databases) to identify bottlenecks, optimize latency, and balance resou...\n",
                        "\n",
                        "âœ… Execution Complete. Check your Phoenix UI to see the multi-node trace.\n"
                    ]
                }
            ],
            "source": [
                "from typing import Annotated, Sequence\n",
                "from typing_extensions import TypedDict\n",
                "from langgraph.graph import StateGraph, START, END\n",
                "from langgraph.graph.message import add_messages\n",
                "from langchain_core.messages import BaseMessage, HumanMessage\n",
                "\n",
                "# --- 2. STATE DEFINITION ---\n",
                "class State(TypedDict):\n",
                "    # add_messages allows us to append to the conversation history\n",
                "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
                "    # We can track extra metadata for tracing\n",
                "    iteration_count: int\n",
                "\n",
                "# Define LLM Node\n",
                "def researcher_node(state: State):\n",
                "    \"\"\"Initial research node to gather info.\"\"\"\n",
                "    system_msg = (\n",
                "        \"You are an expert researcher. Provide a detailed, technical explanation \"\n",
                "        \"of the topic requested. Focus on architecture and performance.\"\n",
                "    )\n",
                "    # We create a specific prompt for this node to show different prompt traces\n",
                "    prompt = ChatPromptTemplate.from_messages([\n",
                "        (\"system\", system_msg),\n",
                "        (\"placeholder\", \"{messages}\")\n",
                "    ])\n",
                "    chain = prompt | llm\n",
                "    response = chain.invoke({\"messages\": state[\"messages\"]})\n",
                "    return {\"messages\": [response], \"iteration_count\": 1}\n",
                "\n",
                "def reviewer_node(state: State):\n",
                "    \"\"\"Reviewer node that critiques the researcher's output.\"\"\"\n",
                "    system_msg = (\n",
                "        \"You are a critical editor. Summarize the research provided \"\n",
                "        \"into three bullet points and add a 'Grade' (A-F) based on clarity.\"\n",
                "    )\n",
                "    # This node shows how context is passed from the previous node\n",
                "    prompt = ChatPromptTemplate.from_messages([\n",
                "        (\"system\", system_msg),\n",
                "        (\"placeholder\", \"{messages}\")\n",
                "    ])\n",
                "    chain = prompt | llm\n",
                "    response = chain.invoke({\"messages\": state[\"messages\"]})\n",
                "    return {\"messages\": [response]}\n",
                "\n",
                "# Build Graph\n",
                "workflow = StateGraph(State)\n",
                "\n",
                "# Add Nodes\n",
                "workflow.add_node(\"researcher\", researcher_node)\n",
                "workflow.add_node(\"reviewer\", reviewer_node)\n",
                "\n",
                "# Define Logic\n",
                "# Start -> Research -> Review -> End\n",
                "workflow.add_edge(START, \"researcher\")\n",
                "workflow.add_edge(\"researcher\", \"reviewer\")\n",
                "workflow.add_edge(\"reviewer\", END)\n",
                "\n",
                "# Compile\n",
                "graph = workflow.compile()\n",
                "\n",
                "# Invoke Graph\n",
                "print(\"ðŸš€ Starting Advanced LangGraph Execution...\")\n",
                "\n",
                "inputs = {\n",
                "    \"messages\": [HumanMessage(content=\"Explain the importance of tracing in LLM apps.\")],\n",
                "    \"iteration_count\": 0\n",
                "}\n",
                "\n",
                "# Streaming allows us to see the trace update in real-time in Arize/Phoenix\n",
                "for output in graph.stream(inputs, stream_mode=\"updates\"):\n",
                "    for node_name, node_state in output.items():\n",
                "        print(f\"\\n--- NODE: {node_name.upper()} ---\")\n",
                "        last_msg = node_state[\"messages\"][-1]\n",
                "        print(f\"Content: {last_msg.content[:200]}...\")\n",
                "\n",
                "print(\"\\nâœ… Execution Complete. Check your Phoenix UI to see the multi-node trace.\")\n",
                "\n",
                "# Check Phoenix UI again. You should see a trace for this execution showing the graph steps."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "You have successfully set up Arize Phoenix and instrumented both a simple LangChain flow and a LangGraph application. \n",
                "Review the traces in the Phoenix UI to understand the internal execution of your LLM calls."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
