{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 2.1: LangGraph Persistence\n",
                "\n",
                "In this lab, we will add **Persistence** (Memory) to our graph. This allows the graph to remember the state across different interactions, enabling long-running conversations.\n",
                "\n",
                "## Key Concepts\n",
                "1. **Checkpointer**: A mechanism to save the state of the graph at every step.\n",
                "2. **MemorySaver**: An in-memory checkpointer for development/testing.\n",
                "3. **Thread ID**: A unique identifier to separate different user sessions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "%pip install -qU langchain-groq langchain-community langgraph"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Setup API Keys\n",
                "import getpass\n",
                "import os\n",
                "\n",
                "if \"GROQ_API_KEY\" not in os.environ:\n",
                "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "<langgraph.graph.state.StateGraph at 0x1ef206fb390>"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# 3. Define Graph (Same as Lab 1.3)\n",
                "from typing import Annotated\n",
                "from typing_extensions import TypedDict\n",
                "from langgraph.graph import StateGraph, START, END\n",
                "from langgraph.graph.message import add_messages\n",
                "from langchain_groq import ChatGroq\n",
                "\n",
                "class State(TypedDict):\n",
                "    messages: Annotated[list, add_messages]\n",
                "    \n",
                "# Initialize LLM\n",
                "llm = ChatGroq(\n",
                "    model=\"qwen/qwen3-32b\",\n",
                "    temperature=0,\n",
                "    reasoning_format=\"parsed\"\n",
                ")\n",
                "\n",
                "def chatbot(state: State):\n",
                "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
                "\n",
                "graph_builder = StateGraph(State)\n",
                "graph_builder.add_node(\"chatbot\", chatbot)\n",
                "graph_builder.add_edge(START, \"chatbot\")\n",
                "graph_builder.add_edge(\"chatbot\", END)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Add Persistence\n",
                "We use `MemorySaver` to store the state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langgraph.checkpoint.memory import MemorySaver\n",
                "\n",
                "memory = MemorySaver()\n",
                "\n",
                "# Compile with checkpointer\n",
                "graph = graph_builder.compile(checkpointer=memory)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. run with Thread ID\n",
                "We use `configurable` to specify the thread."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Assistant: Hi Bob! Nice to meet you. How can I assist you today? ðŸ˜Š\n"
                    ]
                }
            ],
            "source": [
                "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
                "\n",
                "user_input = \"Hi, I am Bob.\"\n",
                "\n",
                "for event in graph.stream({\"messages\": [(\"user\", user_input)]}, config=config):\n",
                "    for value in event.values():\n",
                "        print(\"Assistant:\", value[\"messages\"][-1].content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Verify Memory\n",
                "Ask \"What is my name?\" in a new execution block using the SAME `thread_id`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Assistant: Your name is Bob! ðŸ˜Š You mentioned it earlier. How can I assist you today, Bob?\n"
                    ]
                }
            ],
            "source": [
                "user_input = \"What is my name?\"\n",
                "\n",
                "# Note: We are using the same 'config' with thread_id='1'\n",
                "for event in graph.stream({\"messages\": [(\"user\", user_input)]}, config=config):\n",
                "    for value in event.values():\n",
                "        print(\"Assistant:\", value[\"messages\"][-1].content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. New Thread\n",
                "If we change the `thread_id`, the memory should be empty."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Assistant: I don't have access to your personal information, including your name. If you'd like me to address you by name in our conversation, feel free to share it with me!\n"
                    ]
                }
            ],
            "source": [
                "config_new = {\"configurable\": {\"thread_id\": \"2\"}}\n",
                "\n",
                "# Asking the same question in a new thread\n",
                "user_input = \"What is my name?\"\n",
                "\n",
                "for event in graph.stream({\"messages\": [(\"user\", user_input)]}, config=config_new):\n",
                "    for value in event.values():\n",
                "        print(\"Assistant:\", value[\"messages\"][-1].content)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
