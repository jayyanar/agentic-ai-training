{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayyanar/agentic-ai-training/blob/lab-day-2/labs-day-2/Lab2_2_RAG_Conversational.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs6wbAy90JoU"
      },
      "source": [
        "# Lab 1.2: Conversational RAG - Mortgage Policy Assistant\n",
        "\n",
        "In this lab, we will build a **Banking Policy Assistant** for Wells Fargo. This assistant allows loan officers to query internal mortgage guidelines (e.g., Conforming vs. Jumbo loans) and ask follow-up questions while maintaining conversational context.\n",
        "\n",
        "## Use Case\n",
        "A loan officer needs to quickly check credit score requirements for different loan types without searching through a 200-page PDF manual. They might ask:\n",
        "1. \"What is the min credit score for a Jumbo loan?\"\n",
        "2. \"How does that compare to FHA?\" (Contextual follow-up)\n",
        "\n",
        "## Key Concepts\n",
        "1. **History Aware Retriever**: Rephrases the user's latest query using the chat history so it makes sense as a standalone query to the vector store.\n",
        "2. **Chat History**: Maintaining line of conversation."
      ],
      "id": "Cs6wbAy90JoU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78c4a6b3",
      "metadata": {
        "id": "78c4a6b3"
      },
      "outputs": [],
      "source": [
        "# 1. Install Dependencies\n",
        "print(\"Installing dependencies...\")\n",
        "%pip install -qU langchain langchain-groq langchain-community langchain-huggingface chromadb sentence-transformers\n",
        "print(\"Dependencies installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2059d7b",
      "metadata": {
        "id": "a2059d7b"
      },
      "outputs": [],
      "source": [
        "# 2. Setup API Keys\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "# Optional for LangSmith Tracking\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"RAG Conversational\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c50a37",
      "metadata": {
        "id": "d8c50a37"
      },
      "outputs": [],
      "source": [
        "banking_policy_text = \"\"\"\n",
        "Wells Fargo Mortgage Lending Guidelines - Internal Policy Use Only\n",
        "\n",
        "1. CONFORMING LOANS\n",
        "   - Definition: Loans that meet the purchase limits set by Fannie Mae and Freddie Mac.\n",
        "   - Max Loan Amount: $766,550 for single-family homes (as of 2024).\n",
        "   - Minimum Credit Score: 620.\n",
        "   - Down Payment: Minimum 3% for first-time homebuyers, otherwise 5%.\n",
        "   - DTI Ratio: Maximum 50% with compensating factors.\n",
        "   - Reserves: Not always required, depending on DU/LP findings.\n",
        "\n",
        "2. JUMBO LOANS\n",
        "   - Definition: Loans exceeding the conforming loan limits.\n",
        "   - Minimum Credit Score: 700 for LTV up to 80%; 720 for LTV up to 90%.\n",
        "   - Down Payment: Minimum 10% required.\n",
        "   - DTI Ratio: Strictly capped at 43%.\n",
        "   - Reserves: 6 months of PITI required.\n",
        "   - Appraisal: Two full appraisals required for loan amounts > $1.5M.\n",
        "\n",
        "3. FHA LOANS\n",
        "   - Definition: Government-backed loans insured by the Federal Housing Administration.\n",
        "   - Minimum Credit Score: 580 for 3.5% down payment; 500-579 for 10% down payment.\n",
        "   - DTI Ratio: Up to 57% allowed in some cases.\n",
        "   - MIP (Mortgage Insurance Premium): Upfront 1.75% + Annual MIP required for the life of the loan if LTV > 90%.\n",
        "   - Property Requirements: Must meet FHA safety guidelines (no peeling paint, safety handrails required).\n",
        "\n",
        "4. VA LOANS\n",
        "   - Definition: Loans for eligible veterans and service members.\n",
        "   - Down Payment: 0% required.\n",
        "   - PMI: No private mortgage insurance required.\n",
        "   - Funding Fee: Required unless the veteran has a service-connected disability.\n",
        "   - DTI Ratio: No strict cap, but residual income analysis is key.\n",
        "\n",
        "5. GENERAL UNDERWRITING REQUIREMENTS\n",
        "   - Income: 2 years of consistent employment history required. Self-employed borrowers need 2 years of tax returns.\n",
        "   - Assets: Large deposits must be sourced and explained.\n",
        "   - Bankruptcy:\n",
        "     - Chapter 7: 4-year waiting period for Conventional, 2 years for FHA/VA.\n",
        "     - Chapter 13: 2-year waiting period after discharge for Conventional, 1 year of payout period for FHA/VA.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29be1e50",
      "metadata": {
        "id": "29be1e50"
      },
      "outputs": [],
      "source": [
        "# 3. Setup Vector Store (Mortgage Policy Data)\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Load Internal Policy Document\n",
        "print(\"Loading mortgage policy document...\")\n",
        "docs = [Document(page_content=banking_policy_text, metadata={\"source\": \"internal_policy_doc\"})]\n",
        "print(f\"Loaded {len(docs)} document(s).\")\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "print(f\"Split into {len(splits)} chunks.\")\n",
        "\n",
        "# Embed & Store\n",
        "print(\"Creating vector store...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "retriever = vectorstore.as_retriever()\n",
        "print(\"Vector store created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e4ec8a5",
      "metadata": {
        "id": "3e4ec8a5"
      },
      "source": [
        "## 4. History Aware Retriever\n",
        "We need a chain that takes the `chat_history` and the `input` and generates a search query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3b57e8",
      "metadata": {
        "id": "db3b57e8"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatGroq(\n",
        "    model=\"qwen/qwen3-32b\",\n",
        "    temperature=0,\n",
        "    reasoning_format=\"parsed\"\n",
        ")\n",
        "\n",
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain to rephrase question\n",
        "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
        "print(\"Contextualization chain created.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test_context",
      "metadata": {
        "id": "test_context"
      },
      "outputs": [],
      "source": [
        "# Let's test the contextualization to understand what it does\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "print(\"--- Testing Query Contextualization ---\")\n",
        "# Mock history: user asked about Jumbo loans, AI answered.\n",
        "sample_history = [\n",
        "    HumanMessage(content=\"What is the min credit score for a Jumbo Loan?\"),\n",
        "    AIMessage(content=\"The minimum credit score is 700.\")\n",
        "]\n",
        "# User asks a follow-up specific to the context (referring to 'that')\n",
        "sample_input = \"How does that compare to FHA?\"\n",
        "print(f\"Chat History: {len(sample_history)} messages\")\n",
        "print(f\"User Follow-up: {sample_input}\")\n",
        "\n",
        "rephrased_query = contextualize_q_chain.invoke({\"chat_history\": sample_history, \"input\": sample_input})\n",
        "print(f\"Rephrased Query (for Vector Store): {rephrased_query}\")\n",
        "print(\"---------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61f7fda3",
      "metadata": {
        "id": "61f7fda3"
      },
      "source": [
        "## 5. QA Chain with History\n",
        "Now we create the final chain that uses the retrieved documents to answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b519c13b",
      "metadata": {
        "id": "b519c13b"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "qa_system_prompt = (\n",
        "    \"You are a specialized Mortgage Policy Assistant for Wells Fargo. \"\n",
        "    \"Use the following pieces of retrieved policy context to answer \"\n",
        "    \"the loan officer's question. If you don't know the answer, say that you \"\n",
        "    \"cannot find it in the policy. Keep answers professional and concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def contextualized_question(input: dict):\n",
        "    if input.get(\"chat_history\"):\n",
        "        return contextualize_q_chain\n",
        "    else:\n",
        "        return input.get(\"input\")\n",
        "\n",
        "rag_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=contextualized_question | retriever | format_docs\n",
        "    )\n",
        "    | qa_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "print(\"Full RAG chain created.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bcfd4f3",
      "metadata": {
        "id": "1bcfd4f3"
      },
      "source": [
        "## 6. Testing the Chat\n",
        "We can now manage specific chat sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cb85cda",
      "metadata": {
        "id": "0cb85cda"
      },
      "outputs": [],
      "source": [
        "chat_history = []\n",
        "\n",
        "# First Question: Specific Policy Query\n",
        "user_input = \"What is the minimum credit score for a Jumbo Loan?\"\n",
        "\n",
        "print(f\"User: {user_input}\")\n",
        "# rag_chain returns string now\n",
        "answer = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
        "print(f\"AI: {answer}\")\n",
        "\n",
        "# Update History\n",
        "chat_history.extend([HumanMessage(content=user_input), AIMessage(content=answer)])\n",
        "\n",
        "# Second Question (Follow-up): Contextual Comparison\n",
        "print(\"\\n--- Follow up ---\")\n",
        "user_input = \"How does that compare to FHA loans?\"\n",
        "print(f\"User: {user_input}\")\n",
        "\n",
        "answer = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
        "print(f\"AI: {answer}\")\n",
        "\n",
        "chat_history.extend([HumanMessage(content=user_input), AIMessage(content=answer)])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}