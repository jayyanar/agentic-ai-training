{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 3.5: Advanced LangGraph Concepts (Optional)\n",
                "\n",
                "This optional lab covers advanced patterns that are essential for building production-grade agents.\n",
                "\n",
                "### Learning Objectives\n",
                "1. **Production Persistence**: Moving beyond in-memory checkpointers to SQLite.\n",
                "2. **Time Travel**: Rewinding the graph to a previous state and exploring alternative paths (\"forking\").\n",
                "3. **Advanced Flow Control**: Using the `Command` class for dynamic, edgeless routing.\n",
                "4. **Complex Workflow**: Building a LinkedIn Post Generator with iterative human feedback."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6acdc26b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "print(\"Installing dependencies...\")\n",
                "# We strictly need langgraph 0.2.x+ for the Command class\n",
                "%pip install -qU \"langgraph>=0.2.0\" langchain-groq langchain-community langgraph-checkpoint-sqlite\n",
                "print(\"Dependencies installed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56d5c213",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Setup API Keys\n",
                "from google.colab import userdata\n",
                "import os\n",
                "\n",
                "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
                "# Optional for LangSmith Tracking\n",
                "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
                "os.environ[\"LANGSMITH_TRACING_V2\"] = \"true\"\n",
                "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
                "os.environ[\"LANGSMITH_PROJECT\"] = \"Advanced LangGraph\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5e949989",
            "metadata": {},
            "source": [
                "## Part 1: Production Persistence (SQLite)\n",
                "\n",
                "In previous labs, we used `MemorySaver`. This is great for testing but loses all data if the program restarts.\n",
                "For production, we use persistent databases like Postgres or SQLite.\n",
                "\n",
                "Here, we will use `SqliteSaver` to write valid checkpoints to a local file (`checkpoints.sqlite`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4881cfe6",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sqlite3\n",
                "from langgraph.checkpoint.sqlite import SqliteSaver\n",
                "\n",
                "print(\"Connecting to SQLite database...\")\n",
                "# Connect to a local SQLite database file\n",
                "# check_same_thread=False is needed for some Jupyter environments to avoid threading errors\n",
                "conn = sqlite3.connect(\"checkpoints.sqlite\", check_same_thread=False)\n",
                "memory_sqlite = SqliteSaver(conn)\n",
                "print(\"SQLite checkpointer ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "984b9226",
            "metadata": {},
            "source": [
                "## Part 2: Time Travel (Rewinding and Forking)\n",
                "\n",
                "Time travel allows you to:\n",
                "1. **View History**: See what the state was 3 steps ago.\n",
                "2. **Rewind**: Replay the agent from a previous step.\n",
                "3. **Fork**: Go back to a previous step, provide *different* input, and take a new path.\n",
                "\n",
                "We will build a simple \"Counter\" graph to demonstrate this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "189260ff",
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import TypedDict, Literal\n",
                "from langgraph.graph import StateGraph, START, END\n",
                "\n",
                "class CounterState(TypedDict):\n",
                "    count: int\n",
                "    last_action: str\n",
                "\n",
                "def step_one(state: CounterState):\n",
                "    print(\"  [Executing Step One]\")\n",
                "    new_count = state[\"count\"] + 1\n",
                "    return {\"count\": new_count, \"last_action\": \"Step One\"}\n",
                "\n",
                "def step_two(state: CounterState):\n",
                "    print(\"  [Executing Step Two]\")\n",
                "    new_count = state[\"count\"] * 2\n",
                "    return {\"count\": new_count, \"last_action\": \"Step Two\"}\n",
                "\n",
                "# Build the graph\n",
                "builder = StateGraph(CounterState)\n",
                "builder.add_node(\"step_1\", step_one)\n",
                "builder.add_node(\"step_2\", step_two)\n",
                "\n",
                "builder.add_edge(START, \"step_1\")\n",
                "builder.add_edge(\"step_1\", \"step_2\")\n",
                "builder.add_edge(\"step_2\", END)\n",
                "\n",
                "# interrupt_before step_2 to let us pause and inspect\n",
                "graph = builder.compile(checkpointer=memory_sqlite, interrupt_before=[\"step_2\"])\n",
                "print(\"Graph compiled with checkpointer and interrupt.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "169e414f",
            "metadata": {},
            "outputs": [],
            "source": [
                "config = {\"configurable\": {\"thread_id\": \"counter-demo-1\"}}\n",
                "\n",
                "# Run the graph. It should pause before step_2.\n",
                "print(\"--- Starting Run (Should pause before Step 2) ---\")\n",
                "graph.invoke({\"count\": 10, \"last_action\": \"Init\"}, config=config)\n",
                "\n",
                "# Check current state\n",
                "state = graph.get_state(config)\n",
                "print(f\"Current State Values: {state.values}\")\n",
                "print(f\"Next Step to Execute: {state.next}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5645aa08",
            "metadata": {},
            "source": [
                "### Viewing History\n",
                "Let's look at the history of this thread."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "21e996e3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get all state snapshots for this thread\n",
                "history = list(graph.get_state_history(config))\n",
                "print(f\"Found {len(history)} checkpoints.\")\n",
                "\n",
                "print(\"--- History (Newest First) ---\")\n",
                "for i, snapshot in enumerate(history):\n",
                "    print(f\"[{i}] ID: {snapshot.config['configurable']['checkpoint_id']} | Count: {snapshot.values.get('count')}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f1b3e64b",
            "metadata": {},
            "source": [
                "### The \"Fork\" (Time Travel)\n",
                "\n",
                "We are currently paused before Step 2. Step 1 added 1 to our count (10 -> 11).\n",
                "Step 2 is about to multiply by 2 (11 * 2 = 22).\n",
                "\n",
                "**Goal**: Let's go BACK to before Step 1, change the input to 50, and re-run.\n",
                "\n",
                "Note: To do this, we need the `checkpoint_id` of the state *before* step 1 ran. Usually this is the last one in the history list (the initialization)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "47a2cc14",
            "metadata": {},
            "outputs": [],
            "source": [
                "# We grab the configuration of the initial state (the last item in our history list from above)\n",
                "initial_checkpoint_config = history[-1].config\n",
                "\n",
                "print(\"Rewinding to checkpoint:\", initial_checkpoint_config['configurable']['checkpoint_id'])\n",
                "\n",
                "# We update the state at that past point with NEW values\n",
                "# This effectively creates a \"Fork\" in the history\n",
                "graph.update_state(\n",
                "    initial_checkpoint_config,\n",
                "    {\"count\": 50, \"last_action\": \"Init (Forked)\"} \n",
                ")\n",
                "print(\"State updated at past checkpoint.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3792b456",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Now we resume execution from THAT point. \n",
                "# IMPORTANT: Use config=config (current thread). The graph keys off the Thread ID,\n",
                "# but since we updated a past state, it knows to branch off from there.\n",
                "print(\"--- Resuming from Fork (Count should start at 50) ---\")\n",
                "for event in graph.stream(None, config=config):\n",
                "    print(event)\n",
                "\n",
                "print(\"\\n--- Resuming from Interrupt (Step 2) ---\")\n",
                "# Since we are at step_2 (from the fork), this runs step_2\n",
                "for event in graph.stream(None, config=config):\n",
                "    print(event)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d4f363e1",
            "metadata": {},
            "source": [
                "## Part 3: Advanced Flow Control (The `Command` Class)\n",
                "\n",
                "Normally, we define edges explicitly (`add_edge(\"a\", \"b\")`).\n",
                "The `Command` class allows nodes to decide *dynamically* where to go next, and update the state at the same time.\n",
                "This works great for multi-agent routing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "734746fc",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langgraph.types import Command\n",
                "\n",
                "class AgentState(TypedDict):\n",
                "    sentiment: str\n",
                "    decision: str\n",
                "    msg: str\n",
                "\n",
                "# A simple routing node\n",
                "def oracle(state: AgentState) -> Command[Literal[\"happy_path\", \"sad_path\"]]:\n",
                "    sentiment = state.get(\"sentiment\", \"neutral\")\n",
                "    print(f\"Oracle sees sentiment: {sentiment}\")\n",
                "    \n",
                "    if sentiment == \"happy\":\n",
                "        # Go to 'happy_path' node and update state simultaneously\n",
                "        return Command(goto=\"happy_path\", update={\"decision\": \"Stay Positive!\"})\n",
                "    else:\n",
                "        return Command(goto=\"sad_path\", update={\"decision\": \"Cheer Up!\"})\n",
                "\n",
                "def happy_path(state: AgentState):\n",
                "    return {\"msg\": \"Sunshine and rainbows!\"}\n",
                "\n",
                "def sad_path(state: AgentState):\n",
                "    return {\"msg\": \"Rainy days...\"}\n",
                "\n",
                "# Build Graph\n",
                "edgeless_builder = StateGraph(AgentState)\n",
                "edgeless_builder.add_node(\"oracle\", oracle)\n",
                "edgeless_builder.add_node(\"happy_path\", happy_path)\n",
                "edgeless_builder.add_node(\"sad_path\", sad_path)\n",
                "\n",
                "edgeless_builder.add_edge(START, \"oracle\")\n",
                "# Note: No edges defined from 'oracle' to others! The Command handles it.\n",
                "\n",
                "edgeless_graph = edgeless_builder.compile()\n",
                "print(\"Edgeless graph compiled.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0457841c",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the graph dynamically\n",
                "print(\"--- Test 1: Happy ---\")\n",
                "result_happy = edgeless_graph.invoke({\"sentiment\": \"happy\"})\n",
                "print(\"Result:\", result_happy[\"msg\"])\n",
                "\n",
                "print(\"\\n--- Test 2: Sad ---\")\n",
                "result_sad = edgeless_graph.invoke({\"sentiment\": \"sad\"})\n",
                "print(\"Result:\", result_sad[\"msg\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3e0398f2",
            "metadata": {},
            "source": [
                "## Part 4: Comprehensive Example - LinkedIn Post Generator\n",
                "\n",
                "We will combine these concepts into a workflow:\n",
                "1. **Draft**: Agent drafts a post.\n",
                "2. **Interrupt**: Human reviews it.\n",
                "3. **Feedback**: Human provides feedback.\n",
                "4. **Iterate**: Agent rewrites based on feedback (Loop back to 1)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b4c8e92b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.messages import HumanMessage, AIMessage\n",
                "from typing import Annotated\n",
                "from langgraph.graph.message import add_messages\n",
                "from langchain_groq import ChatGroq\n",
                "\n",
                "# State\n",
                "class LinkedInState(TypedDict):\n",
                "    messages: Annotated[list, add_messages]\n",
                "    # We track the 'next_action' to decide whether to finish or revise\n",
                "    next_action: str \n",
                "\n",
                "print(\"Initializing Llama 3 for LinkedIn Bot...\")\n",
                "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.7)\n",
                "\n",
                "# Node 1: Drafter\n",
                "def drafter(state: LinkedInState):\n",
                "    print(\"  [Drafter Node] Generating post...\")\n",
                "    messages = state[\"messages\"]\n",
                "    response = llm.invoke(messages)\n",
                "    return {\"messages\": [response], \"next_action\": \"review\"}\n",
                "\n",
                "# Node 2: Human Review (Simulated Logic)\n",
                "# In a real app, this node might not exist; the interrupt happens, \n",
                "# and the human interaction happens via 'update_state' before resuming.\n",
                "# But here we define a 'reviewer' node to handle the routing logic AFTER the human input.\n",
                "def reviewer(state: LinkedInState) -> Command[Literal[\"drafter\", END]]:\n",
                "    # We check the LAST message. \n",
                "    # If it's from a Human, it means feedback was provided during interrupt.\n",
                "    last_msg = state[\"messages\"][-1]\n",
                "    print(f\"  [Reviewer Node] Checking last message type: {type(last_msg)}\")\n",
                "    \n",
                "    if isinstance(last_msg, HumanMessage) and \"looks good\" in last_msg.content.lower():\n",
                "        print(\"  [Reviewer Node] Approved.\")\n",
                "        return Command(goto=END)\n",
                "    else:\n",
                "        # If there's feedback, go back to drafter\n",
                "        print(\"  [Reviewer Node] Feedback detected, sending back to Drafter.\")\n",
                "        return Command(goto=\"drafter\")\n",
                "\n",
                "# Build Graph\n",
                "post_builder = StateGraph(LinkedInState)\n",
                "post_builder.add_node(\"drafter\", drafter)\n",
                "post_builder.add_node(\"reviewer\", reviewer)\n",
                "\n",
                "post_builder.add_edge(START, \"drafter\")\n",
                "post_builder.add_edge(\"drafter\", \"reviewer\")\n",
                "\n",
                "# Interrupt BEFORE the reviewer node runs, so the human can inject feedback.\n",
                "post_graph = post_builder.compile(checkpointer=memory_sqlite, interrupt_before=[\"reviewer\"])\n",
                "print(\"LinkedIn Graph compiled.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "975eef05",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Initial Draft\n",
                "config_li = {\"configurable\": {\"thread_id\": \"li-post-1\"}}\n",
                "initial_prompt = \"Draft a LinkedIn post about the importance of AI Agents in 2025.\"\n",
                "\n",
                "print(\"--- Start Drafting ---\")\n",
                "for event in post_graph.stream({\"messages\": [(\"user\", initial_prompt)]}, config=config_li):\n",
                "    print(event)\n",
                "\n",
                "# 2. Inspect Draft\n",
                "state = post_graph.get_state(config_li)\n",
                "print(\"\\n--- Current Draft (Waiting for Review) ---\")\n",
                "print(state.values['messages'][-1].content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3a0f0d43",
            "metadata": {},
            "source": [
                "### Providing Feedback\n",
                "The graph is paused at `reviewer`. We will \"inject\" our feedback as a user message."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "41f93063",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Human provides critical feedback\n",
                "feedback = \"Too generic. Mention 'LangGraph' specifically.\"\n",
                "print(f\"Injecting Feedback: '{feedback}'\")\n",
                "\n",
                "# We update the state by adding a HumanMessage.\n",
                "# Note: 'as_node=\"reviewer\"' tells the graph that this update is happening while we are waiting for 'reviewer' node.\n",
                "post_graph.update_state(\n",
                "    config_li,\n",
                "    {\"messages\": [HumanMessage(content=feedback)]},\n",
                "    as_node=\"reviewer\"\n",
                ")\n",
                "\n",
                "# 4. Resume. \n",
                "# The 'reviewer' node will run now. It will see our feedback (HumanMessage), determine it's not approval, and routing back to 'drafter'.\n",
                "print(\"--- Resuming with Feedback ---\")\n",
                "for event in post_graph.stream(None, config=config_li):\n",
                "    print(event)\n",
                "\n",
                "# 5. Check New Draft\n",
                "state = post_graph.get_state(config_li)\n",
                "print(\"\\n--- New Draft (Waiting for Review) ---\")\n",
                "print(state.values['messages'][-1].content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "66e6a467",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Final Approval\n",
                "approval = \"Looks good!\"\n",
                "print(f\"Injecting Approval: '{approval}'\")\n",
                "\n",
                "post_graph.update_state(\n",
                "    config_li,\n",
                "    {\"messages\": [HumanMessage(content=approval)]},\n",
                "    as_node=\"reviewer\"\n",
                ")\n",
                "\n",
                "print(\"--- Resuming for Approval ---\")\n",
                "for event in post_graph.stream(None, config=config_li):\n",
                "    print(event)\n",
                "\n",
                "print(\"--- Workflow Complete ---\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
