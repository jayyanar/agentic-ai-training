{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXWVMicBM_GT"
      },
      "source": [
        "# Lab 2.4: LLM Observability with Arize Phoenix\n",
        "\n",
        "In this lab, we will learn how to instrument our LLM applications for observability using **Arize Phoenix**. Observability is crucial for understanding how your LLM applications are performing, debugging issues, and evaluating quality.\n",
        "\n",
        "We will cover:\n",
        "1.  **Setup**: Installing Phoenix and launching the local UI.\n",
        "2.  **Part 1: Simple LangChain Flow**: Instrumenting a basic chain.\n",
        "3.  **Part 2: Advanced LangGraph Application**: Instrumenting a stateful agent workflow.\n",
        "\n",
        "### Prerequisites\n",
        "- Ensure you have your Groq API Key ready.\n"
      ],
      "id": "iXWVMicBM_GT"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "312def0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "312def0f",
        "outputId": "bd765668-c919-45f7-b5ce-cd7773cc7cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.8/161.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m432.2/432.2 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m238.7/238.7 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 1. Install Dependencies\n",
        "%pip install -qU arize-phoenix arize-otel openinference-instrumentation-langchain langchain langchain-groq langgraph\n",
        "%pip install -q \"arize[AutoEmbeddings]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5ab4658e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ab4658e",
        "outputId": "ceb438e7-b0a2-4663-d24f-194e05412157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter your Arize Space ID: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter your Arize API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "# 2. Setup API Keys\n",
        "import getpass\n",
        "import os\n",
        "from arize.otel import register\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "if \"ARIZE_SPACE_ID\" not in os.environ:\n",
        "    os.environ[\"ARIZE_SPACE_ID\"] = getpass.getpass(\"Enter your Arize Space ID: \")\n",
        "\n",
        "if \"ARIZE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"ARIZE_API_KEY\"] = getpass.getpass(\"Enter your Arize API Key: \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fd9737",
      "metadata": {
        "id": "47fd9737"
      },
      "source": [
        "## 3. Launch Arize Phoenix (Optional)\n",
        "You can still launch the Phoenix application locally to view traces locally, or you can view them in the Arize Cloud dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "692d2020",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "692d2020",
        "outputId": "6d3476f7-76a3-472c-db65-a7294b3678a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”­ OpenTelemetry Tracing Details ğŸ”­\n",
            "|  Arize Project: arize-lab-demo\n",
            "|  Span Processor: BatchSpanProcessor\n",
            "|  Collector Endpoint: otlp.arize.com\n",
            "|  Transport: gRPC\n",
            "|  Transport Headers: {'authorization': '****', 'api_key': '****', 'arize-space-id': '****', 'space_id': '****', 'arize-interface': '****'}\n",
            "|  \n",
            "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
            "|  \n",
            "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
            "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
            "\n",
            "ğŸ”­ Arize observability configured successfully!\n",
            "ğŸ“Š All LangChain operations will now be automatically traced\n"
          ]
        }
      ],
      "source": [
        "# Import Arize observability components\n",
        "from arize.otel import register\n",
        "from getpass import getpass\n",
        "\n",
        "# Configure Arize tracing\n",
        "tracer_provider = register(\n",
        "    space_id=os.environ[\"ARIZE_SPACE_ID\"],\n",
        "    api_key=os.environ[\"ARIZE_API_KEY\"],\n",
        "    project_name=\"arize-lab-demo\"\n",
        ")\n",
        "\n",
        "# Enable automatic LangChain instrumentation\n",
        "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "\n",
        "print(\"ğŸ”­ Arize observability configured successfully!\")\n",
        "print(\"ğŸ“Š All LangChain operations will now be automatically traced\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c3823c",
      "metadata": {
        "id": "e2c3823c"
      },
      "source": [
        "## Part 1: Simple LangChain Flow (Banking Policy Assistant)\n",
        "\n",
        "We will create a simple **Banking Policy Assistant** that answers internal policy questions. This simulates a basic RAG or Chatbot interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "73e99769",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73e99769",
        "outputId": "698892d2-68e7-4d69-876e-4ddd55f0ecf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invoking Banking Policy Assistant...\n",
            "Question: What is the maximum loan-to-value (LTV) ratio for a jumbo mortgage?\n",
            "Response: The maximum loan-to-value (LTV) ratio for a jumbo mortgage typically ranges between **80% and 90%**, depending on the lender and the borrower's financial profile. Most standard jumbo loan programs require a minimum down payment of **10% to 20%** (i.e., an LTV of 90% to 80%) to mitigate the higher risk associated with these larger-than-conforming loans. \n",
            "\n",
            "However, some lenders may offer higher LTV ratios (e.g., up to 95% or 97%) for borrowers with exceptional credit scores, stable income, and low debt-to-income ratios. These exceptions often come with stricter underwriting criteria and may require private mortgage insurance (PMI) or other risk-mitigation measures. Always confirm specific requirements with the lender, as policies can vary.\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Setup a Banking Policy Assistant\n",
        "llm = ChatGroq(\n",
        "    model=\"qwen/qwen3-32b\",\n",
        "    temperature=0,\n",
        "    reasoning_format=\"parsed\"\n",
        ")\n",
        "\n",
        "# This prompt simulates an internal knowledge base lookup\n",
        "policy_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a Banking Policy Assistant. Answer the question based on standard mortgage guidelines. Keep it professional. Question: {question}\"\n",
        ")\n",
        "chain = policy_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Invoke the chain\n",
        "print(\"Invoking Banking Policy Assistant...\")\n",
        "question = \"What is the maximum loan-to-value (LTV) ratio for a jumbo mortgage?\"\n",
        "print(f\"Question: {question}\")\n",
        "response = chain.invoke({\"question\": question})\n",
        "print(f\"Response: {response}\")\n",
        "\n",
        "# Check the Phoenix UI and Arize Cloud to see the trace!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b58211ff",
      "metadata": {
        "id": "b58211ff"
      },
      "source": [
        "## Part 2: Advanced LangGraph Application (Loan Underwriting)\n",
        "\n",
        "Now, let's look at a complex **Loan Underwriting Pipeline**. This involves multiple steps: Risk Analysis and Final Decision.\n",
        "\n",
        "We will use LangGraph to model this stateful workflow, where a **Risk Analyst** assesses the applicant and a **Senior Underwriter** makes the final call."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a84872a",
      "metadata": {
        "id": "9a84872a"
      },
      "source": [
        "### 2.1 Define State\n",
        "We define a complex state capable of holding the applicant profile and the calculated risk score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c3ad8a1b",
      "metadata": {
        "id": "c3ad8a1b"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Sequence\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "\n",
        "# --- 2. STATE DEFINITION ---\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    applicant_id: str\n",
        "    risk_score: str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c569d7f7",
      "metadata": {
        "id": "c569d7f7"
      },
      "source": [
        "### 2.2 Define Risk Analyst Node\n",
        "This node analyzes the financial data and assigns a risk level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e9a70903",
      "metadata": {
        "id": "e9a70903"
      },
      "outputs": [],
      "source": [
        "# Define Nodes\n",
        "def risk_analyst_node(state: State):\n",
        "    \"\"\"Analyzes the financial health of the applicant.\"\"\"\n",
        "    print(\"--- Node: Risk Analyst ---\")\n",
        "    applicant_data = state[\"messages\"][-1].content\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are a Risk Analyst for a bank. Analyze the provided applicant financial data \"\n",
        "        \"(Income, Debt, Credit Score). Calculate the Debt-to-Income (DTI) ratio. \"\n",
        "        \"Summarize the financial health and assign a risk level (Low, Medium, High).\"\n",
        "    )\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_msg),\n",
        "        (\"human\", \"Applicant Data: {data}\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"data\": applicant_data})\n",
        "\n",
        "    # Extract a dummy risk score to update state (simplification)\n",
        "    # In a real app, we'd use structured output\n",
        "    return {\"messages\": [response], \"risk_score\": \"Calculated\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f88dec",
      "metadata": {
        "id": "f5f88dec"
      },
      "source": [
        "### 2.3 Verification: Test Risk Analyst\n",
        "Let's ensure the risk analyst can corectly interpret data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f22148fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f22148fc",
        "outputId": "3c2a6b45-dfa6-4d14-8d26-b943c537f259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Risk Analyst Node...\n",
            "--- Node: Risk Analyst ---\n",
            "Analysis: **Analysis Summary:**  \n",
            "- **Debt-to-Income (DTI) Ratio:**  \n",
            "  Monthly Debt: $2,000 | Monthly Income: $100,000 Ã· 12 = **~$8,333**  \n",
            "  DTI = ($2,000 Ã· $8,333) Ã— 100 = **~24%**  \n",
            "\n",
            "- **Financial Health:**  \n",
            "  - **DTI Interpretation:** 24% falls within the **acceptable range** (20â€“36%), indicating manageable debt relative to income.  \n",
            "  - **Credit Score:** 700 is a **good score** (700â€“749 is considered \"good\" in most credit scoring models).  \n",
            "\n",
            "**Risk Level:** **Low**  \n",
            "The applicant demonstrates strong financial health with a low DTI and a solid credit score, suggesting a low risk of default.\n"
          ]
        }
      ],
      "source": [
        "# Verification\n",
        "print(\"Testing Risk Analyst Node...\")\n",
        "sample_data = \"Income: $100k, Debt: $2k/mo, Credit: 700\"\n",
        "state = {\"messages\": [HumanMessage(content=sample_data)]}\n",
        "result = risk_analyst_node(state)\n",
        "print(\"Analysis:\", result['messages'][0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9761d64",
      "metadata": {
        "id": "b9761d64"
      },
      "source": [
        "### 2.4 Define Senior Underwriter Node\n",
        "This node takes the risk analysis and makes the final decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8a1a39bd",
      "metadata": {
        "id": "8a1a39bd"
      },
      "outputs": [],
      "source": [
        "def senior_underwriter_node(state: State):\n",
        "    \"\"\"Makes the final loan decision based on risk analysis.\"\"\"\n",
        "    print(\"--- Node: Senior Underwriter ---\")\n",
        "    risk_analysis = state[\"messages\"][-1].content\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are a Senior Underwriter. Review the Risk Analyst's report. \"\n",
        "        \"Make a final decision: APPROVE or DENY. \"\n",
        "        \"If approved, set the interest rate tier (Tier 1 is best). \"\n",
        "        \"Provide a brief justification for the decision.\"\n",
        "    )\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_msg),\n",
        "        (\"human\", \"Risk Analysis Report: {report}\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"report\": risk_analysis})\n",
        "    return {\"messages\": [response]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfa4059d",
      "metadata": {
        "id": "bfa4059d"
      },
      "source": [
        "### 2.5 Build and Compile Graph\n",
        "Connect the nodes to form the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "17e5a93c",
      "metadata": {
        "id": "17e5a93c"
      },
      "outputs": [],
      "source": [
        "# Build Graph\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "# Add Nodes\n",
        "workflow.add_node(\"risk_analyst\", risk_analyst_node)\n",
        "workflow.add_node(\"senior_underwriter\", senior_underwriter_node)\n",
        "\n",
        "# Define Logic\n",
        "workflow.add_edge(START, \"risk_analyst\")\n",
        "workflow.add_edge(\"risk_analyst\", \"senior_underwriter\")\n",
        "workflow.add_edge(\"senior_underwriter\", END)\n",
        "\n",
        "# Compile\n",
        "graph = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2379c688",
      "metadata": {
        "id": "2379c688"
      },
      "source": [
        "### 2.6 Run the Pipeline\n",
        "Now we run the full flow and can observe the trace in Arize Phoenix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "38b2a185",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38b2a185",
        "outputId": "98fd9183-7d73-4d12-c615-6ebba21fd9bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting Loan Underwriting Pipeline...\n",
            "Processing Application: Applicant: John Doe. Income: $150,000/yr. Monthly Debt: $2,500. Credit Score: 780. Loan Amount: $500,000.\n",
            "\n",
            "--- Node: Risk Analyst ---\n",
            "\n",
            "--- NODE: RISK_ANALYST ---\n",
            "Content: **Financial Analysis for John Doe**  \n",
            "\n",
            "### **Key Calculations**  \n",
            "1. **Debt-to-Income (DTI) Ratio**:  \n",
            "   - **Monthly Gross Income**: $150,000 / 12 = **$12,500**  \n",
            "   - **DTI**: ($2,500 Monthly Debt / $12,500 Monthly Income) Ã— 100 = **20%**  \n",
            "\n",
            "2. **Loan-to-Income Ratio**:  \n",
            "   - **Loan Amount / Annu...\n",
            "--- Node: Senior Underwriter ---\n",
            "\n",
            "--- NODE: SENIOR_UNDERWRITER ---\n",
            "Content: **Decision**: APPROVE  \n",
            "**Interest Rate Tier**: Tier 1  \n",
            "\n",
            "**Justification**:  \n",
            "John Doe exhibits a **low-risk profile** with a 20% DTI, excellent credit score (780), and a strong income ($150,000 annually). While the loan-to-income ratio is high (3.33x), his robust financial capacity and disciplined...\n",
            "\n",
            "âœ… Pipeline Complete. Check Phoenix UI for the trace lineage.\n"
          ]
        }
      ],
      "source": [
        "# Invoke Graph\n",
        "print(\"ğŸš€ Starting Loan Underwriting Pipeline...\")\n",
        "\n",
        "applicant_info = \"Applicant: John Doe. Income: $150,000/yr. Monthly Debt: $2,500. Credit Score: 780. Loan Amount: $500,000.\"\n",
        "print(f\"Processing Application: {applicant_info}\\n\")\n",
        "\n",
        "inputs = {\n",
        "    \"messages\": [HumanMessage(content=applicant_info)],\n",
        "    \"applicant_id\": \"APP-12345\"\n",
        "}\n",
        "\n",
        "for output in graph.stream(inputs, stream_mode=\"updates\"):\n",
        "    for node_name, node_state in output.items():\n",
        "        print(f\"\\n--- NODE: {node_name.upper()} ---\")\n",
        "        last_msg = node_state[\"messages\"][-1]\n",
        "        print(f\"Content: {last_msg.content[:300]}...\")\n",
        "\n",
        "print(\"\\nâœ… Pipeline Complete. Check Phoenix UI for the trace lineage.\")\n",
        "\n",
        "# Check Phoenix UI again. You should see a trace for this execution showing the graph steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8221466c",
      "metadata": {
        "id": "8221466c"
      },
      "source": [
        "## Part 3: High-Level Chaining with Runnables (Customer Feedback Processor)\n",
        "\n",
        "In this section, we will see how Arize Phoenix shines when debugging complex chains using `RunnablePassthrough` and `RunnableLambda`.\n",
        "We often want to pass data through multiple steps, augmenting it along the way.\n",
        "\n",
        "We will build a **Customer Feedback Processor** that:\n",
        "1.  Takes a raw review.\n",
        "2.  **Cleans** it (custom function).\n",
        "3.  **Analyzes Sentiment** (LLM call).\n",
        "4.  **Extracts Keywords** (LLM call).\n",
        "5.  **Generates a Summary Response** using all previous inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "part3_chain",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "part3_chain",
        "outputId": "58789a22-03aa-4333-baa1-ab219cb53fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Processing Customer Feedback...\n",
            "\n",
            "Original: '  The product arrived late and was broken. Terrible service!  '\n",
            "Cleaned: 'the product arrived late and was broken. terrible service!'\n",
            "Sentiment: NEGATIVE\n",
            "Keywords: late, broken, terrible service\n",
            "\n",
            "Final Response generated:\n",
            "**Response:**  \n",
            "Dear [Customer Name],  \n",
            "\n",
            "Thank you for taking the time to share your feedback. We sincerely apologize for the inconvenience you experienced with your orderâ€”both the delayed delivery and the damaged product are unacceptable, and we deeply regret the frustration this caused.  \n",
            "\n",
            "To resolve this, we will immediately dispatch a replacement at no cost to you and expedite the shipping to ensure it arrives promptly. Additionally, weâ€™d like to investigate the shipping process to identify where things went wrong and prevent future occurrences.  \n",
            "\n",
            "Your satisfaction is our priority, and weâ€™re committed to improving our service. Please reply to this email with your order number so we can escalate your case and provide a full refund for any additional costs or inconveniences incurred.  \n",
            "\n",
            "Thank you for giving us the opportunity to make things right. We value your trust and hope to regain your confidence.  \n",
            "\n",
            "Best regards,  \n",
            "[Your Name]  \n",
            "Customer Success Manager  \n",
            "[Company Name]  \n",
            "[Contact Information]  \n",
            "\n",
            "---  \n",
            "**Key Elements Addressed:**  \n",
            "- **Empathy & Accountability:** Acknowledges frustration and takes responsibility.  \n",
            "- **Actionable Solutions:** Offers replacement, expedited shipping, and a refund.  \n",
            "- **Improvement Commitment:** Pledges to review processes to prevent recurrence.  \n",
            "- **Call to Action:** Encourages follow-up for resolution.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "# 1. Define specific purpose chains\n",
        "sentiment_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Analyze the sentiment of this review. Return ONLY one word: POSITIVE, NEGATIVE, or NEUTRAL. Review: {cleaned_review}\"\n",
        ")\n",
        "sentiment_chain = sentiment_prompt | llm | StrOutputParser()\n",
        "\n",
        "keyword_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Extract top 3 keywords from this review. Return them as a comma-separated list. Review: {cleaned_review}\"\n",
        ")\n",
        "keyword_chain = keyword_prompt | llm | StrOutputParser()\n",
        "\n",
        "final_response_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a Customer Success Manager. Write a generic response to this review based on the analysis.\\n\"\n",
        "    \"Review: {cleaned_review}\\n\"\n",
        "    \"Sentiment: {sentiment}\\n\"\n",
        "    \"Keywords: {keywords}\\n\\n\"\n",
        "    \"Response:\"\n",
        ")\n",
        "final_response_chain = final_response_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 2. Define custom helper (RunnableLambda)\n",
        "def clean_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "# 3. Build the Super Chain with Passthrough\n",
        "# The input to the chain is just the raw key \"review\"\n",
        "super_chain = (\n",
        "    {\"cleaned_review\": lambda x: clean_text(x[\"review\"])}\n",
        "    | RunnablePassthrough.assign(sentiment=sentiment_chain)\n",
        "    | RunnablePassthrough.assign(keywords=keyword_chain)\n",
        "    | RunnablePassthrough.assign(display_result=final_response_chain)\n",
        ")\n",
        "\n",
        "# Invoke\n",
        "print(\"ğŸš€ Processing Customer Feedback...\")\n",
        "review_text = \"  The product arrived late and was broken. Terrible service!  \"\n",
        "result = super_chain.invoke({\"review\": review_text})\n",
        "\n",
        "print(f\"\\nOriginal: '{review_text}'\")\n",
        "print(f\"Cleaned: '{result['cleaned_review']}'\")\n",
        "print(f\"Sentiment: {result['sentiment']}\")\n",
        "print(f\"Keywords: {result['keywords']}\")\n",
        "print(f\"\\nFinal Response generated:\\n{result['display_result']}\")\n",
        "\n",
        "# Check Arize Phoenix! You will see the 'sentiment_chain' and 'keyword_chain'\n",
        "# as separate spans running in parallel (conceptually) or sequentially,\n",
        "# feeding into the final step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7c2665d",
      "metadata": {
        "id": "a7c2665d"
      },
      "source": [
        "## Conclusion\n",
        "You have successfully set up Arize Phoenix and instrumented both a **Banking Policy Assistant** and a **Loan Underwriting Pipeline**.\n",
        "Review the traces in the Phoenix UI to understand the internal execution of your Critical Financial Workflows."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}