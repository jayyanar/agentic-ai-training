{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.2: Human-in-the-Loop (HITL) Basics\n",
    "\n",
    "In this lab, we will implementation a **Real-World Customer Support Scenario**.\n",
    "\n",
    "### The Scenario\n",
    "We have an AI Agent that drafts responses to customer queries.\n",
    "However, before sending the email, a **Human Manager** (you) must review and approve the draft.\n",
    "If the draft is not good, the human can **edit** it before it is sent.\n",
    "\n",
    "### Key Concepts\n",
    "1. **Interrupts**: Pausing the graph before the email sending node.\n",
    "2. **State Inspection**: Reviewing the AI's draft.\n",
    "3. **State Update**: Manually correcting the draft.\n",
    "4. **Resuming**: Allowing the \"Send Email\" node to execute with the corrected text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "%pip install -qU langchain-groq langchain-community langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup API Keys\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Support Bot Graph\n",
    "\n",
    "- **State**: Takes a `customer_query` and stores a `draft_response`.\n",
    "- **Node 1 (`draft_reply`)**: Uses an LLM to generate a draft.\n",
    "- **Node 2 (`send_email`)**: Simulates sending the final email.\n",
    "- **Interrupt**: We will `interrupt_before` the `send_email` node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0b223",
   "metadata": {},
   "source": [
    "### 3.1 Define State and Imports\n",
    "We define the `SupportState` which holds the customer query, the draft response, and the final status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ff480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Optional\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# --- 1. Define State ---\n",
    "class SupportState(TypedDict):\n",
    "    customer_query: str\n",
    "    draft_response: Optional[str]\n",
    "    final_status: Optional[str]\n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561267f",
   "metadata": {},
   "source": [
    "### 3.2 Define Drafting Node\n",
    "The `draft_reply` node uses the LLM to generate an initial email draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_reply(state: SupportState):\n",
    "    print(\"--- (AI) Drafting Reply ---\")\n",
    "    query = state['customer_query']\n",
    "    # Ask LLM to be polite and concise\n",
    "    response = llm.invoke(f\"Write a polite, concise support email response to this customer complaint: {query}\")\n",
    "    return {\"draft_response\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592b75d",
   "metadata": {},
   "source": [
    "### 3.3 Verification: Test Drafting Node\n",
    "Let's test the drafting capability independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67bafd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "print(\"Testing draft_reply node...\")\n",
    "dummy_state = {\"customer_query\": \"I received a broken product.\"}\n",
    "draft_result = draft_reply(dummy_state)\n",
    "print(\"Draft:\", draft_result['draft_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9f19a",
   "metadata": {},
   "source": [
    "### 3.4 Define Send Email Node\n",
    "This node simulates sending the email. In a real app, this would use an email API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17358469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email(state: SupportState):\n",
    "    print(\"--- (System) Sending Email ---\")\n",
    "    draft = state['draft_response']\n",
    "    # Simulate sending\n",
    "    return {\"final_status\": f\"SENT: {draft}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12059c43",
   "metadata": {},
   "source": [
    "### 3.5 Build and Compile Graph\n",
    "We assemble the graph and add the **Interrupt** before the sending step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cfd64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Build Graph ---\n",
    "builder = StateGraph(SupportState)\n",
    "builder.add_node(\"draft_reply\", draft_reply)\n",
    "builder.add_node(\"send_email\", send_email)\n",
    "\n",
    "builder.add_edge(START, \"draft_reply\")\n",
    "builder.add_edge(\"draft_reply\", \"send_email\")\n",
    "builder.add_edge(\"send_email\", END)\n",
    "\n",
    "# --- 4. Compile with Interrupt ---\n",
    "memory = MemorySaver()\n",
    "\n",
    "# This is the key line: Pause BEFORE executing 'send_email'\n",
    "graph = builder.compile(checkpointer=memory, interrupt_before=[\"send_email\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run the Workflow (Phase 1: Drafting)\n",
    "We start with a customer complaint about a broken widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_config = {\"configurable\": {\"thread_id\": \"ticket-123\"}}\n",
    "\n",
    "initial_input = {\n",
    "    \"customer_query\": \"My widget arrived broken! I am very angry. Fix this now!\"\n",
    "}\n",
    "\n",
    "# Invoking the graph. It should STOP after 'draft_reply'.\n",
    "print(\"Starting workflow...\")\n",
    "for event in graph.stream(initial_input, config=thread_config):\n",
    "    print(event)\n",
    "\n",
    "print(\"\\nWorkflow paused?\", graph.get_state(thread_config).next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Human Review (Inspection)\n",
    "The workflow is paused. As a manager, you need to check what the AI wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = graph.get_state(thread_config)\n",
    "ai_draft = current_state.values['draft_response']\n",
    "\n",
    "print(\"--- Current AI Draft ---\")\n",
    "print(ai_draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Human Correction (Update State)\n",
    "Imagine the AI was too generic. We want to add a specific promise, e.g., \"We will ship a replacement immediately.\"\n",
    "We use `update_state` to overwrite the `draft_response`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_draft = ai_draft + \"\\n\\nP.S. A replacement unit has been shipped via overnight delivery.\"\n",
    "\n",
    "print(\"--- Updating Draft to ---\")\n",
    "print(new_draft)\n",
    "\n",
    "graph.update_state(\n",
    "    thread_config,\n",
    "    {\"draft_response\": new_draft}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Resume Execution\n",
    "Now that the state is corrected, we resume. The graph will pick up at `send_email` using our *new* draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resuming workflow...\")\n",
    "for event in graph.stream(None, config=thread_config):\n",
    "    print(event)\n",
    "\n",
    "# Check final result\n",
    "final_state = graph.get_state(thread_config)\n",
    "print(\"\\nFinal Status:\", final_state.values.get('final_status'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
