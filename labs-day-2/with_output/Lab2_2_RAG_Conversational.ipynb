{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 1.2: Conversational RAG - Mortgage Policy Assistant\n",
                "\n",
                "In this lab, we will build a **Banking Policy Assistant** for Wells Fargo. This assistant allows loan officers to query internal mortgage guidelines (e.g., Conforming vs. Jumbo loans) and ask follow-up questions while maintaining conversational context.\n",
                "\n",
                "## Use Case\n",
                "A loan officer needs to quickly check credit score requirements for different loan types without searching through a 200-page PDF manual. They might ask:\n",
                "1. \"What is the min credit score for a Jumbo loan?\"\n",
                "2. \"How does that compare to FHA?\" (Contextual follow-up)\n",
                "\n",
                "## Key Concepts\n",
                "1. **History Aware Retriever**: Rephrases the user's latest query using the chat history so it makes sense as a standalone query to the vector store.\n",
                "2. **Chat History**: Maintaining line of conversation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "78c4a6b3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "print(\"Installing dependencies...\")\n",
                "%pip install -qU langchain langchain-groq langchain-community langchain-huggingface chromadb sentence-transformers\n",
                "print(\"Dependencies installed.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2059d7b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Setup API Keys\n",
                "import getpass\n",
                "import os\n",
                "\n",
                "if \"GROQ_API_KEY\" not in os.environ:\n",
                "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "29be1e50",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Setup Vector Store (Mortgage Policy Data)\n",
                "from langchain_community.document_loaders import TextLoader\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from langchain_community.vectorstores import Chroma\n",
                "\n",
                "# Load Internal Policy Document\n",
                "print(\"Loading mortgage policy document...\")\n",
                "loader = TextLoader(\"./data/mortgage_policy.txt\")\n",
                "docs = loader.load()\n",
                "print(f\"Loaded {len(docs)} document(s).\")\n",
                "\n",
                "# Split\n",
                "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
                "splits = text_splitter.split_documents(docs)\n",
                "print(f\"Split into {len(splits)} chunks.\")\n",
                "\n",
                "# Embed & Store\n",
                "print(\"Creating vector store...\")\n",
                "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
                "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
                "retriever = vectorstore.as_retriever()\n",
                "print(\"Vector store created.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3e4ec8a5",
            "metadata": {},
            "source": [
                "## 4. History Aware Retriever\n",
                "We need a chain that takes the `chat_history` and the `input` and generates a search query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db3b57e8",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_groq import ChatGroq\n",
                "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.runnables import RunnablePassthrough\n",
                "\n",
                "# Initialize LLM\n",
                "llm = ChatGroq(\n",
                "    model=\"qwen/qwen3-32b\",\n",
                "    temperature=0,\n",
                "    reasoning_format=\"parsed\"\n",
                ")\n",
                "\n",
                "contextualize_q_system_prompt = (\n",
                "    \"Given a chat history and the latest user question \"\n",
                "    \"which might reference context in the chat history, \"\n",
                "    \"formulate a standalone question which can be understood \"\n",
                "    \"without the chat history. Do NOT answer the question, \"\n",
                "    \"just reformulate it if needed and otherwise return it as is.\"\n",
                ")\n",
                "\n",
                "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\"system\", contextualize_q_system_prompt),\n",
                "        MessagesPlaceholder(\"chat_history\"),\n",
                "        (\"human\", \"{input}\"),\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Chain to rephrase question\n",
                "contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()\n",
                "print(\"Contextualization chain created.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "test_context",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's test the contextualization to understand what it does\n",
                "from langchain_core.messages import HumanMessage, AIMessage\n",
                "\n",
                "print(\"--- Testing Query Contextualization ---\")\n",
                "# Mock history: user asked about Jumbo loans, AI answered.\n",
                "sample_history = [\n",
                "    HumanMessage(content=\"What is the min credit score for a Jumbo Loan?\"),\n",
                "    AIMessage(content=\"The minimum credit score is 700.\")\n",
                "]\n",
                "# User asks a follow-up specific to the context (referring to 'that')\n",
                "sample_input = \"How does that compare to FHA?\"\n",
                "print(f\"Chat History: {len(sample_history)} messages\")\n",
                "print(f\"User Follow-up: {sample_input}\")\n",
                "\n",
                "rephrased_query = contextualize_q_chain.invoke({\"chat_history\": sample_history, \"input\": sample_input})\n",
                "print(f\"Rephrased Query (for Vector Store): {rephrased_query}\")\n",
                "print(\"---------------------------------------\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "61f7fda3",
            "metadata": {},
            "source": [
                "## 5. QA Chain with History\n",
                "Now we create the final chain that uses the retrieved documents to answer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b519c13b",
            "metadata": {},
            "outputs": [],
            "source": [
                "def format_docs(docs):\n",
                "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
                "\n",
                "qa_system_prompt = (\n",
                "    \"You are a specialized Mortgage Policy Assistant for Wells Fargo. \"\n",
                "    \"Use the following pieces of retrieved policy context to answer \"\n",
                "    \"the loan officer's question. If you don't know the answer, say that you \"\n",
                "    \"cannot find it in the policy. Keep answers professional and concise.\"\n",
                "    \"\\n\\n\"\n",
                "    \"{context}\"\n",
                ")\n",
                "\n",
                "qa_prompt = ChatPromptTemplate.from_messages(\n",
                "    [\n",
                "        (\"system\", qa_system_prompt),\n",
                "        MessagesPlaceholder(\"chat_history\"),\n",
                "        (\"human\", \"{input}\"),\n",
                "    ]\n",
                ")\n",
                "\n",
                "def contextualized_question(input: dict):\n",
                "    if input.get(\"chat_history\"):\n",
                "        return contextualize_q_chain\n",
                "    else:\n",
                "        return input.get(\"input\")\n",
                "\n",
                "rag_chain = (\n",
                "    RunnablePassthrough.assign(\n",
                "        context=contextualized_question | retriever | format_docs\n",
                "    )\n",
                "    | qa_prompt\n",
                "    | llm\n",
                "    | StrOutputParser()\n",
                ")\n",
                "print(\"Full RAG chain created.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1bcfd4f3",
            "metadata": {},
            "source": [
                "## 6. Testing the Chat\n",
                "We can now manage specific chat sessions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0cb85cda",
            "metadata": {},
            "outputs": [],
            "source": [
                "chat_history = []\n",
                "\n",
                "# First Question: Specific Policy Query\n",
                "user_input = \"What is the minimum credit score for a Jumbo Loan?\"\n",
                "\n",
                "print(f\"User: {user_input}\")\n",
                "# rag_chain returns string now\n",
                "answer = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
                "print(f\"AI: {answer}\")\n",
                "\n",
                "# Update History\n",
                "chat_history.extend([HumanMessage(content=user_input), AIMessage(content=answer)])\n",
                "\n",
                "# Second Question (Follow-up): Contextual Comparison\n",
                "print(\"\\n--- Follow up ---\")\n",
                "user_input = \"How does that compare to FHA loans?\"\n",
                "print(f\"User: {user_input}\")\n",
                "\n",
                "answer = rag_chain.invoke({\"input\": user_input, \"chat_history\": chat_history})\n",
                "print(f\"AI: {answer}\")\n",
                "\n",
                "chat_history.extend([HumanMessage(content=user_input), AIMessage(content=answer)])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}