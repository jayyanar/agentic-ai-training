{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.4: LLM Observability with Arize Phoenix\n",
    "\n",
    "In this lab, we will learn how to instrument our LLM applications for observability using **Arize Phoenix**. Observability is crucial for understanding how your LLM applications are performing, debugging issues, and evaluating quality.\n",
    "\n",
    "We will cover:\n",
    "1.  **Setup**: Installing Phoenix and launching the local UI.\n",
    "2.  **Part 1: Simple LangChain Flow**: Instrumenting a basic chain.\n",
    "3.  **Part 2: Advanced LangGraph Application**: Instrumenting a stateful agent workflow.\n",
    "\n",
    "### Prerequisites\n",
    "- Ensure you have your Groq API Key ready.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312def0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "%pip install -qU arize-phoenix arize-otel openinference-instrumentation-langchain langchain langchain-groq langgraph\n",
    "%pip install -q \"arize[AutoEmbeddings]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab4658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup API Keys\n",
    "import getpass\n",
    "import os\n",
    "from arize.otel import register\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
    "\n",
    "if \"ARIZE_SPACE_ID\" not in os.environ:\n",
    "    os.environ[\"ARIZE_SPACE_ID\"] = getpass.getpass(\"Enter your Arize Space ID: \")\n",
    "\n",
    "if \"ARIZE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"ARIZE_API_KEY\"] = getpass.getpass(\"Enter your Arize API Key: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd9737",
   "metadata": {},
   "source": [
    "## 3. Launch Arize Phoenix (Optional)\n",
    "You can still launch the Phoenix application locally to view traces locally, or you can view them in the Arize Cloud dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "692d2020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Arize observability components\n",
    "from arize.otel import register\n",
    "from getpass import getpass\n",
    "\n",
    "# Configure Arize tracing\n",
    "tracer_provider = register(\n",
    "    space_id=os.environ[\"ARIZE_SPACE_ID\"],\n",
    "    api_key=os.environ[\"ARIZE_API_KEY\"],\n",
    "    project_name=\"arize-lab-demo\"\n",
    ")\n",
    "\n",
    "# Enable automatic LangChain instrumentation\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "\n",
    "print(\"ðŸ”­ Arize observability configured successfully!\")\n",
    "print(\"ðŸ“Š All LangChain operations will now be automatically traced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3823c",
   "metadata": {},
   "source": [
    "## Part 1: Simple LangChain Flow (Banking Policy Assistant)\n",
    "\n",
    "We will create a simple **Banking Policy Assistant** that answers internal policy questions. This simulates a basic RAG or Chatbot interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e99769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Setup a Banking Policy Assistant\n",
    "llm = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    temperature=0,\n",
    "    reasoning_format=\"parsed\"\n",
    ")\n",
    "\n",
    "# This prompt simulates an internal knowledge base lookup\n",
    "policy_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a Banking Policy Assistant. Answer the question based on standard mortgage guidelines. Keep it professional. Question: {question}\"\n",
    ")\n",
    "chain = policy_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke the chain\n",
    "print(\"Invoking Banking Policy Assistant...\")\n",
    "question = \"What is the maximum loan-to-value (LTV) ratio for a jumbo mortgage?\"\n",
    "print(f\"Question: {question}\")\n",
    "response = chain.invoke({\"question\": question})\n",
    "print(f\"Response: {response}\")\n",
    "\n",
    "# Check the Phoenix UI and Arize Cloud to see the trace!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58211ff",
   "metadata": {},
   "source": [
    "## Part 2: Advanced LangGraph Application (Loan Underwriting)\n",
    "\n",
    "Now, let's look at a complex **Loan Underwriting Pipeline**. This involves multiple steps: Risk Analysis and Final Decision.\n",
    "\n",
    "We will use LangGraph to model this stateful workflow, where a **Risk Analyst** assesses the applicant and a **Senior Underwriter** makes the final call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84872a",
   "metadata": {},
   "source": [
    "### 2.1 Define State\n",
    "We define a complex state capable of holding the applicant profile and the calculated risk score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "# --- 2. STATE DEFINITION ---\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    applicant_id: str\n",
    "    risk_score: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c569d7f7",
   "metadata": {},
   "source": [
    "### 2.2 Define Risk Analyst Node\n",
    "This node analyzes the financial data and assigns a risk level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a70903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Nodes\n",
    "def risk_analyst_node(state: State):\n",
    "    \"\"\"Analyzes the financial health of the applicant.\"\"\"\n",
    "    print(\"--- Node: Risk Analyst ---\")\n",
    "    applicant_data = state[\"messages\"][-1].content\n",
    "    \n",
    "    system_msg = (\n",
    "        \"You are a Risk Analyst for a bank. Analyze the provided applicant financial data \"\n",
    "        \"(Income, Debt, Credit Score). Calculate the Debt-to-Income (DTI) ratio. \"\n",
    "        \"Summarize the financial health and assign a risk level (Low, Medium, High).\"\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_msg),\n",
    "        (\"human\", \"Applicant Data: {data}\")\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"data\": applicant_data})\n",
    "    \n",
    "    # Extract a dummy risk score to update state (simplification)\n",
    "    # In a real app, we'd use structured output\n",
    "    return {\"messages\": [response], \"risk_score\": \"Calculated\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f88dec",
   "metadata": {},
   "source": [
    "### 2.3 Verification: Test Risk Analyst\n",
    "Let's ensure the risk analyst can corectly interpret data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22148fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "print(\"Testing Risk Analyst Node...\")\n",
    "sample_data = \"Income: $100k, Debt: $2k/mo, Credit: 700\"\n",
    "state = {\"messages\": [HumanMessage(content=sample_data)]}\n",
    "result = risk_analyst_node(state)\n",
    "print(\"Analysis:\", result['messages'][0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9761d64",
   "metadata": {},
   "source": [
    "### 2.4 Define Senior Underwriter Node\n",
    "This node takes the risk analysis and makes the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def senior_underwriter_node(state: State):\n",
    "    \"\"\"Makes the final loan decision based on risk analysis.\"\"\"\n",
    "    print(\"--- Node: Senior Underwriter ---\")\n",
    "    risk_analysis = state[\"messages\"][-1].content\n",
    "    \n",
    "    system_msg = (\n",
    "        \"You are a Senior Underwriter. Review the Risk Analyst's report. \"\n",
    "        \"Make a final decision: APPROVE or DENY. \"\n",
    "        \"If approved, set the interest rate tier (Tier 1 is best). \"\n",
    "        \"Provide a brief justification for the decision.\"\n",
    "    )\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_msg),\n",
    "        (\"human\", \"Risk Analysis Report: {report}\")\n",
    "    ])\n",
    "    chain = prompt | llm\n",
    "    response = chain.invoke({\"report\": risk_analysis})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa4059d",
   "metadata": {},
   "source": [
    "### 2.5 Build and Compile Graph\n",
    "Connect the nodes to form the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add Nodes\n",
    "workflow.add_node(\"risk_analyst\", risk_analyst_node)\n",
    "workflow.add_node(\"senior_underwriter\", senior_underwriter_node)\n",
    "\n",
    "# Define Logic\n",
    "workflow.add_edge(START, \"risk_analyst\")\n",
    "workflow.add_edge(\"risk_analyst\", \"senior_underwriter\")\n",
    "workflow.add_edge(\"senior_underwriter\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379c688",
   "metadata": {},
   "source": [
    "### 2.6 Run the Pipeline\n",
    "Now we run the full flow and can observe the trace in Arize Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke Graph\n",
    "print(\"ðŸš€ Starting Loan Underwriting Pipeline...\")\n",
    "\n",
    "applicant_info = \"Applicant: John Doe. Income: $150,000/yr. Monthly Debt: $2,500. Credit Score: 780. Loan Amount: $500,000.\"\n",
    "print(f\"Processing Application: {applicant_info}\\n\")\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=applicant_info)],\n",
    "    \"applicant_id\": \"APP-12345\"\n",
    "}\n",
    "\n",
    "for output in graph.stream(inputs, stream_mode=\"updates\"):\n",
    "    for node_name, node_state in output.items():\n",
    "        print(f\"\\n--- NODE: {node_name.upper()} ---\")\n",
    "        last_msg = node_state[\"messages\"][-1]\n",
    "        print(f\"Content: {last_msg.content[:300]}...\")\n",
    "\n",
    "print(\"\\nâœ… Pipeline Complete. Check Phoenix UI for the trace lineage.\")\n",
    "\n",
    "# Check Phoenix UI again. You should see a trace for this execution showing the graph steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8221466c",
   "metadata": {},
   "source": [
    "## Part 3: High-Level Chaining with Runnables (Customer Feedback Processor)\n",
    "\n",
    "In this section, we will see how Arize Phoenix shines when debugging complex chains using `RunnablePassthrough` and `RunnableLambda`. \n",
    "We often want to pass data through multiple steps, augmenting it along the way. \n",
    "\n",
    "We will build a **Customer Feedback Processor** that:\n",
    "1.  Takes a raw review.\n",
    "2.  **Cleans** it (custom function).\n",
    "3.  **Analyzes Sentiment** (LLM call).\n",
    "4.  **Extracts Keywords** (LLM call).\n",
    "5.  **Generates a Summary Response** using all previous inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "part3_chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# 1. Define specific purpose chains\n",
    "sentiment_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Analyze the sentiment of this review. Return ONLY one word: POSITIVE, NEGATIVE, or NEUTRAL. Review: {cleaned_review}\"\n",
    ")\n",
    "sentiment_chain = sentiment_prompt | llm | StrOutputParser()\n",
    "\n",
    "keyword_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Extract top 3 keywords from this review. Return them as a comma-separated list. Review: {cleaned_review}\"\n",
    ")\n",
    "keyword_chain = keyword_prompt | llm | StrOutputParser()\n",
    "\n",
    "final_response_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a Customer Success Manager. Write a generic response to this review based on the analysis.\\n\"\n",
    "    \"Review: {cleaned_review}\\n\"\n",
    "    \"Sentiment: {sentiment}\\n\"\n",
    "    \"Keywords: {keywords}\\n\\n\"\n",
    "    \"Response:\"\n",
    ")\n",
    "final_response_chain = final_response_prompt | llm | StrOutputParser()\n",
    "\n",
    "# 2. Define custom helper (RunnableLambda)\n",
    "def clean_text(text):\n",
    "    return text.strip().lower()\n",
    "\n",
    "# 3. Build the Super Chain with Passthrough\n",
    "# The input to the chain is just the raw key \"review\"\n",
    "super_chain = (\n",
    "    {\"cleaned_review\": lambda x: clean_text(x[\"review\"])}\n",
    "    | RunnablePassthrough.assign(sentiment=sentiment_chain)\n",
    "    | RunnablePassthrough.assign(keywords=keyword_chain)\n",
    "    | RunnablePassthrough.assign(display_result=final_response_chain)\n",
    ")\n",
    "\n",
    "# Invoke\n",
    "print(\"ðŸš€ Processing Customer Feedback...\")\n",
    "review_text = \"  The product arrived late and was broken. Terrible service!  \"\n",
    "result = super_chain.invoke({\"review\": review_text})\n",
    "\n",
    "print(f\"\\nOriginal: '{review_text}'\")\n",
    "print(f\"Cleaned: '{result['cleaned_review']}'\")\n",
    "print(f\"Sentiment: {result['sentiment']}\")\n",
    "print(f\"Keywords: {result['keywords']}\")\n",
    "print(f\"\\nFinal Response generated:\\n{result['display_result']}\")\n",
    "\n",
    "# Check Arize Phoenix! You will see the 'sentiment_chain' and 'keyword_chain' \n",
    "# as separate spans running in parallel (conceptually) or sequentially, \n",
    "# feeding into the final step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "You have successfully set up Arize Phoenix and instrumented both a **Banking Policy Assistant** and a **Loan Underwriting Pipeline**. \n",
    "Review the traces in the Phoenix UI to understand the internal execution of your Critical Financial Workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
