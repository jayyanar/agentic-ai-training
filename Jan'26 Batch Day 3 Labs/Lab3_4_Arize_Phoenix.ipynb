{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 2.4: LLM Observability with Arize Phoenix\n",
                "\n",
                "In this lab, we will learn how to instrument our LLM applications for observability using **Arize Phoenix**. Observability is crucial for understanding how your LLM applications are performing, debugging issues, and evaluating quality.\n",
                "\n",
                "We will cover:\n",
                "1.  **Setup**: Installing Phoenix and launching the local UI.\n",
                "2.  **Part 1: Simple LangChain Flow**: Instrumenting a basic chain.\n",
                "3.  **Part 2: Advanced LangGraph Application**: Instrumenting a stateful agent workflow.\n",
                "\n",
                "### Prerequisites\n",
                "- Ensure you have your Groq API Key ready.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "312def0f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "%pip install -qU arize-phoenix arize-otel openinference-instrumentation-langchain langchain langchain-groq langgraph\n",
                "%pip install -q \"arize[AutoEmbeddings]\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "5ab4658e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Setup API Keys\n",
                "import getpass\n",
                "import os\n",
                "from arize.otel import register\n",
                "\n",
                "if \"GROQ_API_KEY\" not in os.environ:\n",
                "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
                "\n",
                "if \"ARIZE_SPACE_ID\" not in os.environ:\n",
                "    os.environ[\"ARIZE_SPACE_ID\"] = getpass.getpass(\"Enter your Arize Space ID: \")\n",
                "\n",
                "if \"ARIZE_API_KEY\" not in os.environ:\n",
                "    os.environ[\"ARIZE_API_KEY\"] = getpass.getpass(\"Enter your Arize API Key: \")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "47fd9737",
            "metadata": {},
            "source": [
                "## 3. Launch Arize Phoenix (Optional)\n",
                "You can still launch the Phoenix application locally to view traces locally, or you can view them in the Arize Cloud dashboard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "692d2020",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import Arize observability components\n",
                "from arize.otel import register\n",
                "from getpass import getpass\n",
                "\n",
                "# Configure Arize tracing\n",
                "tracer_provider = register(\n",
                "    space_id=os.environ[\"ARIZE_SPACE_ID\"],\n",
                "    api_key=os.environ[\"ARIZE_API_KEY\"],\n",
                "    project_name=\"arize-lab-demo\"\n",
                ")\n",
                "\n",
                "# Enable automatic LangChain instrumentation\n",
                "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
                "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
                "\n",
                "print(\"ðŸ”­ Arize observability configured successfully!\")\n",
                "print(\"ðŸ“Š All LangChain operations will now be automatically traced\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e2c3823c",
            "metadata": {},
            "source": [
                "## Part 1: Simple LangChain Flow (Banking Policy Assistant)\n",
                "\n",
                "We will create a simple **Banking Policy Assistant** that answers internal policy questions. This simulates a basic RAG or Chatbot interaction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "73e99769",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_groq import ChatGroq\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "\n",
                "# Setup a Banking Policy Assistant\n",
                "llm = ChatGroq(\n",
                "    model=\"qwen/qwen3-32b\",\n",
                "    temperature=0,\n",
                "    reasoning_format=\"parsed\"\n",
                ")\n",
                "\n",
                "# This prompt simulates an internal knowledge base lookup\n",
                "policy_prompt = ChatPromptTemplate.from_template(\n",
                "    \"You are a Banking Policy Assistant. Answer the question based on standard mortgage guidelines. Keep it professional. Question: {question}\"\n",
                ")\n",
                "chain = policy_prompt | llm | StrOutputParser()\n",
                "\n",
                "# Invoke the chain\n",
                "print(\"Invoking Banking Policy Assistant...\")\n",
                "question = \"What is the maximum loan-to-value (LTV) ratio for a jumbo mortgage?\"\n",
                "print(f\"Question: {question}\")\n",
                "response = chain.invoke({\"question\": question})\n",
                "print(f\"Response: {response}\")\n",
                "\n",
                "# Check the Phoenix UI and Arize Cloud to see the trace!"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b58211ff",
            "metadata": {},
            "source": [
                "## Part 2: Advanced LangGraph Application (Loan Underwriting)\n",
                "\n",
                "Now, let's look at a complex **Loan Underwriting Pipeline**. This involves multiple steps: Risk Analysis and Final Decision.\n",
                "\n",
                "We will use LangGraph to model this stateful workflow, where a **Risk Analyst** assesses the applicant and a **Senior Underwriter** makes the final call."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "48a30367",
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import Annotated, Sequence\n",
                "from typing_extensions import TypedDict\n",
                "from langgraph.graph import StateGraph, START, END\n",
                "from langgraph.graph.message import add_messages\n",
                "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
                "\n",
                "# --- 2. STATE DEFINITION ---\n",
                "class State(TypedDict):\n",
                "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
                "    applicant_id: str\n",
                "    risk_score: str\n",
                "\n",
                "# Define Nodes\n",
                "def risk_analyst_node(state: State):\n",
                "    \"\"\"Analyzes the financial health of the applicant.\"\"\"\n",
                "    applicant_data = state[\"messages\"][-1].content\n",
                "    \n",
                "    system_msg = (\n",
                "        \"You are a Risk Analyst for a bank. Analyze the provided applicant financial data \"\n",
                "        \"(Income, Debt, Credit Score). Calculate the Debt-to-Income (DTI) ratio. \"\n",
                "        \"Summarize the financial health and assign a risk level (Low, Medium, High).\"\n",
                "    )\n",
                "    \n",
                "    prompt = ChatPromptTemplate.from_messages([\n",
                "        (\"system\", system_msg),\n",
                "        (\"human\", \"Applicant Data: {data}\")\n",
                "    ])\n",
                "    chain = prompt | llm\n",
                "    response = chain.invoke({\"data\": applicant_data})\n",
                "    \n",
                "    # Extract a dummy risk score to update state (simplification)\n",
                "    # In a real app, we'd use structured output\n",
                "    return {\"messages\": [response], \"risk_score\": \"Calculated\"}\n",
                "\n",
                "def senior_underwriter_node(state: State):\n",
                "    \"\"\"Makes the final loan decision based on risk analysis.\"\"\"\n",
                "    risk_analysis = state[\"messages\"][-1].content\n",
                "    \n",
                "    system_msg = (\n",
                "        \"You are a Senior Underwriter. Review the Risk Analyst's report. \"\n",
                "        \"Make a final decision: APPROVE or DENY. \"\n",
                "        \"If approved, set the interest rate tier (Tier 1 is best). \"\n",
                "        \"Provide a brief justification for the decision.\"\n",
                "    )\n",
                "    \n",
                "    prompt = ChatPromptTemplate.from_messages([\n",
                "        (\"system\", system_msg),\n",
                "        (\"human\", \"Risk Analysis Report: {report}\")\n",
                "    ])\n",
                "    chain = prompt | llm\n",
                "    response = chain.invoke({\"report\": risk_analysis})\n",
                "    return {\"messages\": [response]}\n",
                "\n",
                "# Build Graph\n",
                "workflow = StateGraph(State)\n",
                "\n",
                "# Add Nodes\n",
                "workflow.add_node(\"risk_analyst\", risk_analyst_node)\n",
                "workflow.add_node(\"senior_underwriter\", senior_underwriter_node)\n",
                "\n",
                "# Define Logic\n",
                "workflow.add_edge(START, \"risk_analyst\")\n",
                "workflow.add_edge(\"risk_analyst\", \"senior_underwriter\")\n",
                "workflow.add_edge(\"senior_underwriter\", END)\n",
                "\n",
                "# Compile\n",
                "graph = workflow.compile()\n",
                "\n",
                "# Invoke Graph\n",
                "print(\"ðŸš€ Starting Loan Underwriting Pipeline...\")\n",
                "\n",
                "applicant_info = \"Applicant: John Doe. Income: $150,000/yr. Monthly Debt: $2,500. Credit Score: 780. Loan Amount: $500,000.\"\n",
                "print(f\"Processing Application: {applicant_info}\\n\")\n",
                "\n",
                "inputs = {\n",
                "    \"messages\": [HumanMessage(content=applicant_info)],\n",
                "    \"applicant_id\": \"APP-12345\"\n",
                "}\n",
                "\n",
                "for output in graph.stream(inputs, stream_mode=\"updates\"):\n",
                "    for node_name, node_state in output.items():\n",
                "        print(f\"\\n--- NODE: {node_name.upper()} ---\")\n",
                "        last_msg = node_state[\"messages\"][-1]\n",
                "        print(f\"Content: {last_msg.content[:300]}...\")\n",
                "\n",
                "print(\"\\nâœ… Pipeline Complete. Check Phoenix UI for the trace lineage.\")\n",
                "\n",
                "# Check Phoenix UI again. You should see a trace for this execution showing the graph steps."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "You have successfully set up Arize Phoenix and instrumented both a **Banking Policy Assistant** and a **Loan Underwriting Pipeline**. \n",
                "Review the traces in the Phoenix UI to understand the internal execution of your Critical Financial Workflows."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
