{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "gA0S7DuzBqoL",
      "metadata": {
        "id": "gA0S7DuzBqoL"
      },
      "source": [
        "# Lab 2.4: LLM Observability with Arize Phoenix\n",
        "\n",
        "In this lab, we will learn how to instrument our LLM applications for observability using **Arize Phoenix**. Observability is crucial for understanding how your LLM applications are performing, debugging issues, and evaluating quality.\n",
        "\n",
        "We will cover:\n",
        "1.  **Setup**: Installing Phoenix and launching the local UI.\n",
        "2.  **Part 1: Simple LangChain Flow**: Instrumenting a basic chain.\n",
        "3.  **Part 2: Advanced LangGraph Application**: Instrumenting a stateful agent workflow.\n",
        "\n",
        "### Prerequisites\n",
        "- Ensure you have your Groq API Key ready.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "312def0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "312def0f",
        "outputId": "9c3dabb6-4668-44db-f4be-9515505065f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/309.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m108.5/108.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.8/161.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m432.2/432.2 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m238.7/238.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 1. Install Dependencies\n",
        "%pip install -qU arize-phoenix arize-otel openinference-instrumentation-langchain langchain langchain-groq langgraph\n",
        "%pip install -q \"arize[AutoEmbeddings]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5ab4658e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ab4658e",
        "outputId": "78405a8e-97c1-4eea-a5d0-8d22237c28d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Groq API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter your Arize Space ID: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter your Arize API Key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ],
      "source": [
        "# 2. Setup API Keys\n",
        "import getpass\n",
        "import os\n",
        "from arize.otel import register\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "if \"ARIZE_SPACE_ID\" not in os.environ:\n",
        "    os.environ[\"ARIZE_SPACE_ID\"] = getpass.getpass(\"Enter your Arize Space ID: \")\n",
        "\n",
        "if \"ARIZE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"ARIZE_API_KEY\"] = getpass.getpass(\"Enter your Arize API Key: \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47fd9737",
      "metadata": {
        "id": "47fd9737"
      },
      "source": [
        "## 3. Launch Arize Phoenix (Optional)\n",
        "You can still launch the Phoenix application locally to view traces locally, or you can view them in the Arize Cloud dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "692d2020",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "692d2020",
        "outputId": "e596639b-b055-45a2-dd6c-c449484ac158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”­ OpenTelemetry Tracing Details ğŸ”­\n",
            "|  Arize Project: arize-lab-demo\n",
            "|  Span Processor: BatchSpanProcessor\n",
            "|  Collector Endpoint: otlp.arize.com\n",
            "|  Transport: gRPC\n",
            "|  Transport Headers: {'authorization': '****', 'api_key': '****', 'arize-space-id': '****', 'space_id': '****', 'arize-interface': '****'}\n",
            "|  \n",
            "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
            "|  \n",
            "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
            "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
            "\n",
            "ğŸ”­ Arize observability configured successfully!\n",
            "ğŸ“Š All LangChain operations will now be automatically traced\n"
          ]
        }
      ],
      "source": [
        "# Import Arize observability components\n",
        "from arize.otel import register\n",
        "from getpass import getpass\n",
        "\n",
        "# Configure Arize tracing\n",
        "tracer_provider = register(\n",
        "    space_id=os.environ[\"ARIZE_SPACE_ID\"],\n",
        "    api_key=os.environ[\"ARIZE_API_KEY\"],\n",
        "    project_name=\"arize-lab-demo\"\n",
        ")\n",
        "\n",
        "# Enable automatic LangChain instrumentation\n",
        "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
        "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
        "\n",
        "print(\"ğŸ”­ Arize observability configured successfully!\")\n",
        "print(\"ğŸ“Š All LangChain operations will now be automatically traced\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c3823c",
      "metadata": {
        "id": "e2c3823c"
      },
      "source": [
        "## Part 1: Simple LangChain Flow (Banking Policy Assistant)\n",
        "\n",
        "We will create a simple **Banking Policy Assistant** that answers internal policy questions. This simulates a basic RAG or Chatbot interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "73e99769",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73e99769",
        "outputId": "d34874fd-3406-41bd-9195-dcf38de08b70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invoking Banking Policy Assistant...\n",
            "Question: What is the maximum loan-to-value (LTV) ratio for a jumbo mortgage?\n",
            "Response: The maximum loan-to-value (LTV) ratio for a jumbo mortgage typically ranges between **75% and 85%**, depending on the lender and the borrower's financial profile. Most lenders require a minimum down payment of **20% to 25%** for jumbo loans, resulting in an LTV of **80% to 85%**. However, borrowers with strong credit scores, stable income, and low debt-to-income ratios may qualify for higher LTVs in some cases. \n",
            "\n",
            "It is important to note that jumbo loans are not eligible for purchase by Fannie Mae or Freddie Mac and are subject to stricter underwriting standards due to their higher risk profile. Always confirm specific requirements with your lender, as policies can vary.\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Setup a Banking Policy Assistant\n",
        "llm = ChatGroq(\n",
        "    model=\"qwen/qwen3-32b\",\n",
        "    temperature=0,\n",
        "    reasoning_format=\"parsed\"\n",
        ")\n",
        "\n",
        "# This prompt simulates an internal knowledge base lookup\n",
        "policy_prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a Banking Policy Assistant. Answer the question based on standard mortgage guidelines. Keep it professional. Question: {question}\"\n",
        ")\n",
        "chain = policy_prompt | llm | StrOutputParser()\n",
        "\n",
        "# Invoke the chain\n",
        "print(\"Invoking Banking Policy Assistant...\")\n",
        "question = \"What is the maximum loan-to-value (LTV) ratio for a jumbo mortgage?\"\n",
        "print(f\"Question: {question}\")\n",
        "response = chain.invoke({\"question\": question})\n",
        "print(f\"Response: {response}\")\n",
        "\n",
        "# Check the Phoenix UI and Arize Cloud to see the trace!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b58211ff",
      "metadata": {
        "id": "b58211ff"
      },
      "source": [
        "## Part 2: Advanced LangGraph Application (Loan Underwriting)\n",
        "\n",
        "Now, let's look at a complex **Loan Underwriting Pipeline**. This involves multiple steps: Risk Analysis and Final Decision.\n",
        "\n",
        "We will use LangGraph to model this stateful workflow, where a **Risk Analyst** assesses the applicant and a **Senior Underwriter** makes the final call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "48a30367",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48a30367",
        "outputId": "9f7dbebd-1cb2-405b-d3c0-41134a3302c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Starting Loan Underwriting Pipeline...\n",
            "Processing Application: Applicant: John Doe. Income: $150,000/yr. Monthly Debt: $2,500. Credit Score: 780. Loan Amount: $500,000.\n",
            "\n",
            "\n",
            "--- NODE: RISK_ANALYST ---\n",
            "Content: **Financial Analysis for John Doe**  \n",
            "\n",
            "### **Key Calculations**  \n",
            "1. **Debt-to-Income (DTI) Ratio**:  \n",
            "   - **Monthly Income**: $150,000 / 12 = **$12,500**  \n",
            "   - **DTI**: ($2,500 Monthly Debt / $12,500 Monthly Income) Ã— 100 = **20%**  \n",
            "\n",
            "2. **Loan-to-Income Ratio**:  \n",
            "   - **Loan Amount / Annual Inc...\n",
            "\n",
            "--- NODE: SENIOR_UNDERWRITER ---\n",
            "Content: **Final Decision**: APPROVE  \n",
            "**Interest Rate Tier**: Tier 1  \n",
            "\n",
            "**Justification**:  \n",
            "John Doeâ€™s financial profile is exceptionally strong, with a low DTI (20%), excellent credit score (780), and high income ($150,000 annually). While the $500,000 loan is 3.33x his income, his robust repayment capaci...\n",
            "\n",
            "âœ… Pipeline Complete. Check Phoenix UI for the trace lineage.\n"
          ]
        }
      ],
      "source": [
        "from typing import Annotated, Sequence\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "\n",
        "# --- 2. STATE DEFINITION ---\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    applicant_id: str\n",
        "    risk_score: str\n",
        "\n",
        "# Define Nodes\n",
        "def risk_analyst_node(state: State):\n",
        "    \"\"\"Analyzes the financial health of the applicant.\"\"\"\n",
        "    applicant_data = state[\"messages\"][-1].content\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are a Risk Analyst for a bank. Analyze the provided applicant financial data \"\n",
        "        \"(Income, Debt, Credit Score). Calculate the Debt-to-Income (DTI) ratio. \"\n",
        "        \"Summarize the financial health and assign a risk level (Low, Medium, High).\"\n",
        "    )\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_msg),\n",
        "        (\"human\", \"Applicant Data: {data}\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"data\": applicant_data})\n",
        "\n",
        "    # Extract a dummy risk score to update state (simplification)\n",
        "    # In a real app, we'd use structured output\n",
        "    return {\"messages\": [response], \"risk_score\": \"Calculated\"}\n",
        "\n",
        "def senior_underwriter_node(state: State):\n",
        "    \"\"\"Makes the final loan decision based on risk analysis.\"\"\"\n",
        "    risk_analysis = state[\"messages\"][-1].content\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are a Senior Underwriter. Review the Risk Analyst's report. \"\n",
        "        \"Make a final decision: APPROVE or DENY. \"\n",
        "        \"If approved, set the interest rate tier (Tier 1 is best). \"\n",
        "        \"Provide a brief justification for the decision.\"\n",
        "    )\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_msg),\n",
        "        (\"human\", \"Risk Analysis Report: {report}\")\n",
        "    ])\n",
        "    chain = prompt | llm\n",
        "    response = chain.invoke({\"report\": risk_analysis})\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Build Graph\n",
        "workflow = StateGraph(State)\n",
        "\n",
        "# Add Nodes\n",
        "workflow.add_node(\"risk_analyst\", risk_analyst_node)\n",
        "workflow.add_node(\"senior_underwriter\", senior_underwriter_node)\n",
        "\n",
        "# Define Logic\n",
        "workflow.add_edge(START, \"risk_analyst\")\n",
        "workflow.add_edge(\"risk_analyst\", \"senior_underwriter\")\n",
        "workflow.add_edge(\"senior_underwriter\", END)\n",
        "\n",
        "# Compile\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Invoke Graph\n",
        "print(\"ğŸš€ Starting Loan Underwriting Pipeline...\")\n",
        "\n",
        "applicant_info = \"Applicant: John Doe. Income: $150,000/yr. Monthly Debt: $2,500. Credit Score: 780. Loan Amount: $500,000.\"\n",
        "print(f\"Processing Application: {applicant_info}\\n\")\n",
        "\n",
        "inputs = {\n",
        "    \"messages\": [HumanMessage(content=applicant_info)],\n",
        "    \"applicant_id\": \"APP-12345\"\n",
        "}\n",
        "\n",
        "for output in graph.stream(inputs, stream_mode=\"updates\"):\n",
        "    for node_name, node_state in output.items():\n",
        "        print(f\"\\n--- NODE: {node_name.upper()} ---\")\n",
        "        last_msg = node_state[\"messages\"][-1]\n",
        "        print(f\"Content: {last_msg.content[:300]}...\")\n",
        "\n",
        "print(\"\\nâœ… Pipeline Complete. Check Phoenix UI for the trace lineage.\")\n",
        "\n",
        "# Check Phoenix UI again. You should see a trace for this execution showing the graph steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0jJdY8ywBqoN",
      "metadata": {
        "id": "0jJdY8ywBqoN"
      },
      "source": [
        "## Conclusion\n",
        "You have successfully set up Arize Phoenix and instrumented both a **Banking Policy Assistant** and a **Loan Underwriting Pipeline**.\n",
        "Review the traces in the Phoenix UI to understand the internal execution of your Critical Financial Workflows."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
