{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i5GiWdqQDgH"
      },
      "source": [
        "### LCEL Deepdive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install required dependencies for LangChain, Groq API integration, and vector storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPDhk2lxQWtV"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-groq langchain_community langchain_huggingface faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up environment variables and import necessary LangChain components for building chains with the Groq LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDCje1SHQDgI"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "#from dotenv import load_dotenv\n",
        "#load_dotenv()\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a simple LCEL chain: prompt → model → output parser. This demonstrates the pipe operator (|) chaining components together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "CplBwP7dQDgI",
        "outputId": "caaadb0a-6969-4a56-e09c-f380ed4c464b"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"Explain about {topic} in detail\")\n",
        "#model = ChatOpenAI()\n",
        "# Initialize the Groq model\n",
        "model = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        ")\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "chain.invoke({\"topic\": \"ice cream\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect the prompt template output to see how variables are formatted before being passed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYsPkRcuQDgJ",
        "outputId": "14ae66dd-e066-428d-de85-00e7ebb24b8d"
      },
      "outputs": [],
      "source": [
        "print(prompt.invoke({\"topic\": \"ice cream\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Invoke the model directly with chat messages to generate a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IypsvO3oQDgJ",
        "outputId": "866e0515-1038-4ac7-81a2-9b8a1a3a3677"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages.human import HumanMessage\n",
        "\n",
        "messages = [HumanMessage(content='tell me a short joke about ice cream')]\n",
        "model.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHpjNf7WQDgJ"
      },
      "source": [
        "### What is this \"|\" in Python?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement a custom Runnable class to understand how the pipe operator (|) works. This creates a chain pattern where each component processes data sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGht2RsIQDgJ"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class CRunnable(ABC):\n",
        "    def __init__(self):\n",
        "        self.next = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def process(self, data):\n",
        "        \"\"\"\n",
        "        This method must be implemented by subclasses to define\n",
        "        data processing behavior.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def invoke(self, data):\n",
        "        processed_data = self.process(data)\n",
        "        if self.next is not None:\n",
        "            return self.next.invoke(processed_data)\n",
        "        return processed_data\n",
        "\n",
        "    def __or__(self, other):\n",
        "        return CRunnableSequence(self, other)\n",
        "\n",
        "class CRunnableSequence(CRunnable):\n",
        "    def __init__(self, first, second):\n",
        "        super().__init__()\n",
        "        self.first = first\n",
        "        self.second = second\n",
        "\n",
        "    def process(self, data):\n",
        "        return data\n",
        "\n",
        "    def invoke(self, data):\n",
        "        first_result = self.first.invoke(data)\n",
        "        return self.second.invoke(first_result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define concrete Runnable implementations (AddTen, MultiplyByTwo, ConvertToString) to demonstrate data transformation through a chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyVu3RAEQDgJ"
      },
      "outputs": [],
      "source": [
        "class AddTen(CRunnable):\n",
        "    def process(self, data):\n",
        "        print(\"AddTen: \", data)\n",
        "        return data + 10\n",
        "\n",
        "class MultiplyByTwo(CRunnable):\n",
        "    def process(self, data):\n",
        "        print(\"Multiply by 2: \", data)\n",
        "        return data * 2\n",
        "\n",
        "class ConvertToString(CRunnable):\n",
        "    def process(self, data):\n",
        "        print(\"Convert to string: \", data)\n",
        "        return f\"Result: {data}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a chain by composing three custom Runnables using the pipe operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N1o2mcNQDgK"
      },
      "outputs": [],
      "source": [
        "a = AddTen()\n",
        "b = MultiplyByTwo()\n",
        "c = ConvertToString()\n",
        "\n",
        "chain = a | b | c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the chain and observe how data flows through each component sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJv5G-x2QDgK",
        "outputId": "b50a1727-600b-4aa7-ba7b-50d353dde654"
      },
      "outputs": [],
      "source": [
        "result = chain.invoke(10)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9k_RxZCQDgK"
      },
      "source": [
        "### Runnables from LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import LangChain's built-in Runnable types: RunnablePassthrough (passes data through unchanged), RunnableLambda (wraps functions), and RunnableParallel (processes multiple branches)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCfNvqMRQDgK"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chain multiple RunnablePassthrough instances together, demonstrating that passthrough components return data unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z8dxYbUGQDgK",
        "outputId": "f8252ce8-a84a-4a97-fdc9-287644635654"
      },
      "outputs": [],
      "source": [
        "chain = RunnablePassthrough() | RunnablePassthrough () | RunnablePassthrough ()\n",
        "chain.invoke(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a lambda function that converts input strings to uppercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sqHfgWqQDgK"
      },
      "outputs": [],
      "source": [
        "def input_to_upper(input: str):\n",
        "    output = input.upper()\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chain RunnablePassthrough with RunnableLambda to apply the uppercase transformation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WJPXmLEnQDgK",
        "outputId": "b491ef46-830b-4769-dc1d-08f7e65dbb8e"
      },
      "outputs": [],
      "source": [
        "chain = RunnablePassthrough() | RunnableLambda(input_to_upper) | RunnablePassthrough()\n",
        "chain.invoke(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a RunnableParallel that simultaneously processes the same input through multiple branches, each stored as a key in the output dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgL28CSVQDgK"
      },
      "outputs": [],
      "source": [
        "chain = RunnableParallel({\"x\": RunnablePassthrough(), \"y\": RunnablePassthrough()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the parallel chain with a simple string input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j0pcnA6QDgL",
        "outputId": "673a5a22-bb3d-43be-a6fe-035881993310"
      },
      "outputs": [],
      "source": [
        "chain.invoke(\"hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the parallel chain with a dictionary input to see how RunnablePassthrough handles structured data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoRPPQALQDgL",
        "outputId": "975b245d-afb6-45ea-b58c-2639fed6c354"
      },
      "outputs": [],
      "source": [
        "chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a RunnableParallel with a lambda function that extracts a specific key from the input dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrqZ_nrqQDgL"
      },
      "outputs": [],
      "source": [
        "chain = RunnableParallel({\"x\": RunnablePassthrough(), \"y\": lambda z: z[\"input2\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the parallel chain, demonstrating selective extraction of input values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rxIIM9PQDgL",
        "outputId": "b63525f1-8409-44db-f59a-3036f645d3ca"
      },
      "outputs": [],
      "source": [
        "chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMlOLFWCQDgL"
      },
      "source": [
        "### Nested chains - now it gets more complicated!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a lambda function that extracts and transforms a specific key from the input dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_rqdmT2QDgL"
      },
      "outputs": [],
      "source": [
        "def find_keys_to_uppercase(input: dict):\n",
        "    output = input.get(\"input\", \"not found\").upper()\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a complex nested chain with RunnableParallel containing a sequential pipe (RunnablePassthrough | RunnableLambda) in one branch and a lambda in another."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4JEpMo_QDgL"
      },
      "outputs": [],
      "source": [
        "chain = RunnableParallel({\"x\": RunnablePassthrough() | RunnableLambda(find_keys_to_uppercase), \"y\": lambda z: z[\"input2\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the nested chain to show how complex transformations combine parallel and sequential processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LdbNEBIQDgL",
        "outputId": "079cd8a0-f6f2-47df-e1f7-ad51ca3c200a"
      },
      "outputs": [],
      "source": [
        "chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define helper functions for assignment and multiplication, then create a RunnableParallel as a base for further chaining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vegnFMbmQDgL"
      },
      "outputs": [],
      "source": [
        "chain = RunnableParallel({\"x\": RunnablePassthrough()})\n",
        "\n",
        "def assign_func(input):\n",
        "    return 100\n",
        "\n",
        "def multiply(input):\n",
        "    return input * 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the parallel chain with dictionary input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxO4LKE-QDgL",
        "outputId": "3341aeb6-e7c6-4a33-8edb-a8074341e6ea"
      },
      "outputs": [],
      "source": [
        "chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the .assign() method to add new computed fields to the chain output while preserving existing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUOJalpIQDgL"
      },
      "outputs": [],
      "source": [
        "chain = RunnableParallel({\"x\": RunnablePassthrough()}).assign(extra=RunnableLambda(assign_func))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the chain with assignment to display the combined result with both original and computed values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJn6KsmuQDgL",
        "outputId": "8993b985-6d77-492a-e966-7f23c06ff130"
      },
      "outputs": [],
      "source": [
        "result = chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euzpl0q_QDgM"
      },
      "source": [
        "### Combine multiple chains (incl. coercion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create extraction and transformation functions, then compose them into a new chain that extracts data and applies uppercase conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLj8ph1dQDgM"
      },
      "outputs": [],
      "source": [
        "def extractor(input: dict):\n",
        "    return input.get(\"extra\", \"Key not found\")\n",
        "\n",
        "def cupper(upper: str):\n",
        "    return str(upper).upper()\n",
        "\n",
        "new_chain = RunnableLambda(extractor) | RunnableLambda(cupper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the extraction and transformation chain on a dictionary input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CmZ_HmW5QDgM",
        "outputId": "8cd7f75f-a174-47fa-9aad-f1d101a0e562"
      },
      "outputs": [],
      "source": [
        "new_chain.invoke({\"extra\": \"test\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Combine the complex nested chain with the extraction chain to demonstrate chain coercion and composition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8EANjuTDQDgM",
        "outputId": "6dd010d0-3684-473a-e3dd-947ee47e81b2"
      },
      "outputs": [],
      "source": [
        "final_chain = chain | new_chain\n",
        "final_chain.invoke({\"input\": \"hello\", \"input2\": \"goodbye\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2YB8IXNQDgM"
      },
      "source": [
        "### Real World Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build a Retrieval-Augmented Generation (RAG) chain: create embeddings, initialize a vector store with sample data, set up a retriever, and chain it with a prompt and LLM for context-aware question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLPYCzJyQDgM"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "vectorstore = FAISS.from_texts(\n",
        "    [\"Cats love thuna\"], embedding=embeddings\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template=template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    RunnableParallel({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()})\n",
        "    | prompt\n",
        "    | ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0, max_retries=2)\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Query the RAG chain with a question to generate an answer based on retrieved context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YBaX_HG7QDgM",
        "outputId": "7a380107-a51a-4b89-adf7-17cc607f0dc6"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"What do cats like to eat?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Debug the RAG chain by inspecting the output of the RunnableParallel component to see the retrieved context and question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdHywXy7QDgM",
        "outputId": "7aa3081f-74d2-42fe-85ac-231ddc4ad1c6"
      },
      "outputs": [],
      "source": [
        "RunnableParallel({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}).invoke(\"What do cats like to eat?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect the formatted prompt with injected context and question values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiQmvzNpQDgM",
        "outputId": "0297f79c-e426-4551-8c5c-5f530269c8d2"
      },
      "outputs": [],
      "source": [
        "prompt.invoke({\"context\": \"Cats love thuna\", \"question\": \"What do cats like to eat?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute the model on the formatted prompt to see the final LLM response before parsing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht2rISCuQDgM",
        "outputId": "12eb1c85-41ea-4aea-ca4e-e41e04d4e8ac"
      },
      "outputs": [],
      "source": [
        "model.invoke(prompt.invoke({\"context\": \"Cats love thuna\", \"question\": \"What do cats like to eat?\"}))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchain-crash-course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
