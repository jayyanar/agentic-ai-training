{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayyanar/agentic-ai-training/blob/lab-day-1/batch2/lca-langchainV1-essentials/mandatory/output/L3_streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "483ab18e-f419-46ad-9bbe-171ffd05f983",
      "metadata": {
        "id": "483ab18e-f419-46ad-9bbe-171ffd05f983"
      },
      "source": [
        "# Streaming\n",
        "\n",
        "<img src=\"https://github.com/jayyanar/agentic-ai-training/blob/lab-day-1/batch2/lca-langchainV1-essentials/assets/LC_streaming.png?raw=1\" width=\"400\">\n",
        "\n",
        "Streaming reduces the latency between generating data and the user receiving it.\n",
        "There are two types frequently used with Agents:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f0f22c-9724-46ce-baf3-60a2de701fb3",
      "metadata": {
        "id": "66f0f22c-9724-46ce-baf3-60a2de701fb3"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b76bab7-fa52-46f4-86bc-b157067e0168",
      "metadata": {
        "id": "6b76bab7-fa52-46f4-86bc-b157067e0168"
      },
      "source": [
        "Load and/or check for needed environmental variables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0d7c65",
      "metadata": {
        "id": "6c0d7c65"
      },
      "source": [
        "What we're doing: Install required packages for streaming examples in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "EaeYUvxIB8G4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaeYUvxIB8G4",
        "outputId": "ec405b2b-1663-4576-ed4b-589acd431560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/157.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m153.6/157.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m2.1/2.5 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-groq langgraph langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2659bde",
      "metadata": {
        "id": "a2659bde"
      },
      "source": [
        "What we're doing: Load the GROQ API key from Colab userdata into the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b2fcfd93-0004-4ff1-9b60-0b21baf68c6d",
      "metadata": {
        "id": "b2fcfd93-0004-4ff1-9b60-0b21baf68c6d"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ca4692",
      "metadata": {
        "id": "51ca4692"
      },
      "source": [
        "What we're doing: Initialize the Groq LLM used in streaming examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "166fc0b1-2322-4dd2-a358-89309fb9f4ce",
      "metadata": {
        "id": "166fc0b1-2322-4dd2-a358-89309fb9f4ce"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "418bbbd6",
      "metadata": {
        "id": "418bbbd6"
      },
      "source": [
        "What we're doing: Create an agent with a simple system prompt for streaming demonstrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a26b4586-453a-4c10-9fae-4be3f4cb6cd4",
      "metadata": {
        "id": "a26b4586-453a-4c10-9fae-4be3f4cb6cd4"
      },
      "outputs": [],
      "source": [
        "agent = create_agent(\n",
        "    #model=\"openai:gpt-5\",\n",
        "    model=llm,\n",
        "    system_prompt=\"You are a full-stack comedian\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a907aa9-a608-47e2-92d4-6758a1728cb2",
      "metadata": {
        "id": "7a907aa9-a608-47e2-92d4-6758a1728cb2"
      },
      "source": [
        "## No Streaming (invoke)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "198393ca",
      "metadata": {
        "id": "198393ca"
      },
      "source": [
        "What we're doing: Invoke the agent synchronously (no streaming) to get a full response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fc384cf0-b208-4ab1-b7e2-f4b93dab08bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc384cf0-b208-4ab1-b7e2-f4b93dab08bf",
        "outputId": "34821151-d07e-4dd0-d778-4f00e485072c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's one:\n",
            "\n",
            "You know what's wild? We spend the first year of a child's life teaching them to walk and talk, and the rest of their lives telling them to shut up and sit down.\n"
          ]
        }
      ],
      "source": [
        "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke\"}]})\n",
        "print(result[\"messages\"][1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a4d7975-8d94-4d5e-8493-e68ac9fcedf9",
      "metadata": {
        "id": "9a4d7975-8d94-4d5e-8493-e68ac9fcedf9"
      },
      "source": [
        "## values\n",
        "You have seen this streaming mode in our examples so far."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c2d7db",
      "metadata": {
        "id": "e6c2d7db"
      },
      "source": [
        "What we're doing: Stream the agent's `values` mode to show chunked outputs as they arrive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e30a2273-1dd3-47e8-a38e-05ed4b750000",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e30a2273-1dd3-47e8-a38e-05ed4b750000",
        "outputId": "d2c6e9a1-6f65-4bee-efa4-742396ec1d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Tell me a Dad joke\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Here's one:\n",
            "\n",
            "\"I told my wife she was drawing her eyebrows too high. She looked surprised.\"\n",
            "\n",
            "How was that? Did I make you groan?\n"
          ]
        }
      ],
      "source": [
        "# Stream = values\n",
        "for step in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a Dad joke\"}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada88835-3c66-4241-b3d9-4f3d38390c86",
      "metadata": {
        "id": "ada88835-3c66-4241-b3d9-4f3d38390c86"
      },
      "source": [
        "## messages\n",
        "Messages stream data token by token - the lowest latency possible. This is perfect for interactive applications like chatbots."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b1547f",
      "metadata": {
        "id": "f4b1547f"
      },
      "source": [
        "What we're doing: Stream token-level `messages` to demonstrate lowest-latency output (token-by-token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2a9cc553-7357-4d36-b88d-25eaf7462cf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a9cc553-7357-4d36-b88d-25eaf7462cf6",
        "outputId": "6e1fc947-e6fa-4ed3-8fdb-859b422fcaf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a family-friendly poem for you:\n",
            "\n",
            "There once was a family so bright,\n",
            "Their laughter and love shone with delight.\n",
            "They'd gather 'round, hand in hand,\n",
            "And make memories that would forever stand.\n",
            "\n",
            "Their house was a home, full of cheer,\n",
            "Where hugs and kisses were always near.\n",
            "Their table was filled with yummy treats,\n",
            "And their hearts were full of love that skips beats.\n",
            "\n",
            "The kids would play outside all day,\n",
            "Chasing butterflies in a sunny way.\n",
            "Their parents would watch with a smile so wide,\n",
            "Proud of the little ones, side by side.\n",
            "\n",
            "As the sun sets and the day grows old,\n",
            "The family would snuggle up, young and bold.\n",
            "They'd share stories and secrets, and dreams so bright,\n",
            "And fill each other's hearts with love and light.\n",
            "\n",
            "So here's to the family, a shining star,\n",
            "A bundle of love that goes near and far.\n",
            "May their laughter and joy be contagious and free,\n",
            "A family's love, a treasure to see!"
          ]
        }
      ],
      "source": [
        "for token, metadata in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Write me a family friendly poem.\"}]},\n",
        "    stream_mode=\"messages\",\n",
        "):\n",
        "    print(f\"{token.content}\", end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d47c4477-24ff-4321-8f50-aff3324fa831",
      "metadata": {
        "id": "d47c4477-24ff-4321-8f50-aff3324fa831"
      },
      "source": [
        "## Tools can stream too!\n",
        "Streaming generally means delivering information to the user before the final result is ready. There are many cases where this is useful. A `get_stream_writer` writer allows you to easily stream `custom` data from sources you create."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5081bef",
      "metadata": {
        "id": "b5081bef"
      },
      "source": [
        "What we're doing: Define a streaming tool (`get_weather`) that emits `custom` stream chunks and attach it to the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c68179e6-d388-494a-b10d-109c230f6ee0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c68179e6-d388-494a-b10d-109c230f6ee0",
        "outputId": "aedf0885-b709-4c17-c9c8-a5a6cc266f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('values', {'messages': [HumanMessage(content='What is the weather in SF?', additional_kwargs={}, response_metadata={}, id='40fffc1f-d534-4a43-bdf7-f8bc44ee3545')]})\n",
            "('values', {'messages': [HumanMessage(content='What is the weather in SF?', additional_kwargs={}, response_metadata={}, id='40fffc1f-d534-4a43-bdf7-f8bc44ee3545'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '1gqnzrr9j', 'function': {'arguments': '{\"city\":\"SF\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 219, 'total_tokens': 233, 'completion_time': 0.040678001, 'completion_tokens_details': None, 'prompt_time': 0.016393076, 'prompt_tokens_details': None, 'queue_time': 0.005322135, 'total_time': 0.057071077}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bc75a-e4ea-7751-b48e-166e0533b5f8-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'SF'}, 'id': '1gqnzrr9j', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 219, 'output_tokens': 14, 'total_tokens': 233})]})\n",
            "('custom', 'Looking up data for city: SF')\n",
            "('custom', 'Acquired data for city: SF')\n",
            "('values', {'messages': [HumanMessage(content='What is the weather in SF?', additional_kwargs={}, response_metadata={}, id='40fffc1f-d534-4a43-bdf7-f8bc44ee3545'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '1gqnzrr9j', 'function': {'arguments': '{\"city\":\"SF\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 219, 'total_tokens': 233, 'completion_time': 0.040678001, 'completion_tokens_details': None, 'prompt_time': 0.016393076, 'prompt_tokens_details': None, 'queue_time': 0.005322135, 'total_time': 0.057071077}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bc75a-e4ea-7751-b48e-166e0533b5f8-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'SF'}, 'id': '1gqnzrr9j', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 219, 'output_tokens': 14, 'total_tokens': 233}), ToolMessage(content=\"It's always sunny in SF!\", name='get_weather', id='850564f3-dbae-4bca-a24a-423614fb353b', tool_call_id='1gqnzrr9j')]})\n",
            "('values', {'messages': [HumanMessage(content='What is the weather in SF?', additional_kwargs={}, response_metadata={}, id='40fffc1f-d534-4a43-bdf7-f8bc44ee3545'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '1gqnzrr9j', 'function': {'arguments': '{\"city\":\"SF\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 219, 'total_tokens': 233, 'completion_time': 0.040678001, 'completion_tokens_details': None, 'prompt_time': 0.016393076, 'prompt_tokens_details': None, 'queue_time': 0.005322135, 'total_time': 0.057071077}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bc75a-e4ea-7751-b48e-166e0533b5f8-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'SF'}, 'id': '1gqnzrr9j', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 219, 'output_tokens': 14, 'total_tokens': 233}), ToolMessage(content=\"It's always sunny in SF!\", name='get_weather', id='850564f3-dbae-4bca-a24a-423614fb353b', tool_call_id='1gqnzrr9j'), AIMessage(content='However, I should note that the function call I provided earlier will not return the actual weather in SF. The function \\'get_weather\\' is not a real function and does not have any implementation.\\n\\nIf you want to get the actual weather in SF, you would need to implement the \\'get_weather\\' function or use a real-world API that provides weather information.\\n\\nHere\\'s an example of how you might implement the \\'get_weather\\' function using the OpenWeatherMap API:\\n\\n```python\\nimport requests\\n\\ndef get_weather(city):\\n    api_key = \"YOUR_OPENWEATHERMAP_API_KEY\"\\n    base_url = \"http://api.openweathermap.org/data/2.5/weather\"\\n    params = {\\n        \"q\": city,\\n        \"appid\": api_key,\\n        \"units\": \"metric\"\\n    }\\n    response = requests.get(base_url, params=params)\\n    weather_data = response.json()\\n    return weather_data\\n\\n# Usage:\\nweather = get_weather(\"SF\")\\nprint(weather)\\n```\\n\\nYou would need to replace \"YOUR_OPENWEATHERMAP_API_KEY\" with your actual OpenWeatherMap API key.\\n\\nAgain, I apologize for the confusion earlier. I hope this helps! Let me know if you have any other questions.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 249, 'prompt_tokens': 250, 'total_tokens': 499, 'completion_time': 0.33171186, 'completion_tokens_details': None, 'prompt_time': 0.025097705, 'prompt_tokens_details': None, 'queue_time': 0.010749795, 'total_time': 0.356809565}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_6c980774ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bc75a-e569-7cd0-bcac-94a8bd98f780-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 250, 'output_tokens': 249, 'total_tokens': 499})]})\n"
          ]
        }
      ],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langgraph.config import get_stream_writer\n",
        "\n",
        "\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get weather for a given city.\"\"\"\n",
        "    writer = get_stream_writer()\n",
        "    # stream any arbitrary data\n",
        "    writer(f\"Looking up data for city: {city}\")\n",
        "    writer(f\"Acquired data for city: {city}\")\n",
        "    return f\"It's always sunny in {city}!\"\n",
        "\n",
        "\n",
        "agent = create_agent(\n",
        "    #model=\"openai:gpt-5-mini\",\n",
        "    model=llm,\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
        "    stream_mode=[\"values\", \"custom\"],\n",
        "):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c0aca1",
      "metadata": {
        "id": "71c0aca1"
      },
      "source": [
        "What we're doing: Stream only `custom` tool output to observe the tool's emitted chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b4d7ef47-e857-4e07-a233-888306e3e0c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4d7ef47-e857-4e07-a233-888306e3e0c5",
        "outputId": "50c76e2d-ce50-4c39-a9af-100426566e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('custom', 'Looking up data for city: SF')\n",
            "('custom', 'Acquired data for city: SF')\n"
          ]
        }
      ],
      "source": [
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
        "    stream_mode=[\"custom\"],\n",
        "):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3845c067-761f-46a0-817c-2cc42066ce9a",
      "metadata": {
        "id": "3845c067-761f-46a0-817c-2cc42066ce9a"
      },
      "source": [
        "## Try different modes on your own!\n",
        "Modify the stream mode and the select to produce different results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "881eb386",
      "metadata": {
        "id": "881eb386"
      },
      "source": [
        "What we're doing: Filter the streamed chunks to handle `custom` tool output differently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "92943e4f-6c17-4fa3-ad00-f86464ba66f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92943e4f-6c17-4fa3-ad00-f86464ba66f3",
        "outputId": "81fdbd6b-3c6f-42a5-9184-937bb9f1d306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking up data for city: SF\n",
            "Acquired data for city: SF\n"
          ]
        }
      ],
      "source": [
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
        "    stream_mode=[\"values\", \"custom\"],\n",
        "):\n",
        "    if chunk[0] == \"custom\":\n",
        "        print(chunk[1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}