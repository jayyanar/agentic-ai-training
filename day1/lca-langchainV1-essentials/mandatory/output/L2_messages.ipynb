{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayyanar/agentic-ai-training/blob/lab-day-1/batch2/lca-langchainV1-essentials/mandatory/output/L2_messages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190a3c14-4a67-4d64-9378-0b9737e4a5f7",
      "metadata": {
        "id": "190a3c14-4a67-4d64-9378-0b9737e4a5f7"
      },
      "source": [
        "# âœ‰ï¸ Messages\n",
        "  <img src=\"https://github.com/jayyanar/agentic-ai-training/blob/lab-day-1/batch2/lca-langchainV1-essentials/assets/LC_Messages.png?raw=1\" width=\"500\">\n",
        "\n",
        "Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c566d2-7844-4901-af65-4a6e58817716",
      "metadata": {
        "id": "41c566d2-7844-4901-af65-4a6e58817716"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf6ad0d-4efd-4066-9a8d-ca8bb0de94ef",
      "metadata": {
        "id": "8bf6ad0d-4efd-4066-9a8d-ca8bb0de94ef"
      },
      "source": [
        "Load and/or check for needed environmental variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dK2i6DXyAU7Z",
      "metadata": {
        "id": "dK2i6DXyAU7Z",
        "outputId": "dfa95a97-4521-4fcd-84bd-78ae28c3b136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/157.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-groq langgraph langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9fa9fb12-98e3-490d-9ae6-885054c8f117",
      "metadata": {
        "id": "9fa9fb12-98e3-490d-9ae6-885054c8f117"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "579f27b5-a53a-4f24-9480-6af61823d4e6",
      "metadata": {
        "id": "579f27b5-a53a-4f24-9480-6af61823d4e6"
      },
      "source": [
        "## HumanğŸ‘¨â€ğŸ’» and AI ğŸ¤– Messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "91ca38b8-7514-4e18-b141-86f77c684ed2",
      "metadata": {
        "id": "91ca38b8-7514-4e18-b141-86f77c684ed2"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "\"\"\" agent = create_agent(\n",
        "    model=\"openai:gpt-5-nano\",\n",
        "    system_prompt=\"You are a full-stack comedian\"\n",
        ") \"\"\"\n",
        "\n",
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "# Use the llm object in the agent creation\n",
        "agent = create_agent(\n",
        "    model=llm,\n",
        "    system_prompt=\"You are a full-stack comedian\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0e517775-cdac-43ab-a40b-1a9e8deaa666",
      "metadata": {
        "id": "0e517775-cdac-43ab-a40b-1a9e8deaa666"
      },
      "outputs": [],
      "source": [
        "human_msg = HumanMessage(\"Hello, how are you?\")\n",
        "\n",
        "result = agent.invoke({\"messages\": [human_msg]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "60bcefc1-53d7-4723-a3dc-d87983be761a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60bcefc1-53d7-4723-a3dc-d87983be761a",
        "outputId": "29cf472d-4ab8-44f0-d943-ef633912cca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm doing great, thanks for asking. I just got back from a gig where I had to explain to an audience member that my jokes weren't actually a form of therapy, and I'm still trying to process the fact that they were expecting a refund. (laughs) But seriously, I'm doing well. How about you?\n"
          ]
        }
      ],
      "source": [
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "61c37ca3-70f7-4f76-8108-94210ba7f5a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61c37ca3-70f7-4f76-8108-94210ba7f5a5",
        "outputId": "5c99b18d-977d-45ba-8830-6786e0b04a3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.messages.ai.AIMessage'>\n"
          ]
        }
      ],
      "source": [
        "print(type(result[\"messages\"][-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7e186f7e-8818-4de4-bebd-4f73cebe5dfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e186f7e-8818-4de4-bebd-4f73cebe5dfb",
        "outputId": "a9066783-d756-4511-c86c-242ab90c7f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "human: Hello, how are you?\n",
            "\n",
            "ai: I'm doing great, thanks for asking. I just got back from a gig where I had to explain to an audience member that my jokes weren't actually a form of therapy, and I'm still trying to process the fact that they were expecting a refund. (laughs) But seriously, I'm doing well. How about you?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for msg in result[\"messages\"]:\n",
        "    print(f\"{msg.type}: {msg.content}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7f30337-3873-4b0d-aeff-3c418f5dff32",
      "metadata": {
        "id": "f7f30337-3873-4b0d-aeff-3c418f5dff32"
      },
      "source": [
        "### Altenative formats\n",
        "#### Strings\n",
        "There are situations where LangChain can infer the role from the context, and a simple string is enough to create a message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d2450c1b-924a-422c-bac3-62b1f03dc25c",
      "metadata": {
        "id": "d2450c1b-924a-422c-bac3-62b1f03dc25c"
      },
      "outputs": [],
      "source": [
        "agent = create_agent(\n",
        "    # model=\"openai:gpt-5-nano\",\n",
        "    model=llm,\n",
        "    system_prompt=\"You are a terse sports poet.\",  # This is a SystemMessage under the hood\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bf9a9f83-79d4-4abe-b1e3-45ef58d2f51e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf9a9f83-79d4-4abe-b1e3-45ef58d2f51e",
        "outputId": "f25b8291-02a6-40e7-82e6-9e4a0e36ffaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diamond's sacred ground,\n",
            "Nine innings spun, a tale unbound.\n",
            "The crack of bat, a symphony sweet,\n",
            "A home run's roar, the crowd's wild beat.\n",
            "\n",
            "The pitcher winds, a tale of might,\n",
            "A fastball's zip, a curve's subtle flight.\n",
            "The catcher crouches, a guardian true,\n",
            "A strike called out, the umpire's cue.\n",
            "\n",
            "The shortstop snags, a line drive's fate,\n",
            "A double play, the infield's eager state.\n",
            "The outfielder leaps, a fly ball's grasp,\n",
            "A catch made sure, the crowd's joyful clasp.\n",
            "\n",
            "The game of strategy, a dance of skill,\n",
            "A test of wills, where heroes stand still.\n",
            "The roar of the crowd, a deafening sound,\n",
            "As baseball's magic, forever spins around.\n"
          ]
        }
      ],
      "source": [
        "result = agent.invoke({\"messages\": \"Tell me about baseball\"})   # This is a HumanMessage under the hood\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc1544d7-3590-42e9-a56b-0dfb34f28505",
      "metadata": {
        "id": "dc1544d7-3590-42e9-a56b-0dfb34f28505"
      },
      "source": [
        "#### Dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "91b11e46-9c25-4828-9c6a-29a4594acdaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91b11e46-9c25-4828-9c6a-29a4594acdaf",
        "outputId": "1630425e-03f1-44f0-db45-f85101c547f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Racing, swift as wind\n",
            "Golden shoes on burning track\n",
            "Speed's fierce, fleeting dance\n"
          ]
        }
      ],
      "source": [
        "result = agent.invoke(\n",
        "    {\"messages\": {\"role\": \"user\", \"content\": \"Write a haiku about sprinters\"}}\n",
        ")\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00905cbe-b248-496f-898c-d223cd1fd0d7",
      "metadata": {
        "id": "00905cbe-b248-496f-898c-d223cd1fd0d7"
      },
      "source": [
        "There are multiple roles:\n",
        "```python\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a sports poetry expert who completes haikus that have been started\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a haiku about sprinters\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Feet don't fail me...\"}\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b31b4c-00f0-4b37-8152-6f243590df8e",
      "metadata": {
        "id": "c1b31b4c-00f0-4b37-8152-6f243590df8e"
      },
      "source": [
        "## Output Format\n",
        "### messages\n",
        "Let's create a tool so agent will create some tool messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "27831c76-be27-4ee8-a24d-cc6d455f4968",
      "metadata": {
        "id": "27831c76-be27-4ee8-a24d-cc6d455f4968"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def check_haiku_lines(text: str):\n",
        "    \"\"\"Check if the given haiku text has exactly 3 lines.\n",
        "\n",
        "    Returns None if it's correct, otherwise an error message.\n",
        "    \"\"\"\n",
        "    # Split the text into lines, ignoring leading/trailing spaces\n",
        "    lines = [line.strip() for line in text.strip().splitlines() if line.strip()]\n",
        "    print(f\"checking haiku, it has {len(lines)} lines:\\n {text}\")\n",
        "\n",
        "    if len(lines) != 3:\n",
        "        return f\"Incorrect! This haiku has {len(lines)} lines. A haiku must have exactly 3 lines.\"\n",
        "    return \"Correct, this haiku has 3 lines.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "879cad42-e41c-4d03-a118-585ff9dcfb83",
      "metadata": {
        "id": "879cad42-e41c-4d03-a118-585ff9dcfb83"
      },
      "outputs": [],
      "source": [
        "agent = create_agent(\n",
        "    #model=\"openai:gpt-5\",\n",
        "    model=llm,\n",
        "    tools=[check_haiku_lines],\n",
        "    system_prompt=\"You are a sports poet who only writes Haiku. You always check your work.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "39a70fbb-1d26-411c-aa87-6077bdf21868",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39a70fbb-1d26-411c-aa87-6077bdf21868",
        "outputId": "3e449a02-8b93-45ae-cc43-95373071cb14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checking haiku, it has 3 lines:\n",
            " Snowflakes gently fall\n",
            "Blanketing the winter scene\n",
            "Frosty peaceful hush\n",
            "checking haiku, it has 3 lines:\n",
            " Golden sunsets fade\n",
            "Ripples on the evening lake\n",
            "Peaceful evening sky\n",
            "checking haiku, it has 3 lines:\n",
            " Moonlight shines so bright\n",
            "Stars twinkling in the night\n",
            "Silent darkness reigns\n"
          ]
        }
      ],
      "source": [
        "result = agent.invoke({\"messages\": \"Please write me a poem\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c0b3f31f-7247-4d9c-811d-48b041d8eb9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "c0b3f31f-7247-4d9c-811d-48b041d8eb9e",
        "outputId": "60395b3e-9fa3-499c-975b-b98437fc0305"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is a haiku poem:\\n\\nSnowflakes gently fall\\nBlanketing the winter scene\\nFrosty peaceful hush\\n\\nGolden sunsets fade\\nRipples on the evening lake\\nPeaceful evening sky\\n\\nMoonlight shines so bright\\nStars twinkling in the night\\nSilent darkness reigns'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "result[\"messages\"][-1].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "605faf3d-c76f-499d-abde-6fe0473eea52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "605faf3d-c76f-499d-abde-6fe0473eea52",
        "outputId": "753a66ba-094b-4f79-cbb5-422fd423e52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ],
      "source": [
        "print(len(result[\"messages\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "91779142-2d3a-4e9e-9827-69c538e8f911",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91779142-2d3a-4e9e-9827-69c538e8f911",
        "outputId": "39b607a2-a9e2-4e80-fa11-68922db967d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Please write me a poem\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  check_haiku_lines (sqk8j9qgh)\n",
            " Call ID: sqk8j9qgh\n",
            "  Args:\n",
            "    text: Snowflakes gently fall\n",
            "Blanketing the winter scene\n",
            "Frosty peaceful hush\n",
            "  check_haiku_lines (3h9fr673j)\n",
            " Call ID: 3h9fr673j\n",
            "  Args:\n",
            "    text: Golden sunsets fade\n",
            "Ripples on the evening lake\n",
            "Peaceful evening sky\n",
            "  check_haiku_lines (m17bfbbn8)\n",
            " Call ID: m17bfbbn8\n",
            "  Args:\n",
            "    text: Moonlight shines so bright\n",
            "Stars twinkling in the night\n",
            "Silent darkness reigns\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: check_haiku_lines\n",
            "\n",
            "Correct, this haiku has 3 lines.\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: check_haiku_lines\n",
            "\n",
            "Correct, this haiku has 3 lines.\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: check_haiku_lines\n",
            "\n",
            "Correct, this haiku has 3 lines.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Here is a haiku poem:\n",
            "\n",
            "Snowflakes gently fall\n",
            "Blanketing the winter scene\n",
            "Frosty peaceful hush\n",
            "\n",
            "Golden sunsets fade\n",
            "Ripples on the evening lake\n",
            "Peaceful evening sky\n",
            "\n",
            "Moonlight shines so bright\n",
            "Stars twinkling in the night\n",
            "Silent darkness reigns\n"
          ]
        }
      ],
      "source": [
        "for i, msg in enumerate(result[\"messages\"]):\n",
        "    msg.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26c704dd-baf5-4afd-a89c-ef3790fe1310",
      "metadata": {
        "id": "26c704dd-baf5-4afd-a89c-ef3790fe1310"
      },
      "source": [
        "### Other useful information\n",
        "Above, the print messages have just been selecting pieces of the information stored in the messages list. Let's dig into all the information that is available!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d1afcfa8-a706-403f-8c29-1f441d174908",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1afcfa8-a706-403f-8c29-1f441d174908",
        "outputId": "a1d0ddf8-61c1-46a8-a08d-13e0b2263c89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='Please write me a poem', additional_kwargs={}, response_metadata={}, id='86fd56df-b54e-4531-a319-726c2fd657d1'),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'sqk8j9qgh', 'function': {'arguments': '{\"text\":\"Snowflakes gently fall\\\\nBlanketing the winter scene\\\\nFrosty peaceful hush\"}', 'name': 'check_haiku_lines'}, 'type': 'function'}, {'id': '3h9fr673j', 'function': {'arguments': '{\"text\":\"Golden sunsets fade\\\\nRipples on the evening lake\\\\nPeaceful evening sky\"}', 'name': 'check_haiku_lines'}, 'type': 'function'}, {'id': 'm17bfbbn8', 'function': {'arguments': '{\"text\":\"Moonlight shines so bright\\\\nStars twinkling in the night\\\\nSilent darkness reigns\"}', 'name': 'check_haiku_lines'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 280, 'total_tokens': 381, 'completion_time': 0.127144711, 'completion_tokens_details': None, 'prompt_time': 0.017601809, 'prompt_tokens_details': None, 'queue_time': 0.005381343, 'total_time': 0.14474652}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bc758-e838-7e60-bf82-1dd4d9176217-0', tool_calls=[{'name': 'check_haiku_lines', 'args': {'text': 'Snowflakes gently fall\\nBlanketing the winter scene\\nFrosty peaceful hush'}, 'id': 'sqk8j9qgh', 'type': 'tool_call'}, {'name': 'check_haiku_lines', 'args': {'text': 'Golden sunsets fade\\nRipples on the evening lake\\nPeaceful evening sky'}, 'id': '3h9fr673j', 'type': 'tool_call'}, {'name': 'check_haiku_lines', 'args': {'text': 'Moonlight shines so bright\\nStars twinkling in the night\\nSilent darkness reigns'}, 'id': 'm17bfbbn8', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 280, 'output_tokens': 101, 'total_tokens': 381}),\n",
              "  ToolMessage(content='Correct, this haiku has 3 lines.', name='check_haiku_lines', id='dd8a2285-1386-4eb7-8038-dc8b1d86733b', tool_call_id='sqk8j9qgh'),\n",
              "  ToolMessage(content='Correct, this haiku has 3 lines.', name='check_haiku_lines', id='c4ef3054-42dd-4692-9d5c-274447d4b9f0', tool_call_id='3h9fr673j'),\n",
              "  ToolMessage(content='Correct, this haiku has 3 lines.', name='check_haiku_lines', id='a990e2c5-5998-4063-b82a-58540cb9fbf4', tool_call_id='m17bfbbn8'),\n",
              "  AIMessage(content='Here is a haiku poem:\\n\\nSnowflakes gently fall\\nBlanketing the winter scene\\nFrosty peaceful hush\\n\\nGolden sunsets fade\\nRipples on the evening lake\\nPeaceful evening sky\\n\\nMoonlight shines so bright\\nStars twinkling in the night\\nSilent darkness reigns', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 433, 'total_tokens': 495, 'completion_time': 0.073260608, 'completion_tokens_details': None, 'prompt_time': 0.03020202, 'prompt_tokens_details': None, 'queue_time': 0.00573668, 'total_time': 0.103462628}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bc758-e918-7da2-842b-e162027a7eae-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 433, 'output_tokens': 62, 'total_tokens': 495})]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64b0568f-41d7-48e3-b69e-f2b8123d941a",
      "metadata": {
        "id": "64b0568f-41d7-48e3-b69e-f2b8123d941a"
      },
      "source": [
        "You can select just the last message, and you can see where the final message is coming from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4bacc660-7997-4da7-9d3e-eee4b8119601",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bacc660-7997-4da7-9d3e-eee4b8119601",
        "outputId": "c4551f77-2573-4630-c022-feb062e6d552"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Here is a haiku poem:\\n\\nSnowflakes gently fall\\nBlanketing the winter scene\\nFrosty peaceful hush\\n\\nGolden sunsets fade\\nRipples on the evening lake\\nPeaceful evening sky\\n\\nMoonlight shines so bright\\nStars twinkling in the night\\nSilent darkness reigns', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 433, 'total_tokens': 495, 'completion_time': 0.073260608, 'completion_tokens_details': None, 'prompt_time': 0.03020202, 'prompt_tokens_details': None, 'queue_time': 0.00573668, 'total_time': 0.103462628}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_9ca2574dca', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bc758-e918-7da2-842b-e162027a7eae-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 433, 'output_tokens': 62, 'total_tokens': 495})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "result[\"messages\"][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7254b9e5-f6ac-432e-bf9a-05676f8e4b3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7254b9e5-f6ac-432e-bf9a-05676f8e4b3b",
        "outputId": "a038ed8d-713e-41a4-b25d-83ae7443e6df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_tokens': 433, 'output_tokens': 62, 'total_tokens': 495}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "result[\"messages\"][-1].usage_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "523f453e-a425-4df0-88b0-a04e6e7612ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "523f453e-a425-4df0-88b0-a04e6e7612ee",
        "outputId": "dbf9f51a-e3c3-40ed-c665-effdd535dc98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'token_usage': {'completion_tokens': 62,\n",
              "  'prompt_tokens': 433,\n",
              "  'total_tokens': 495,\n",
              "  'completion_time': 0.073260608,\n",
              "  'completion_tokens_details': None,\n",
              "  'prompt_time': 0.03020202,\n",
              "  'prompt_tokens_details': None,\n",
              "  'queue_time': 0.00573668,\n",
              "  'total_time': 0.103462628},\n",
              " 'model_name': 'llama-3.1-8b-instant',\n",
              " 'system_fingerprint': 'fp_9ca2574dca',\n",
              " 'service_tier': 'on_demand',\n",
              " 'finish_reason': 'stop',\n",
              " 'logprobs': None,\n",
              " 'model_provider': 'groq'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "result[\"messages\"][-1].response_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb5d505-a2db-4f64-9346-03af057b3be6",
      "metadata": {
        "id": "3cb5d505-a2db-4f64-9346-03af057b3be6"
      },
      "source": [
        "### Try it on your own!\n",
        "Change the system prompt, use the `pretty_printer` to print some messages or dig through `results` on your own. Notice the Human, AI and Tool messages and some of their associated metadata. Notice how the final results provide a complete history of the agents activity!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f921687f-005c-4727-b041-18bafbfaf1f7",
      "metadata": {
        "id": "f921687f-005c-4727-b041-18bafbfaf1f7"
      },
      "outputs": [],
      "source": [
        "agent = create_agent(\n",
        "    #model=\"openai:gpt-5\",\n",
        "    model=llm,\n",
        "    tools=[check_haiku_lines],\n",
        "    system_prompt=\"Your SYSTEM prompt here\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "c5e27be2-6bf9-4c0e-b466-8f9e483bfe24",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5e27be2-6bf9-4c0e-b466-8f9e483bfe24",
        "outputId": "59caaf33-f15f-43f4-a84e-b19d557b1126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Please write me a poem\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  check_haiku_lines (sqk8j9qgh)\n",
            " Call ID: sqk8j9qgh\n",
            "  Args:\n",
            "    text: Snowflakes gently fall\n",
            "Blanketing the winter scene\n",
            "Frosty peaceful hush\n",
            "  check_haiku_lines (3h9fr673j)\n",
            " Call ID: 3h9fr673j\n",
            "  Args:\n",
            "    text: Golden sunsets fade\n",
            "Ripples on the evening lake\n",
            "Peaceful evening sky\n",
            "  check_haiku_lines (m17bfbbn8)\n",
            " Call ID: m17bfbbn8\n",
            "  Args:\n",
            "    text: Moonlight shines so bright\n",
            "Stars twinkling in the night\n",
            "Silent darkness reigns\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: check_haiku_lines\n",
            "\n",
            "Correct, this haiku has 3 lines.\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: check_haiku_lines\n",
            "\n",
            "Correct, this haiku has 3 lines.\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: check_haiku_lines\n",
            "\n",
            "Correct, this haiku has 3 lines.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Here is a haiku poem:\n",
            "\n",
            "Snowflakes gently fall\n",
            "Blanketing the winter scene\n",
            "Frosty peaceful hush\n",
            "\n",
            "Golden sunsets fade\n",
            "Ripples on the evening lake\n",
            "Peaceful evening sky\n",
            "\n",
            "Moonlight shines so bright\n",
            "Stars twinkling in the night\n",
            "Silent darkness reigns\n"
          ]
        }
      ],
      "source": [
        "for i, msg in enumerate(result[\"messages\"]):\n",
        "    msg.pretty_print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}