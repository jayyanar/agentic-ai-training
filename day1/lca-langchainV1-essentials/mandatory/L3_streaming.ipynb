{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "483ab18e-f419-46ad-9bbe-171ffd05f983",
      "metadata": {
        "id": "483ab18e-f419-46ad-9bbe-171ffd05f983"
      },
      "source": [
        "# Streaming\n",
        "\n",
        "<img src=\"https://github.com/jayyanar/agentic-ai-training/blob/lab-day-1/batch2/lca-langchainV1-essentials/assets/LC_streaming.png?raw=1\" width=\"400\">\n",
        "\n",
        "Streaming reduces the latency between generating data and the user receiving it.\n",
        "There are two types frequently used with Agents:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66f0f22c-9724-46ce-baf3-60a2de701fb3",
      "metadata": {
        "id": "66f0f22c-9724-46ce-baf3-60a2de701fb3"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b76bab7-fa52-46f4-86bc-b157067e0168",
      "metadata": {
        "id": "6b76bab7-fa52-46f4-86bc-b157067e0168"
      },
      "source": [
        "Load and/or check for needed environmental variables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c0d7c65",
      "metadata": {},
      "source": [
        "What we're doing: Install required packages for streaming examples in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EaeYUvxIB8G4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaeYUvxIB8G4",
        "outputId": "336339a6-4149-4678-a5b9-c8f47222912a"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-groq langgraph langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2659bde",
      "metadata": {},
      "source": [
        "What we're doing: Load the GROQ API key from Colab userdata into the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2fcfd93-0004-4ff1-9b60-0b21baf68c6d",
      "metadata": {
        "id": "b2fcfd93-0004-4ff1-9b60-0b21baf68c6d"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51ca4692",
      "metadata": {},
      "source": [
        "What we're doing: Initialize the Groq LLM used in streaming examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "166fc0b1-2322-4dd2-a358-89309fb9f4ce",
      "metadata": {
        "id": "166fc0b1-2322-4dd2-a358-89309fb9f4ce"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Initialize the Groq model\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "418bbbd6",
      "metadata": {},
      "source": [
        "What we're doing: Create an agent with a simple system prompt for streaming demonstrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a26b4586-453a-4c10-9fae-4be3f4cb6cd4",
      "metadata": {
        "id": "a26b4586-453a-4c10-9fae-4be3f4cb6cd4"
      },
      "outputs": [],
      "source": [
        "agent = create_agent(\n",
        "    #model=\"openai:gpt-5\",\n",
        "    model=llm,\n",
        "    system_prompt=\"You are a full-stack comedian\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a907aa9-a608-47e2-92d4-6758a1728cb2",
      "metadata": {
        "id": "7a907aa9-a608-47e2-92d4-6758a1728cb2"
      },
      "source": [
        "## No Streaming (invoke)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "198393ca",
      "metadata": {},
      "source": [
        "What we're doing: Invoke the agent synchronously (no streaming) to get a full response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc384cf0-b208-4ab1-b7e2-f4b93dab08bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc384cf0-b208-4ab1-b7e2-f4b93dab08bf",
        "outputId": "6f0951ac-da6b-49ad-b855-26ee993539a4"
      },
      "outputs": [],
      "source": [
        "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke\"}]})\n",
        "print(result[\"messages\"][1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a4d7975-8d94-4d5e-8493-e68ac9fcedf9",
      "metadata": {
        "id": "9a4d7975-8d94-4d5e-8493-e68ac9fcedf9"
      },
      "source": [
        "## values\n",
        "You have seen this streaming mode in our examples so far."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c2d7db",
      "metadata": {},
      "source": [
        "What we're doing: Stream the agent's `values` mode to show chunked outputs as they arrive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e30a2273-1dd3-47e8-a38e-05ed4b750000",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e30a2273-1dd3-47e8-a38e-05ed4b750000",
        "outputId": "e58ba3a7-7d35-423d-a4bf-78c53c465df9"
      },
      "outputs": [],
      "source": [
        "# Stream = values\n",
        "for step in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a Dad joke\"}]},\n",
        "    stream_mode=\"values\",\n",
        "):\n",
        "    step[\"messages\"][-1].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada88835-3c66-4241-b3d9-4f3d38390c86",
      "metadata": {
        "id": "ada88835-3c66-4241-b3d9-4f3d38390c86"
      },
      "source": [
        "## messages\n",
        "Messages stream data token by token - the lowest latency possible. This is perfect for interactive applications like chatbots."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b1547f",
      "metadata": {},
      "source": [
        "What we're doing: Stream token-level `messages` to demonstrate lowest-latency output (token-by-token)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9cc553-7357-4d36-b88d-25eaf7462cf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a9cc553-7357-4d36-b88d-25eaf7462cf6",
        "outputId": "53125867-e74c-41c1-dfab-75af3b15b991"
      },
      "outputs": [],
      "source": [
        "for token, metadata in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Write me a family friendly poem.\"}]},\n",
        "    stream_mode=\"messages\",\n",
        "):\n",
        "    print(f\"{token.content}\", end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d47c4477-24ff-4321-8f50-aff3324fa831",
      "metadata": {
        "id": "d47c4477-24ff-4321-8f50-aff3324fa831"
      },
      "source": [
        "## Tools can stream too!\n",
        "Streaming generally means delivering information to the user before the final result is ready. There are many cases where this is useful. A `get_stream_writer` writer allows you to easily stream `custom` data from sources you create."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5081bef",
      "metadata": {},
      "source": [
        "What we're doing: Define a streaming tool (`get_weather`) that emits `custom` stream chunks and attach it to the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c68179e6-d388-494a-b10d-109c230f6ee0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c68179e6-d388-494a-b10d-109c230f6ee0",
        "outputId": "8be9de1f-4bc6-4576-cd86-eab1ddc23d03"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langgraph.config import get_stream_writer\n",
        "\n",
        "\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get weather for a given city.\"\"\"\n",
        "    writer = get_stream_writer()\n",
        "    # stream any arbitrary data\n",
        "    writer(f\"Looking up data for city: {city}\")\n",
        "    writer(f\"Acquired data for city: {city}\")\n",
        "    return f\"It's always sunny in {city}!\"\n",
        "\n",
        "\n",
        "agent = create_agent(\n",
        "    #model=\"openai:gpt-5-mini\",\n",
        "    model=llm,\n",
        "    tools=[get_weather],\n",
        ")\n",
        "\n",
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
        "    stream_mode=[\"values\", \"custom\"],\n",
        "):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c0aca1",
      "metadata": {},
      "source": [
        "What we're doing: Stream only `custom` tool output to observe the tool's emitted chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d7ef47-e857-4e07-a233-888306e3e0c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4d7ef47-e857-4e07-a233-888306e3e0c5",
        "outputId": "9a4f3416-193f-4aa8-c2e9-967dd6719751"
      },
      "outputs": [],
      "source": [
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
        "    stream_mode=[\"custom\"],\n",
        "):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3845c067-761f-46a0-817c-2cc42066ce9a",
      "metadata": {
        "id": "3845c067-761f-46a0-817c-2cc42066ce9a"
      },
      "source": [
        "## Try different modes on your own!\n",
        "Modify the stream mode and the select to produce different results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "881eb386",
      "metadata": {},
      "source": [
        "What we're doing: Filter the streamed chunks to handle `custom` tool output differently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92943e4f-6c17-4fa3-ad00-f86464ba66f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92943e4f-6c17-4fa3-ad00-f86464ba66f3",
        "outputId": "27fdd298-c47a-4fdd-a92f-0e8d97eac4ff"
      },
      "outputs": [],
      "source": [
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]},\n",
        "    stream_mode=[\"values\", \"custom\"],\n",
        "):\n",
        "    if chunk[0] == \"custom\":\n",
        "        print(chunk[1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
